
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>UAS &#8212; Penambangan Data</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'UAS (1)';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Pra UAS" href="uaspra.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/foto.png" class="logo__image only-light" alt="Penambangan Data - Home"/>
    <script>document.write(`<img src="_static/foto.png" class="logo__image only-dark" alt="Penambangan Data - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Penambangan Data
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Tugas1.html">Tugas 1 Pendata</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Tugas2.html">Penjelasan Outlier Deteksi</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="knn.html">Deteksi Outlier dengan K-Nearest Neighbors (KNN) dalam Data Understanding</a></li>
<li class="toctree-l2"><a class="reference internal" href="LOF.html">Deteksi Outlier dengan metode Local Outlier Factor (LOF) dalam Data Understanding</a></li>

</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="Uts.html">UTS Pendata</a></li>
<li class="toctree-l1"><a class="reference internal" href="k-means%20%281%29.html"><strong>Algoritma <em>K-Means</em></strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="Fuzzy_CMeans.html"><strong>FUZZY C-MEANS</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="DecisionnTree.html">Decision Tree</a></li>
<li class="toctree-l1"><a class="reference internal" href="Binning.html">Teknik Binning</a></li>
<li class="toctree-l1"><a class="reference internal" href="uaspra.html">Pra UAS</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">UAS</a></li>

</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FUAS (1).html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/UAS (1).ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>UAS</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">UAS</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#efisiensi-energi">Efisiensi Energi</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-understanding">Data Understanding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sumber-data-set">Sumber Data Set</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#penjelasan-fitur-dan-variabel">Penjelasan Fitur dan Variabel</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#integrasi-data">Integrasi Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visulaisasi-data">Visulaisasi Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#missing-value">Missing Value</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocessing-data">Preprocessing Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalisasi-data">Normalisasi Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#split-data">Split Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reduksi-dimensi-dengan-pca">Reduksi Dimensi dengan PCA</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hasil-pre-processing">Hasil Pre Processing</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modelling">Modelling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pemodelan-prediksi-efisiensi-energi-bangunan-menggunakan-decision-tree">Pemodelan Prediksi Efisiensi Energi Bangunan Menggunakan Decision Tree</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pemodelan-kebutuhan-energi-bangunan-menggunakan-k-nearest-neighbors-knn">Pemodelan Kebutuhan Energi Bangunan Menggunakan K-Nearest Neighbors (KNN)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pemodelan-prediksi-kebutuhan-energi-bangunan-dengan-gaussian-naive-bayes">Pemodelan Prediksi Kebutuhan Energi Bangunan dengan Gaussian Naive Bayes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluasi">Evaluasi</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deploy">Deploy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hasil-web">Hasil Web</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="uas">
<h1>UAS<a class="headerlink" href="#uas" title="Link to this heading">#</a></h1>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="efisiensi-energi">
<h1>Efisiensi Energi<a class="headerlink" href="#efisiensi-energi" title="Link to this heading">#</a></h1>
<section id="data-understanding">
<h2>Data Understanding<a class="headerlink" href="#data-understanding" title="Link to this heading">#</a></h2>
<section id="sumber-data-set">
<h3>Sumber Data Set<a class="headerlink" href="#sumber-data-set" title="Link to this heading">#</a></h3>
<p>Data Set diambil dari:</p>
<p><a class="reference external" href="https://archive.ics.uci.edu/dataset/242/energy+efficiency">https://archive.ics.uci.edu/dataset/242/energy+efficiency</a></p>
<p>Tsanas, A. &amp; Xifara, A. (2012). Energy Efficiency [Dataset]. UCI Machine Learning Repository. <a class="reference external" href="https://doi.org/10.24432/C51307">https://doi.org/10.24432/C51307</a>.</p>
</section>
<section id="penjelasan-fitur-dan-variabel">
<h3>Penjelasan Fitur dan Variabel<a class="headerlink" href="#penjelasan-fitur-dan-variabel" title="Link to this heading">#</a></h3>
<p>Dataset ini digunakan untuk memprediksi efisiensi energi bangunan, yang diukur melalui dua indikator utama: Heating Load (Y1) dan Cooling Load (Y2). Setiap baris pada dataset mewakili satu desain bangunan dengan sejumlah parameter fisik yang berpengaruh terhadap konsumsi energi.</p>
<p>Fitur-fitur yang dianalisis dalam dataset ini meliputi:</p>
<ul class="simple">
<li><p>Relative Compactness: ukuran seberapa ringkas bentuk bangunan,</p></li>
<li><p>Surface Area dan Wall Area: luas permukaan luar dan dinding bangunan,</p></li>
<li><p>Roof Area: total area atap,</p></li>
<li><p>Overall Height: tinggi total bangunan,</p></li>
<li><p>Orientation: orientasi bangunan terhadap arah mata angin,</p></li>
<li><p>Glazing Area: persentase permukaan bangunan yang terdiri dari kaca,</p></li>
<li><p>Glazing Area Distribution: distribusi letak kaca pada sisi bangunan.</p></li>
</ul>
<p>Semua fitur ini bersifat numerik dan dirancang untuk mengukur pengaruh bentuk, ukuran, dan pencahayaan alami terhadap kebutuhan energi bangunan.</p>
<p>Adapun variabel target dalam dataset ini adalah:</p>
<ul class="simple">
<li><p>Y1 (Heating Load): jumlah energi yang dibutuhkan untuk pemanasan ruang,</p></li>
<li><p>Y2 (Cooling Load): jumlah energi yang dibutuhkan untuk pendinginan ruang.</p></li>
</ul>
<p>Kedua target ini diukur secara kontinu, dan bertujuan membantu dalam perancangan bangunan yang hemat energi.</p>
</section>
<section id="integrasi-data">
<h3>Integrasi Data<a class="headerlink" href="#integrasi-data" title="Link to this heading">#</a></h3>
<p>untuk mengambil data agar dapat diolah, perlu untuk menginstall package yang telah disediakan oleh UCI Dataset. Instalasi dilakukan berguna untuk menarik data yang berasal dari UCI dataset agar dapat diolah. peritah untuk mengambil data dari UCI dataset dapat di lihat ketika menekan tombol import in python pada datase yang diinginkan dan ikuti perintah tersebut agar data dapat diambil dari UCI dataset. Contoh pengambilan data dari UCI dataset dapat dilihat pada gambar dan perintah berikut:<img alt="image.png" src="data:image/png;base64,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" /></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">ucimlrepo</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: ucimlrepo in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.0.7)
Requirement already satisfied: pandas&gt;=1.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from ucimlrepo) (2.2.3)
Requirement already satisfied: certifi&gt;=2020.12.5 in /home/codespace/.local/lib/python3.12/site-packages (from ucimlrepo) (2024.8.30)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: numpy&gt;=1.26.0 in /home/codespace/.local/lib/python3.12/site-packages (from pandas&gt;=1.0.0-&gt;ucimlrepo) (2.2.0)
Requirement already satisfied: python-dateutil&gt;=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas&gt;=1.0.0-&gt;ucimlrepo) (2.9.0.post0)
Requirement already satisfied: pytz&gt;=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas&gt;=1.0.0-&gt;ucimlrepo) (2024.2)
Requirement already satisfied: tzdata&gt;=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas&gt;=1.0.0-&gt;ucimlrepo) (2024.2)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: six&gt;=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas&gt;=1.0.0-&gt;ucimlrepo) (1.17.0)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">[</span><span class=" -Color -Color-Blue">notice</span><span class=" -Color -Color-Bold">]</span> A new release of pip is available: <span class=" -Color -Color-Red">24.3.1</span> -&gt; <span class=" -Color -Color-Green">25.1.1</span>
<span class=" -Color -Color-Bold">[</span><span class=" -Color -Color-Blue">notice</span><span class=" -Color -Color-Bold">]</span> To update, run: <span class=" -Color -Color-Green">python3 -m pip install --upgrade pip</span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Note: you may need to restart the kernel to use updated packages.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ucimlrepo</span> <span class="kn">import</span> <span class="n">fetch_ucirepo</span>

<span class="c1"># fetch dataset</span>
<span class="n">energy_efficiency</span> <span class="o">=</span> <span class="n">fetch_ucirepo</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">242</span><span class="p">)</span>

<span class="c1"># data (as pandas dataframes)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">energy_efficiency</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">features</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">energy_efficiency</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">targets</span>

 <span class="c1">## data (as pandas dataframes)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">energy_efficiency</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">original</span>
  <span class="c1"># menyimpan hasil komputasi ke dalam csv</span>
<span class="n">data</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s2">&quot;energy_efficiency.csv&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">info</span><span class="p">())</span> <span class="c1">#untuk menampilkan info fitur-fitur yang ada di tabel</span>

<span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">())</span> <span class="c1">#untuk menampilkan 5 baris pertama</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 768 entries, 0 to 767
Data columns (total 10 columns):
 #   Column  Non-Null Count  Dtype  
---  ------  --------------  -----  
 0   X1      768 non-null    float64
 1   X2      768 non-null    float64
 2   X3      768 non-null    float64
 3   X4      768 non-null    float64
 4   X5      768 non-null    float64
 5   X6      768 non-null    int64  
 6   X7      768 non-null    float64
 7   X8      768 non-null    int64  
 8   Y1      768 non-null    float64
 9   Y2      768 non-null    float64
dtypes: float64(8), int64(2)
memory usage: 60.1 KB
None
     X1     X2     X3      X4   X5  X6   X7  X8     Y1     Y2
0  0.98  514.5  294.0  110.25  7.0   2  0.0   0  15.55  21.33
1  0.98  514.5  294.0  110.25  7.0   3  0.0   0  15.55  21.33
2  0.98  514.5  294.0  110.25  7.0   4  0.0   0  15.55  21.33
3  0.98  514.5  294.0  110.25  7.0   5  0.0   0  15.55  21.33
4  0.90  563.5  318.5  122.50  7.0   2  0.0   0  20.84  28.28
</pre></div>
</div>
</div>
</div>
</section>
<section id="visulaisasi-data">
<h3>Visulaisasi Data<a class="headerlink" href="#visulaisasi-data" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="c1">#display dataset</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>X1</th>
      <th>X2</th>
      <th>X3</th>
      <th>X4</th>
      <th>X5</th>
      <th>X6</th>
      <th>X7</th>
      <th>X8</th>
      <th>Y1</th>
      <th>Y2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.98</td>
      <td>514.5</td>
      <td>294.0</td>
      <td>110.25</td>
      <td>7.0</td>
      <td>2</td>
      <td>0.0</td>
      <td>0</td>
      <td>15.55</td>
      <td>21.33</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.98</td>
      <td>514.5</td>
      <td>294.0</td>
      <td>110.25</td>
      <td>7.0</td>
      <td>3</td>
      <td>0.0</td>
      <td>0</td>
      <td>15.55</td>
      <td>21.33</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.98</td>
      <td>514.5</td>
      <td>294.0</td>
      <td>110.25</td>
      <td>7.0</td>
      <td>4</td>
      <td>0.0</td>
      <td>0</td>
      <td>15.55</td>
      <td>21.33</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.98</td>
      <td>514.5</td>
      <td>294.0</td>
      <td>110.25</td>
      <td>7.0</td>
      <td>5</td>
      <td>0.0</td>
      <td>0</td>
      <td>15.55</td>
      <td>21.33</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.90</td>
      <td>563.5</td>
      <td>318.5</td>
      <td>122.50</td>
      <td>7.0</td>
      <td>2</td>
      <td>0.0</td>
      <td>0</td>
      <td>20.84</td>
      <td>28.28</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>763</th>
      <td>0.64</td>
      <td>784.0</td>
      <td>343.0</td>
      <td>220.50</td>
      <td>3.5</td>
      <td>5</td>
      <td>0.4</td>
      <td>5</td>
      <td>17.88</td>
      <td>21.40</td>
    </tr>
    <tr>
      <th>764</th>
      <td>0.62</td>
      <td>808.5</td>
      <td>367.5</td>
      <td>220.50</td>
      <td>3.5</td>
      <td>2</td>
      <td>0.4</td>
      <td>5</td>
      <td>16.54</td>
      <td>16.88</td>
    </tr>
    <tr>
      <th>765</th>
      <td>0.62</td>
      <td>808.5</td>
      <td>367.5</td>
      <td>220.50</td>
      <td>3.5</td>
      <td>3</td>
      <td>0.4</td>
      <td>5</td>
      <td>16.44</td>
      <td>17.11</td>
    </tr>
    <tr>
      <th>766</th>
      <td>0.62</td>
      <td>808.5</td>
      <td>367.5</td>
      <td>220.50</td>
      <td>3.5</td>
      <td>4</td>
      <td>0.4</td>
      <td>5</td>
      <td>16.48</td>
      <td>16.61</td>
    </tr>
    <tr>
      <th>767</th>
      <td>0.62</td>
      <td>808.5</td>
      <td>367.5</td>
      <td>220.50</td>
      <td>3.5</td>
      <td>5</td>
      <td>0.4</td>
      <td>5</td>
      <td>16.64</td>
      <td>16.03</td>
    </tr>
  </tbody>
</table>
<p>768 rows × 10 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="c1"># Loop semua kolom</span>
<span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">data</span><span class="p">[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="s1">&#39;object&#39;</span> <span class="ow">or</span> <span class="n">data</span><span class="p">[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">nunique</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">:</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">countplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">column</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Distribusi Kategori: </span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">dropna</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;skyblue&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Distribusi Numerik: </span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/9e9dfc21d7654dbae8f1f5bf7f85addbd911e7262b4beacb26034d0b5b80b561.png" src="_images/9e9dfc21d7654dbae8f1f5bf7f85addbd911e7262b4beacb26034d0b5b80b561.png" />
<img alt="_images/6c0e9ecd929094e7ba931471f590015a9796584fe9e4ba69e90b1e6838151399.png" src="_images/6c0e9ecd929094e7ba931471f590015a9796584fe9e4ba69e90b1e6838151399.png" />
<img alt="_images/4e5d76394476f4723d9344d88eacab1abb3a80de4c94fffbf1dfa6e7c9f76347.png" src="_images/4e5d76394476f4723d9344d88eacab1abb3a80de4c94fffbf1dfa6e7c9f76347.png" />
<img alt="_images/759c74faa143f1cd87085212b6ee7c827f0a051d4706918d3c2aba1b3d9c6819.png" src="_images/759c74faa143f1cd87085212b6ee7c827f0a051d4706918d3c2aba1b3d9c6819.png" />
<img alt="_images/91229c6d3fe6db2bf5af2705204c560bffbad9fffcb0602006ba94228b6ade16.png" src="_images/91229c6d3fe6db2bf5af2705204c560bffbad9fffcb0602006ba94228b6ade16.png" />
<img alt="_images/fa2b608c65e4b026c2eae651e2b8dd6be0620f844a966d0a921a0cfa8187f311.png" src="_images/fa2b608c65e4b026c2eae651e2b8dd6be0620f844a966d0a921a0cfa8187f311.png" />
<img alt="_images/894016d30f8c888b6d9ebbe971c28d956e8a099d8f43670f2295d539b267064f.png" src="_images/894016d30f8c888b6d9ebbe971c28d956e8a099d8f43670f2295d539b267064f.png" />
<img alt="_images/d84f4b6aa09df55a077f5559cc68d703af98d21e02ffbe22405d77d1ea7a16d3.png" src="_images/d84f4b6aa09df55a077f5559cc68d703af98d21e02ffbe22405d77d1ea7a16d3.png" />
<img alt="_images/0fba5a25d24753a327566669822db508c39d33f164a8133d646ac5d87357a3f5.png" src="_images/0fba5a25d24753a327566669822db508c39d33f164a8133d646ac5d87357a3f5.png" />
<img alt="_images/29269914e8269170f9d2a09a843c7011f6433f3fbd4c2451060938d8dbdc75bc.png" src="_images/29269914e8269170f9d2a09a843c7011f6433f3fbd4c2451060938d8dbdc75bc.png" />
</div>
</div>
</section>
<section id="missing-value">
<h3>Missing Value<a class="headerlink" href="#missing-value" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Detect rows with missing values</span>
<span class="n">rows_with_missing</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span>

<span class="c1"># Tampilkan baris dengan nilai yang hilang beserta ID, fitur, dan labelnya</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Rows with Missing Values:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rows_with_missing</span><span class="p">)</span>

<span class="c1"># Detect missing values</span>
<span class="n">missing_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span>

<span class="c1"># Hitung nilai yang hilang di setiap kolom</span>
<span class="n">missing_counts</span> <span class="o">=</span> <span class="n">missing_values</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="c1"># Display missing value counts</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Missing Value Counts:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">missing_counts</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Rows with Missing Values:
Empty DataFrame
Columns: [X1, X2, X3, X4, X5, X6, X7, X8, Y1, Y2]
Index: []
Missing Value Counts:
X1    0
X2    0
X3    0
X4    0
X5    0
X6    0
X7    0
X8    0
Y1    0
Y2    0
dtype: int64
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="preprocessing-data">
<h2>Preprocessing Data<a class="headerlink" href="#preprocessing-data" title="Link to this heading">#</a></h2>
<section id="normalisasi-data">
<h3>Normalisasi Data<a class="headerlink" href="#normalisasi-data" title="Link to this heading">#</a></h3>
<p>Kode tersebut bertujuan untuk melakukan praproses awal terhadap dataset efisiensi energi bangunan dari UCI. Langkah yang dilakukan meliputi pengambilan data, pengecekan dan penanganan nilai kosong (meskipun data ini umumnya bersih), pemisahan antara fitur (X1–X8) dan target (Y1 dan Y2), serta normalisasi fitur menggunakan MinMaxScaler agar seluruh nilai berada dalam rentang 0 hingga 1. Normalisasi ini penting agar fitur memiliki skala yang seimbang sebelum digunakan dalam pemodelan machine learning. Hasil akhirnya berupa data yang telah siap digunakan untuk proses pelatihan dan evaluasi model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>
<span class="kn">from</span> <span class="nn">ucimlrepo</span> <span class="kn">import</span> <span class="n">fetch_ucirepo</span>

<span class="c1"># Ambil dataset dari UCI</span>
<span class="n">energy_efficiency</span> <span class="o">=</span> <span class="n">fetch_ucirepo</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">242</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">energy_efficiency</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">original</span>

<span class="c1"># Tangani missing value jika ada (meskipun dataset ini bersih)</span>
<span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">data</span><span class="p">[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">data</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">median</span><span class="p">())</span>

<span class="c1"># Pisahkan fitur dan target</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="s1">&#39;X1&#39;</span><span class="p">,</span> <span class="s1">&#39;X2&#39;</span><span class="p">,</span> <span class="s1">&#39;X3&#39;</span><span class="p">,</span> <span class="s1">&#39;X4&#39;</span><span class="p">,</span> <span class="s1">&#39;X5&#39;</span><span class="p">,</span> <span class="s1">&#39;X6&#39;</span><span class="p">,</span> <span class="s1">&#39;X7&#39;</span><span class="p">,</span> <span class="s1">&#39;X8&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="s1">&#39;Y1&#39;</span><span class="p">,</span> <span class="s1">&#39;Y2&#39;</span><span class="p">]]</span>

<span class="c1"># Normalisasi fitur dengan MinMaxScaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">X_minmax</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Ubah ke DataFrame kembali</span>
<span class="n">X_minmax_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_minmax</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

<span class="c1"># Tampilkan hasil normalisasi</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hasil normalisasi dengan MinMaxScaler:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_minmax_df</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Hasil normalisasi dengan MinMaxScaler:
         X1        X2        X3        X4   X5        X6   X7   X8
0  1.000000  0.000000  0.285714  0.000000  1.0  0.000000  0.0  0.0
1  1.000000  0.000000  0.285714  0.000000  1.0  0.333333  0.0  0.0
2  1.000000  0.000000  0.285714  0.000000  1.0  0.666667  0.0  0.0
3  1.000000  0.000000  0.285714  0.000000  1.0  1.000000  0.0  0.0
4  0.777778  0.166667  0.428571  0.111111  1.0  0.000000  0.0  0.0
</pre></div>
</div>
</div>
</div>
</section>
<section id="split-data">
<h3>Split Data<a class="headerlink" href="#split-data" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Split data (gunakan hasil dari normalisasi di atas)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X_minmax_df</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="c1"># Tampilkan bentuk data</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Bentuk data setelah split:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X_train:&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X_test :&quot;</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y_train:&quot;</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y_test :&quot;</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Bentuk data setelah split:
X_train: (614, 8)
X_test : (154, 8)
y_train: (614, 2)
y_test : (154, 2)
</pre></div>
</div>
</div>
</div>
</section>
<section id="reduksi-dimensi-dengan-pca">
<h3>Reduksi Dimensi dengan PCA<a class="headerlink" href="#reduksi-dimensi-dengan-pca" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="c1"># Inisialisasi PCA untuk reduksi ke 2 komponen</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_minmax_df</span><span class="p">)</span>

<span class="c1"># Ubah ke DataFrame</span>
<span class="n">X_pca_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_pca</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;PC1&#39;</span><span class="p">,</span> <span class="s1">&#39;PC2&#39;</span><span class="p">])</span>

<span class="c1"># Tampilkan hasil reduksi dimensi</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hasil reduksi dimensi dengan PCA (2 komponen utama):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_pca_df</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>

<span class="c1"># Variansi yang dijelaskan oleh masing-masing komponen</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variansi dijelaskan oleh PCA:&quot;</span><span class="p">,</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Hasil reduksi dimensi dengan PCA (2 komponen utama):
        PC1       PC2
0  1.062000 -0.500000
1  1.062000 -0.166667
2  1.062000  0.166667
3  1.062000  0.500000
4  0.868368 -0.500000
Variansi dijelaskan oleh PCA: [0.55368141 0.13835355]
</pre></div>
</div>
</div>
</div>
</section>
<section id="hasil-pre-processing">
<h3>Hasil Pre Processing<a class="headerlink" href="#hasil-pre-processing" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Gabungkan kembali fitur hasil normalisasi (X_minmax_df) dengan target (y)</span>
<span class="n">df_processed</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X_minmax_df</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Tampilkan jumlah total baris</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Total baris dataset setelah preprocessing:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">df_processed</span><span class="p">))</span>

<span class="c1"># Tampilkan seluruh data</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Data setelah preprocessing lengkap (fitur X1–X8 + Y1 dan Y2):</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_processed</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total baris dataset setelah preprocessing: 768

Data setelah preprocessing lengkap (fitur X1–X8 + Y1 dan Y2):
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>      X1       X2       X3       X4  X5       X6    X7  X8    Y1    Y2
1.000000 0.000000 0.285714 0.000000 1.0 0.000000 0.000 0.0 15.55 21.33
1.000000 0.000000 0.285714 0.000000 1.0 0.333333 0.000 0.0 15.55 21.33
1.000000 0.000000 0.285714 0.000000 1.0 0.666667 0.000 0.0 15.55 21.33
1.000000 0.000000 0.285714 0.000000 1.0 1.000000 0.000 0.0 15.55 21.33
0.777778 0.166667 0.428571 0.111111 1.0 0.000000 0.000 0.0 20.84 28.28
0.777778 0.166667 0.428571 0.111111 1.0 0.333333 0.000 0.0 21.46 25.38
0.777778 0.166667 0.428571 0.111111 1.0 0.666667 0.000 0.0 20.71 25.16
0.777778 0.166667 0.428571 0.111111 1.0 1.000000 0.000 0.0 19.68 29.60
0.666667 0.250000 0.285714 0.333333 1.0 0.000000 0.000 0.0 19.50 27.30
0.666667 0.250000 0.285714 0.333333 1.0 0.333333 0.000 0.0 19.95 21.97
0.666667 0.250000 0.285714 0.333333 1.0 0.666667 0.000 0.0 19.34 23.49
0.666667 0.250000 0.285714 0.333333 1.0 1.000000 0.000 0.0 18.31 27.87
0.555556 0.333333 0.428571 0.333333 1.0 0.000000 0.000 0.0 17.05 23.77
0.555556 0.333333 0.428571 0.333333 1.0 0.333333 0.000 0.0 17.41 21.46
0.555556 0.333333 0.428571 0.333333 1.0 0.666667 0.000 0.0 16.95 21.16
0.555556 0.333333 0.428571 0.333333 1.0 1.000000 0.000 0.0 15.98 24.93
0.472222 0.416667 0.571429 0.333333 1.0 0.000000 0.000 0.0 28.52 37.73
0.472222 0.416667 0.571429 0.333333 1.0 0.333333 0.000 0.0 29.90 31.27
0.472222 0.416667 0.571429 0.333333 1.0 0.666667 0.000 0.0 29.63 30.93
0.472222 0.416667 0.571429 0.333333 1.0 1.000000 0.000 0.0 28.75 39.44
0.388889 0.500000 1.000000 0.111111 1.0 0.000000 0.000 0.0 24.77 29.79
0.388889 0.500000 1.000000 0.111111 1.0 0.333333 0.000 0.0 23.93 29.68
0.388889 0.500000 1.000000 0.111111 1.0 0.666667 0.000 0.0 24.77 29.79
0.388889 0.500000 1.000000 0.111111 1.0 1.000000 0.000 0.0 23.93 29.40
0.333333 0.583333 0.000000 1.000000 0.0 0.000000 0.000 0.0  6.07 10.90
0.333333 0.583333 0.000000 1.000000 0.0 0.333333 0.000 0.0  6.05 11.19
0.333333 0.583333 0.000000 1.000000 0.0 0.666667 0.000 0.0  6.01 10.94
0.333333 0.583333 0.000000 1.000000 0.0 1.000000 0.000 0.0  6.04 11.17
0.250000 0.666667 0.142857 1.000000 0.0 0.000000 0.000 0.0  6.37 11.27
0.250000 0.666667 0.142857 1.000000 0.0 0.333333 0.000 0.0  6.40 11.72
0.250000 0.666667 0.142857 1.000000 0.0 0.666667 0.000 0.0  6.37 11.29
0.250000 0.666667 0.142857 1.000000 0.0 1.000000 0.000 0.0  6.40 11.67
0.194444 0.750000 0.285714 1.000000 0.0 0.000000 0.000 0.0  6.85 11.74
0.194444 0.750000 0.285714 1.000000 0.0 0.333333 0.000 0.0  6.79 12.05
0.194444 0.750000 0.285714 1.000000 0.0 0.666667 0.000 0.0  6.77 11.73
0.194444 0.750000 0.285714 1.000000 0.0 1.000000 0.000 0.0  6.81 11.93
0.111111 0.833333 0.428571 1.000000 0.0 0.000000 0.000 0.0  7.18 12.40
0.111111 0.833333 0.428571 1.000000 0.0 0.333333 0.000 0.0  7.10 12.23
0.111111 0.833333 0.428571 1.000000 0.0 0.666667 0.000 0.0  7.10 12.40
0.111111 0.833333 0.428571 1.000000 0.0 1.000000 0.000 0.0  7.10 12.14
0.055556 0.916667 0.571429 1.000000 0.0 0.000000 0.000 0.0 10.85 16.78
0.055556 0.916667 0.571429 1.000000 0.0 0.333333 0.000 0.0 10.54 16.80
0.055556 0.916667 0.571429 1.000000 0.0 0.666667 0.000 0.0 10.77 16.75
0.055556 0.916667 0.571429 1.000000 0.0 1.000000 0.000 0.0 10.56 16.67
0.000000 1.000000 0.714286 1.000000 0.0 0.000000 0.000 0.0  8.60 12.07
0.000000 1.000000 0.714286 1.000000 0.0 0.333333 0.000 0.0  8.49 12.22
0.000000 1.000000 0.714286 1.000000 0.0 0.666667 0.000 0.0  8.45 12.08
0.000000 1.000000 0.714286 1.000000 0.0 1.000000 0.000 0.0  8.50 12.04
1.000000 0.000000 0.285714 0.000000 1.0 0.000000 0.250 0.2 24.58 26.47
1.000000 0.000000 0.285714 0.000000 1.0 0.333333 0.250 0.2 24.63 26.37
1.000000 0.000000 0.285714 0.000000 1.0 0.666667 0.250 0.2 24.63 26.44
1.000000 0.000000 0.285714 0.000000 1.0 1.000000 0.250 0.2 24.59 26.29
0.777778 0.166667 0.428571 0.111111 1.0 0.000000 0.250 0.2 29.03 32.92
0.777778 0.166667 0.428571 0.111111 1.0 0.333333 0.250 0.2 29.87 29.87
0.777778 0.166667 0.428571 0.111111 1.0 0.666667 0.250 0.2 29.14 29.58
0.777778 0.166667 0.428571 0.111111 1.0 1.000000 0.250 0.2 28.09 34.33
0.666667 0.250000 0.285714 0.333333 1.0 0.000000 0.250 0.2 26.28 30.89
0.666667 0.250000 0.285714 0.333333 1.0 0.333333 0.250 0.2 26.91 25.60
0.666667 0.250000 0.285714 0.333333 1.0 0.666667 0.250 0.2 26.37 27.03
0.666667 0.250000 0.285714 0.333333 1.0 1.000000 0.250 0.2 25.27 31.73
0.555556 0.333333 0.428571 0.333333 1.0 0.000000 0.250 0.2 23.53 27.31
0.555556 0.333333 0.428571 0.333333 1.0 0.333333 0.250 0.2 24.03 24.91
0.555556 0.333333 0.428571 0.333333 1.0 0.666667 0.250 0.2 23.54 24.61
0.555556 0.333333 0.428571 0.333333 1.0 1.000000 0.250 0.2 22.58 28.51
0.472222 0.416667 0.571429 0.333333 1.0 0.000000 0.250 0.2 35.56 41.68
0.472222 0.416667 0.571429 0.333333 1.0 0.333333 0.250 0.2 37.12 35.28
0.472222 0.416667 0.571429 0.333333 1.0 0.666667 0.250 0.2 36.90 34.43
0.472222 0.416667 0.571429 0.333333 1.0 1.000000 0.250 0.2 35.94 43.33
0.388889 0.500000 1.000000 0.111111 1.0 0.000000 0.250 0.2 32.96 33.87
0.388889 0.500000 1.000000 0.111111 1.0 0.333333 0.250 0.2 32.12 34.07
0.388889 0.500000 1.000000 0.111111 1.0 0.666667 0.250 0.2 32.94 34.14
0.388889 0.500000 1.000000 0.111111 1.0 1.000000 0.250 0.2 32.21 33.67
0.333333 0.583333 0.000000 1.000000 0.0 0.000000 0.250 0.2 10.36 13.43
0.333333 0.583333 0.000000 1.000000 0.0 0.333333 0.250 0.2 10.43 13.71
0.333333 0.583333 0.000000 1.000000 0.0 0.666667 0.250 0.2 10.36 13.48
0.333333 0.583333 0.000000 1.000000 0.0 1.000000 0.250 0.2 10.39 13.70
0.250000 0.666667 0.142857 1.000000 0.0 0.000000 0.250 0.2 10.71 13.80
0.250000 0.666667 0.142857 1.000000 0.0 0.333333 0.250 0.2 10.80 14.28
0.250000 0.666667 0.142857 1.000000 0.0 0.666667 0.250 0.2 10.70 13.87
0.250000 0.666667 0.142857 1.000000 0.0 1.000000 0.250 0.2 10.75 14.27
0.194444 0.750000 0.285714 1.000000 0.0 0.000000 0.250 0.2 11.11 14.28
0.194444 0.750000 0.285714 1.000000 0.0 0.333333 0.250 0.2 11.13 14.61
0.194444 0.750000 0.285714 1.000000 0.0 0.666667 0.250 0.2 11.09 14.30
0.194444 0.750000 0.285714 1.000000 0.0 1.000000 0.250 0.2 11.16 14.45
0.111111 0.833333 0.428571 1.000000 0.0 0.000000 0.250 0.2 11.68 13.90
0.111111 0.833333 0.428571 1.000000 0.0 0.333333 0.250 0.2 11.69 13.72
0.111111 0.833333 0.428571 1.000000 0.0 0.666667 0.250 0.2 11.70 13.88
0.111111 0.833333 0.428571 1.000000 0.0 1.000000 0.250 0.2 11.69 13.65
0.055556 0.916667 0.571429 1.000000 0.0 0.000000 0.250 0.2 15.41 19.37
0.055556 0.916667 0.571429 1.000000 0.0 0.333333 0.250 0.2 15.20 19.43
0.055556 0.916667 0.571429 1.000000 0.0 0.666667 0.250 0.2 15.42 19.34
0.055556 0.916667 0.571429 1.000000 0.0 1.000000 0.250 0.2 15.21 19.32
0.000000 1.000000 0.714286 1.000000 0.0 0.000000 0.250 0.2 12.96 14.34
0.000000 1.000000 0.714286 1.000000 0.0 0.333333 0.250 0.2 12.97 14.50
0.000000 1.000000 0.714286 1.000000 0.0 0.666667 0.250 0.2 12.93 14.33
0.000000 1.000000 0.714286 1.000000 0.0 1.000000 0.250 0.2 13.02 14.27
1.000000 0.000000 0.285714 0.000000 1.0 0.000000 0.250 0.4 24.29 25.95
1.000000 0.000000 0.285714 0.000000 1.0 0.333333 0.250 0.4 24.31 25.63
1.000000 0.000000 0.285714 0.000000 1.0 0.666667 0.250 0.4 24.13 26.13
1.000000 0.000000 0.285714 0.000000 1.0 1.000000 0.250 0.4 24.25 25.89
0.777778 0.166667 0.428571 0.111111 1.0 0.000000 0.250 0.4 28.88 32.54
0.777778 0.166667 0.428571 0.111111 1.0 0.333333 0.250 0.4 29.68 29.44
0.777778 0.166667 0.428571 0.111111 1.0 0.666667 0.250 0.4 28.83 29.36
0.777778 0.166667 0.428571 0.111111 1.0 1.000000 0.250 0.4 27.90 34.20
0.666667 0.250000 0.285714 0.333333 1.0 0.000000 0.250 0.4 26.48 30.91
0.666667 0.250000 0.285714 0.333333 1.0 0.333333 0.250 0.4 27.02 25.63
0.666667 0.250000 0.285714 0.333333 1.0 0.666667 0.250 0.4 26.33 27.36
0.666667 0.250000 0.285714 0.333333 1.0 1.000000 0.250 0.4 25.36 31.90
0.555556 0.333333 0.428571 0.333333 1.0 0.000000 0.250 0.4 23.75 27.38
0.555556 0.333333 0.428571 0.333333 1.0 0.333333 0.250 0.4 24.23 25.02
0.555556 0.333333 0.428571 0.333333 1.0 0.666667 0.250 0.4 23.67 24.80
0.555556 0.333333 0.428571 0.333333 1.0 1.000000 0.250 0.4 22.79 28.79
0.472222 0.416667 0.571429 0.333333 1.0 0.000000 0.250 0.4 35.65 41.07
0.472222 0.416667 0.571429 0.333333 1.0 0.333333 0.250 0.4 37.26 34.62
0.472222 0.416667 0.571429 0.333333 1.0 0.666667 0.250 0.4 36.97 33.87
0.472222 0.416667 0.571429 0.333333 1.0 1.000000 0.250 0.4 36.03 42.86
0.388889 0.500000 1.000000 0.111111 1.0 0.000000 0.250 0.4 33.16 33.91
0.388889 0.500000 1.000000 0.111111 1.0 0.333333 0.250 0.4 32.40 34.07
0.388889 0.500000 1.000000 0.111111 1.0 0.666667 0.250 0.4 33.12 34.17
0.388889 0.500000 1.000000 0.111111 1.0 1.000000 0.250 0.4 32.41 33.78
0.333333 0.583333 0.000000 1.000000 0.0 0.000000 0.250 0.4 10.42 13.39
0.333333 0.583333 0.000000 1.000000 0.0 0.333333 0.250 0.4 10.46 13.72
0.333333 0.583333 0.000000 1.000000 0.0 0.666667 0.250 0.4 10.32 13.57
0.333333 0.583333 0.000000 1.000000 0.0 1.000000 0.250 0.4 10.45 13.79
0.250000 0.666667 0.142857 1.000000 0.0 0.000000 0.250 0.4 10.64 13.67
0.250000 0.666667 0.142857 1.000000 0.0 0.333333 0.250 0.4 10.72 14.11
0.250000 0.666667 0.142857 1.000000 0.0 0.666667 0.250 0.4 10.55 13.80
0.250000 0.666667 0.142857 1.000000 0.0 1.000000 0.250 0.4 10.68 14.21
0.194444 0.750000 0.285714 1.000000 0.0 0.000000 0.250 0.4 11.45 13.20
0.194444 0.750000 0.285714 1.000000 0.0 0.333333 0.250 0.4 11.46 13.54
0.194444 0.750000 0.285714 1.000000 0.0 0.666667 0.250 0.4 11.32 13.32
0.194444 0.750000 0.285714 1.000000 0.0 1.000000 0.250 0.4 11.49 13.51
0.111111 0.833333 0.428571 1.000000 0.0 0.000000 0.250 0.4 11.45 14.86
0.111111 0.833333 0.428571 1.000000 0.0 0.333333 0.250 0.4 11.42 14.75
0.111111 0.833333 0.428571 1.000000 0.0 0.666667 0.250 0.4 11.33 15.00
0.111111 0.833333 0.428571 1.000000 0.0 1.000000 0.250 0.4 11.43 14.74
0.055556 0.916667 0.571429 1.000000 0.0 0.000000 0.250 0.4 15.41 19.23
0.055556 0.916667 0.571429 1.000000 0.0 0.333333 0.250 0.4 15.18 19.34
0.055556 0.916667 0.571429 1.000000 0.0 0.666667 0.250 0.4 15.34 19.32
0.055556 0.916667 0.571429 1.000000 0.0 1.000000 0.250 0.4 15.19 19.30
0.000000 1.000000 0.714286 1.000000 0.0 0.000000 0.250 0.4 12.88 14.37
0.000000 1.000000 0.714286 1.000000 0.0 0.333333 0.250 0.4 13.00 14.57
0.000000 1.000000 0.714286 1.000000 0.0 0.666667 0.250 0.4 12.97 14.27
0.000000 1.000000 0.714286 1.000000 0.0 1.000000 0.250 0.4 13.04 14.24
1.000000 0.000000 0.285714 0.000000 1.0 0.000000 0.250 0.6 24.28 25.68
1.000000 0.000000 0.285714 0.000000 1.0 0.333333 0.250 0.6 24.40 26.02
1.000000 0.000000 0.285714 0.000000 1.0 0.666667 0.250 0.6 24.11 25.84
1.000000 0.000000 0.285714 0.000000 1.0 1.000000 0.250 0.6 24.35 26.14
0.777778 0.166667 0.428571 0.111111 1.0 0.000000 0.250 0.6 28.07 34.14
0.777778 0.166667 0.428571 0.111111 1.0 0.333333 0.250 0.6 29.01 32.85
0.777778 0.166667 0.428571 0.111111 1.0 0.666667 0.250 0.6 29.62 30.08
0.777778 0.166667 0.428571 0.111111 1.0 1.000000 0.250 0.6 29.05 29.67
0.666667 0.250000 0.285714 0.333333 1.0 0.000000 0.250 0.6 25.41 31.73
0.666667 0.250000 0.285714 0.333333 1.0 0.333333 0.250 0.6 26.47 31.01
0.666667 0.250000 0.285714 0.333333 1.0 0.666667 0.250 0.6 26.89 25.90
0.666667 0.250000 0.285714 0.333333 1.0 1.000000 0.250 0.6 26.46 27.40
0.555556 0.333333 0.428571 0.333333 1.0 0.000000 0.250 0.6 22.93 28.68
0.555556 0.333333 0.428571 0.333333 1.0 0.333333 0.250 0.6 23.84 27.54
0.555556 0.333333 0.428571 0.333333 1.0 0.666667 0.250 0.6 24.17 25.35
0.555556 0.333333 0.428571 0.333333 1.0 1.000000 0.250 0.6 23.87 24.93
0.472222 0.416667 0.571429 0.333333 1.0 0.000000 0.250 0.6 35.78 43.12
0.472222 0.416667 0.571429 0.333333 1.0 0.333333 0.250 0.6 35.48 41.22
0.472222 0.416667 0.571429 0.333333 1.0 0.666667 0.250 0.6 36.97 35.10
0.472222 0.416667 0.571429 0.333333 1.0 1.000000 0.250 0.6 36.70 34.29
0.388889 0.500000 1.000000 0.111111 1.0 0.000000 0.250 0.6 32.52 33.85
0.388889 0.500000 1.000000 0.111111 1.0 0.333333 0.250 0.6 33.28 34.11
0.388889 0.500000 1.000000 0.111111 1.0 0.666667 0.250 0.6 32.33 34.48
0.388889 0.500000 1.000000 0.111111 1.0 1.000000 0.250 0.6 33.24 34.50
0.333333 0.583333 0.000000 1.000000 0.0 0.000000 0.250 0.6 10.39 13.60
0.333333 0.583333 0.000000 1.000000 0.0 0.333333 0.250 0.6 10.34 13.36
0.333333 0.583333 0.000000 1.000000 0.0 0.666667 0.250 0.6 10.35 13.65
0.333333 0.583333 0.000000 1.000000 0.0 1.000000 0.250 0.6 10.38 13.49
0.250000 0.666667 0.142857 1.000000 0.0 0.000000 0.250 0.6 10.77 14.14
0.250000 0.666667 0.142857 1.000000 0.0 0.333333 0.250 0.6 10.68 13.77
0.250000 0.666667 0.142857 1.000000 0.0 0.666667 0.250 0.6 10.68 14.30
0.250000 0.666667 0.142857 1.000000 0.0 1.000000 0.250 0.6 10.70 13.87
0.194444 0.750000 0.285714 1.000000 0.0 0.000000 0.250 0.6 11.22 14.44
0.194444 0.750000 0.285714 1.000000 0.0 0.333333 0.250 0.6 11.16 14.27
0.194444 0.750000 0.285714 1.000000 0.0 0.666667 0.250 0.6 11.10 14.67
0.194444 0.750000 0.285714 1.000000 0.0 1.000000 0.250 0.6 11.14 14.40
0.111111 0.833333 0.428571 1.000000 0.0 0.000000 0.250 0.6 11.59 13.46
0.111111 0.833333 0.428571 1.000000 0.0 0.333333 0.250 0.6 11.60 13.70
0.111111 0.833333 0.428571 1.000000 0.0 0.666667 0.250 0.6 11.53 13.59
0.111111 0.833333 0.428571 1.000000 0.0 1.000000 0.250 0.6 11.61 13.83
0.055556 0.916667 0.571429 1.000000 0.0 0.000000 0.250 0.6 15.16 19.14
0.055556 0.916667 0.571429 1.000000 0.0 0.333333 0.250 0.6 15.36 19.18
0.055556 0.916667 0.571429 1.000000 0.0 0.666667 0.250 0.6 15.12 19.37
0.055556 0.916667 0.571429 1.000000 0.0 1.000000 0.250 0.6 15.36 19.29
0.000000 1.000000 0.714286 1.000000 0.0 0.000000 0.250 0.6 12.68 14.09
0.000000 1.000000 0.714286 1.000000 0.0 0.333333 0.250 0.6 12.63 14.23
0.000000 1.000000 0.714286 1.000000 0.0 0.666667 0.250 0.6 12.71 14.14
0.000000 1.000000 0.714286 1.000000 0.0 1.000000 0.250 0.6 12.73 13.89
1.000000 0.000000 0.285714 0.000000 1.0 0.000000 0.250 0.8 24.38 25.91
1.000000 0.000000 0.285714 0.000000 1.0 0.333333 0.250 0.8 24.23 25.72
1.000000 0.000000 0.285714 0.000000 1.0 0.666667 0.250 0.8 24.04 26.18
1.000000 0.000000 0.285714 0.000000 1.0 1.000000 0.250 0.8 24.32 25.87
0.777778 0.166667 0.428571 0.111111 1.0 0.000000 0.250 0.8 29.06 29.34
0.777778 0.166667 0.428571 0.111111 1.0 0.333333 0.250 0.8 28.05 33.91
0.777778 0.166667 0.428571 0.111111 1.0 0.666667 0.250 0.8 28.86 32.83
0.777778 0.166667 0.428571 0.111111 1.0 1.000000 0.250 0.8 29.79 29.92
0.666667 0.250000 0.285714 0.333333 1.0 0.000000 0.250 0.8 26.44 27.17
0.666667 0.250000 0.285714 0.333333 1.0 0.333333 0.250 0.8 25.37 31.76
0.666667 0.250000 0.285714 0.333333 1.0 0.666667 0.250 0.8 26.33 31.06
0.666667 0.250000 0.285714 0.333333 1.0 1.000000 0.250 0.8 27.03 25.81
0.555556 0.333333 0.428571 0.333333 1.0 0.000000 0.250 0.8 23.80 24.61
0.555556 0.333333 0.428571 0.333333 1.0 0.333333 0.250 0.8 22.80 28.61
0.555556 0.333333 0.428571 0.333333 1.0 0.666667 0.250 0.8 23.59 27.57
0.555556 0.333333 0.428571 0.333333 1.0 1.000000 0.250 0.8 24.24 25.16
0.472222 0.416667 0.571429 0.333333 1.0 0.000000 0.250 0.8 36.86 34.25
0.472222 0.416667 0.571429 0.333333 1.0 0.333333 0.250 0.8 35.89 43.30
0.472222 0.416667 0.571429 0.333333 1.0 0.666667 0.250 0.8 35.45 41.86
0.472222 0.416667 0.571429 0.333333 1.0 1.000000 0.250 0.8 37.10 35.29
0.388889 0.500000 1.000000 0.111111 1.0 0.000000 0.250 0.8 33.08 34.11
0.388889 0.500000 1.000000 0.111111 1.0 0.333333 0.250 0.8 32.38 33.62
0.388889 0.500000 1.000000 0.111111 1.0 0.666667 0.250 0.8 33.09 33.89
0.388889 0.500000 1.000000 0.111111 1.0 1.000000 0.250 0.8 32.31 34.05
0.333333 0.583333 0.000000 1.000000 0.0 0.000000 0.250 0.8 10.08 13.20
0.333333 0.583333 0.000000 1.000000 0.0 0.333333 0.250 0.8 10.15 13.36
0.333333 0.583333 0.000000 1.000000 0.0 0.666667 0.250 0.8 10.07 13.21
0.333333 0.583333 0.000000 1.000000 0.0 1.000000 0.250 0.8 10.14 13.53
0.250000 0.666667 0.142857 1.000000 0.0 0.000000 0.250 0.8 10.66 13.67
0.250000 0.666667 0.142857 1.000000 0.0 0.333333 0.250 0.8 10.68 14.12
0.250000 0.666667 0.142857 1.000000 0.0 0.666667 0.250 0.8 10.53 13.79
0.250000 0.666667 0.142857 1.000000 0.0 1.000000 0.250 0.8 10.72 14.20
0.194444 0.750000 0.285714 1.000000 0.0 0.000000 0.250 0.8 11.18 14.29
0.194444 0.750000 0.285714 1.000000 0.0 0.333333 0.250 0.8 11.22 14.49
0.194444 0.750000 0.285714 1.000000 0.0 0.666667 0.250 0.8 11.07 14.42
0.194444 0.750000 0.285714 1.000000 0.0 1.000000 0.250 0.8 11.20 14.73
0.111111 0.833333 0.428571 1.000000 0.0 0.000000 0.250 0.8 11.44 14.86
0.111111 0.833333 0.428571 1.000000 0.0 0.333333 0.250 0.8 11.42 14.67
0.111111 0.833333 0.428571 1.000000 0.0 0.666667 0.250 0.8 11.33 15.00
0.111111 0.833333 0.428571 1.000000 0.0 1.000000 0.250 0.8 11.43 14.83
0.055556 0.916667 0.571429 1.000000 0.0 0.000000 0.250 0.8 15.40 19.24
0.055556 0.916667 0.571429 1.000000 0.0 0.333333 0.250 0.8 15.19 19.25
0.055556 0.916667 0.571429 1.000000 0.0 0.666667 0.250 0.8 15.32 19.42
0.055556 0.916667 0.571429 1.000000 0.0 1.000000 0.250 0.8 15.16 19.48
0.000000 1.000000 0.714286 1.000000 0.0 0.000000 0.250 0.8 12.85 14.37
0.000000 1.000000 0.714286 1.000000 0.0 0.333333 0.250 0.8 13.04 14.34
0.000000 1.000000 0.714286 1.000000 0.0 0.666667 0.250 0.8 13.00 14.28
0.000000 1.000000 0.714286 1.000000 0.0 1.000000 0.250 0.8 13.00 14.47
1.000000 0.000000 0.285714 0.000000 1.0 0.000000 0.250 1.0 24.35 25.64
1.000000 0.000000 0.285714 0.000000 1.0 0.333333 0.250 1.0 24.33 25.98
1.000000 0.000000 0.285714 0.000000 1.0 0.666667 0.250 1.0 24.03 25.88
1.000000 0.000000 0.285714 0.000000 1.0 1.000000 0.250 1.0 24.26 26.18
0.777778 0.166667 0.428571 0.111111 1.0 0.000000 0.250 1.0 29.83 29.82
0.777778 0.166667 0.428571 0.111111 1.0 0.333333 0.250 1.0 29.08 29.52
0.777778 0.166667 0.428571 0.111111 1.0 0.666667 0.250 1.0 28.03 34.45
0.777778 0.166667 0.428571 0.111111 1.0 1.000000 0.250 1.0 29.02 33.01
0.666667 0.250000 0.285714 0.333333 1.0 0.000000 0.250 1.0 27.03 25.82
0.666667 0.250000 0.285714 0.333333 1.0 0.333333 0.250 1.0 26.45 27.33
0.666667 0.250000 0.285714 0.333333 1.0 0.666667 0.250 1.0 25.36 32.04
0.666667 0.250000 0.285714 0.333333 1.0 1.000000 0.250 1.0 26.45 31.28
0.555556 0.333333 0.428571 0.333333 1.0 0.000000 0.250 1.0 24.37 25.11
0.555556 0.333333 0.428571 0.333333 1.0 0.333333 0.250 1.0 23.89 24.77
0.555556 0.333333 0.428571 0.333333 1.0 0.666667 0.250 1.0 22.89 28.88
0.555556 0.333333 0.428571 0.333333 1.0 1.000000 0.250 1.0 23.86 27.69
0.472222 0.416667 0.571429 0.333333 1.0 0.000000 0.250 1.0 37.03 34.99
0.472222 0.416667 0.571429 0.333333 1.0 0.333333 0.250 1.0 36.71 34.18
0.472222 0.416667 0.571429 0.333333 1.0 0.666667 0.250 1.0 36.77 43.14
0.472222 0.416667 0.571429 0.333333 1.0 1.000000 0.250 1.0 35.48 41.26
0.388889 0.500000 1.000000 0.111111 1.0 0.000000 0.250 1.0 32.31 34.25
0.388889 0.500000 1.000000 0.111111 1.0 0.333333 0.250 1.0 33.21 34.35
0.388889 0.500000 1.000000 0.111111 1.0 0.666667 0.250 1.0 32.46 33.64
0.388889 0.500000 1.000000 0.111111 1.0 1.000000 0.250 1.0 33.27 33.88
0.333333 0.583333 0.000000 1.000000 0.0 0.000000 0.250 1.0 10.47 13.65
0.333333 0.583333 0.000000 1.000000 0.0 0.333333 0.250 1.0 10.37 13.44
0.333333 0.583333 0.000000 1.000000 0.0 0.666667 0.250 1.0 10.34 13.72
0.333333 0.583333 0.000000 1.000000 0.0 1.000000 0.250 1.0 10.39 13.50
0.250000 0.666667 0.142857 1.000000 0.0 0.000000 0.250 1.0 10.78 14.18
0.250000 0.666667 0.142857 1.000000 0.0 0.333333 0.250 1.0 10.70 13.75
0.250000 0.666667 0.142857 1.000000 0.0 0.666667 0.250 1.0 10.67 14.26
0.250000 0.666667 0.142857 1.000000 0.0 1.000000 0.250 1.0 13.69 13.89
0.194444 0.750000 0.285714 1.000000 0.0 0.000000 0.250 1.0 11.21 14.55
0.194444 0.750000 0.285714 1.000000 0.0 0.333333 0.250 1.0 11.14 14.28
0.194444 0.750000 0.285714 1.000000 0.0 0.666667 0.250 1.0 11.11 14.46
0.194444 0.750000 0.285714 1.000000 0.0 1.000000 0.250 1.0 11.16 14.39
0.111111 0.833333 0.428571 1.000000 0.0 0.000000 0.250 1.0 11.38 14.54
0.111111 0.833333 0.428571 1.000000 0.0 0.333333 0.250 1.0 11.34 14.81
0.111111 0.833333 0.428571 1.000000 0.0 0.666667 0.250 1.0 11.22 14.65
0.111111 0.833333 0.428571 1.000000 0.0 1.000000 0.250 1.0 11.34 14.87
0.055556 0.916667 0.571429 1.000000 0.0 0.000000 0.250 1.0 15.16 19.24
0.055556 0.916667 0.571429 1.000000 0.0 0.333333 0.250 1.0 15.37 19.18
0.055556 0.916667 0.571429 1.000000 0.0 0.666667 0.250 1.0 15.12 19.26
0.055556 0.916667 0.571429 1.000000 0.0 1.000000 0.250 1.0 15.36 19.29
0.000000 1.000000 0.714286 1.000000 0.0 0.000000 0.250 1.0 12.59 14.24
0.000000 1.000000 0.714286 1.000000 0.0 0.333333 0.250 1.0 12.74 13.97
0.000000 1.000000 0.714286 1.000000 0.0 0.666667 0.250 1.0 12.80 13.99
0.000000 1.000000 0.714286 1.000000 0.0 1.000000 0.250 1.0 12.62 14.15
1.000000 0.000000 0.285714 0.000000 1.0 0.000000 0.625 0.2 28.15 29.79
1.000000 0.000000 0.285714 0.000000 1.0 0.333333 0.625 0.2 28.15 29.79
1.000000 0.000000 0.285714 0.000000 1.0 0.666667 0.625 0.2 28.37 29.28
1.000000 0.000000 0.285714 0.000000 1.0 1.000000 0.625 0.2 28.41 29.49
0.777778 0.166667 0.428571 0.111111 1.0 0.000000 0.625 0.2 32.68 36.12
0.777778 0.166667 0.428571 0.111111 1.0 0.333333 0.625 0.2 33.48 33.17
0.777778 0.166667 0.428571 0.111111 1.0 0.666667 0.625 0.2 32.84 32.71
0.777778 0.166667 0.428571 0.111111 1.0 1.000000 0.625 0.2 32.00 37.58
0.666667 0.250000 0.285714 0.333333 1.0 0.000000 0.625 0.2 29.54 33.98
0.666667 0.250000 0.285714 0.333333 1.0 0.333333 0.625 0.2 30.05 28.61
0.666667 0.250000 0.285714 0.333333 1.0 0.666667 0.625 0.2 29.60 30.12
0.666667 0.250000 0.285714 0.333333 1.0 1.000000 0.625 0.2 28.66 34.73
0.555556 0.333333 0.428571 0.333333 1.0 0.000000 0.625 0.2 26.84 30.17
0.555556 0.333333 0.428571 0.333333 1.0 0.333333 0.625 0.2 27.27 27.84
0.555556 0.333333 0.428571 0.333333 1.0 0.666667 0.625 0.2 26.97 27.25
0.555556 0.333333 0.428571 0.333333 1.0 1.000000 0.625 0.2 26.19 31.39
0.472222 0.416667 0.571429 0.333333 1.0 0.000000 0.625 0.2 38.67 43.80
0.472222 0.416667 0.571429 0.333333 1.0 0.333333 0.625 0.2 40.03 37.81
0.472222 0.416667 0.571429 0.333333 1.0 0.666667 0.625 0.2 39.86 36.85
0.472222 0.416667 0.571429 0.333333 1.0 1.000000 0.625 0.2 39.04 45.52
0.388889 0.500000 1.000000 0.111111 1.0 0.000000 0.625 0.2 36.96 36.85
0.388889 0.500000 1.000000 0.111111 1.0 0.333333 0.625 0.2 36.13 37.58
0.388889 0.500000 1.000000 0.111111 1.0 0.666667 0.625 0.2 36.91 37.45
0.388889 0.500000 1.000000 0.111111 1.0 1.000000 0.625 0.2 36.43 36.62
0.333333 0.583333 0.000000 1.000000 0.0 0.000000 0.625 0.2 12.43 15.19
0.333333 0.583333 0.000000 1.000000 0.0 0.333333 0.625 0.2 12.50 15.50
0.333333 0.583333 0.000000 1.000000 0.0 0.666667 0.625 0.2 12.41 15.28
0.333333 0.583333 0.000000 1.000000 0.0 1.000000 0.625 0.2 12.45 15.50
0.250000 0.666667 0.142857 1.000000 0.0 0.000000 0.625 0.2 12.57 15.42
0.250000 0.666667 0.142857 1.000000 0.0 0.333333 0.625 0.2 12.65 15.85
0.250000 0.666667 0.142857 1.000000 0.0 0.666667 0.625 0.2 12.57 15.44
0.250000 0.666667 0.142857 1.000000 0.0 1.000000 0.625 0.2 12.63 15.81
0.194444 0.750000 0.285714 1.000000 0.0 0.000000 0.625 0.2 12.78 15.21
0.194444 0.750000 0.285714 1.000000 0.0 0.333333 0.625 0.2 12.93 15.63
0.194444 0.750000 0.285714 1.000000 0.0 0.666667 0.625 0.2 12.73 15.48
0.194444 0.750000 0.285714 1.000000 0.0 1.000000 0.625 0.2 12.72 15.78
0.111111 0.833333 0.428571 1.000000 0.0 0.000000 0.625 0.2 13.17 16.39
0.111111 0.833333 0.428571 1.000000 0.0 0.333333 0.625 0.2 13.18 16.27
0.111111 0.833333 0.428571 1.000000 0.0 0.666667 0.625 0.2 13.17 16.39
0.111111 0.833333 0.428571 1.000000 0.0 1.000000 0.625 0.2 13.18 16.19
0.055556 0.916667 0.571429 1.000000 0.0 0.000000 0.625 0.2 17.50 21.13
0.055556 0.916667 0.571429 1.000000 0.0 0.333333 0.625 0.2 17.35 21.19
0.055556 0.916667 0.571429 1.000000 0.0 0.666667 0.625 0.2 17.52 21.09
0.055556 0.916667 0.571429 1.000000 0.0 1.000000 0.625 0.2 17.37 21.08
0.000000 1.000000 0.714286 1.000000 0.0 0.000000 0.625 0.2 15.09 15.77
0.000000 1.000000 0.714286 1.000000 0.0 0.333333 0.625 0.2 15.12 15.95
0.000000 1.000000 0.714286 1.000000 0.0 0.666667 0.625 0.2 15.08 15.77
0.000000 1.000000 0.714286 1.000000 0.0 1.000000 0.625 0.2 15.16 15.76
1.000000 0.000000 0.285714 0.000000 1.0 0.000000 0.625 0.4 28.67 29.62
1.000000 0.000000 0.285714 0.000000 1.0 0.333333 0.625 0.4 28.57 29.69
1.000000 0.000000 0.285714 0.000000 1.0 0.666667 0.625 0.4 28.18 30.18
1.000000 0.000000 0.285714 0.000000 1.0 1.000000 0.625 0.4 28.60 30.02
0.777778 0.166667 0.428571 0.111111 1.0 0.000000 0.625 0.4 32.46 35.56
0.777778 0.166667 0.428571 0.111111 1.0 0.333333 0.625 0.4 33.27 32.64
0.777778 0.166667 0.428571 0.111111 1.0 0.666667 0.625 0.4 32.33 32.77
0.777778 0.166667 0.428571 0.111111 1.0 1.000000 0.625 0.4 31.66 37.72
0.666667 0.250000 0.285714 0.333333 1.0 0.000000 0.625 0.4 29.34 33.37
0.666667 0.250000 0.285714 0.333333 1.0 0.333333 0.625 0.4 29.87 27.89
0.666667 0.250000 0.285714 0.333333 1.0 0.666667 0.625 0.4 29.27 29.90
0.666667 0.250000 0.285714 0.333333 1.0 1.000000 0.625 0.4 28.40 34.52
0.555556 0.333333 0.428571 0.333333 1.0 0.000000 0.625 0.4 25.74 28.27
0.555556 0.333333 0.428571 0.333333 1.0 0.333333 0.625 0.4 25.98 26.96
0.555556 0.333333 0.428571 0.333333 1.0 0.666667 0.625 0.4 25.38 26.72
0.555556 0.333333 0.428571 0.333333 1.0 1.000000 0.625 0.4 24.94 29.88
0.472222 0.416667 0.571429 0.333333 1.0 0.000000 0.625 0.4 38.57 43.86
0.472222 0.416667 0.571429 0.333333 1.0 0.333333 0.625 0.4 40.19 37.41
0.472222 0.416667 0.571429 0.333333 1.0 0.666667 0.625 0.4 39.97 36.77
0.472222 0.416667 0.571429 0.333333 1.0 1.000000 0.625 0.4 38.98 45.97
0.388889 0.500000 1.000000 0.111111 1.0 0.000000 0.625 0.4 36.95 36.87
0.388889 0.500000 1.000000 0.111111 1.0 0.333333 0.625 0.4 36.28 37.35
0.388889 0.500000 1.000000 0.111111 1.0 0.666667 0.625 0.4 36.86 37.28
0.388889 0.500000 1.000000 0.111111 1.0 1.000000 0.625 0.4 36.45 36.81
0.333333 0.583333 0.000000 1.000000 0.0 0.000000 0.625 0.4 12.35 14.73
0.333333 0.583333 0.000000 1.000000 0.0 0.333333 0.625 0.4 12.45 15.10
0.333333 0.583333 0.000000 1.000000 0.0 0.666667 0.625 0.4 12.16 15.18
0.333333 0.583333 0.000000 1.000000 0.0 1.000000 0.625 0.4 12.30 15.44
0.250000 0.666667 0.142857 1.000000 0.0 0.000000 0.625 0.4 12.33 14.91
0.250000 0.666667 0.142857 1.000000 0.0 0.333333 0.625 0.4 12.29 15.40
0.250000 0.666667 0.142857 1.000000 0.0 0.666667 0.625 0.4 12.20 14.94
0.250000 0.666667 0.142857 1.000000 0.0 1.000000 0.625 0.4 12.49 15.32
0.194444 0.750000 0.285714 1.000000 0.0 0.000000 0.625 0.4 12.85 15.52
0.194444 0.750000 0.285714 1.000000 0.0 0.333333 0.625 0.4 12.87 15.85
0.194444 0.750000 0.285714 1.000000 0.0 0.666667 0.625 0.4 12.73 15.66
0.194444 0.750000 0.285714 1.000000 0.0 1.000000 0.625 0.4 12.95 15.99
0.111111 0.833333 0.428571 1.000000 0.0 0.000000 0.625 0.4 13.05 15.89
0.111111 0.833333 0.428571 1.000000 0.0 0.333333 0.625 0.4 12.93 15.85
0.111111 0.833333 0.428571 1.000000 0.0 0.666667 0.625 0.4 12.77 16.22
0.111111 0.833333 0.428571 1.000000 0.0 1.000000 0.625 0.4 13.00 15.87
0.055556 0.916667 0.571429 1.000000 0.0 0.000000 0.625 0.4 17.14 20.47
0.055556 0.916667 0.571429 1.000000 0.0 0.333333 0.625 0.4 16.84 20.56
0.055556 0.916667 0.571429 1.000000 0.0 0.666667 0.625 0.4 17.02 20.48
0.055556 0.916667 0.571429 1.000000 0.0 1.000000 0.625 0.4 17.11 20.43
0.000000 1.000000 0.714286 1.000000 0.0 0.000000 0.625 0.4 14.34 15.32
0.000000 1.000000 0.714286 1.000000 0.0 0.333333 0.625 0.4 14.66 15.64
0.000000 1.000000 0.714286 1.000000 0.0 0.666667 0.625 0.4 14.60 15.14
0.000000 1.000000 0.714286 1.000000 0.0 1.000000 0.625 0.4 14.60 15.30
1.000000 0.000000 0.285714 0.000000 1.0 0.000000 0.625 0.6 28.67 29.43
1.000000 0.000000 0.285714 0.000000 1.0 0.333333 0.625 0.6 28.56 29.78
1.000000 0.000000 0.285714 0.000000 1.0 0.666667 0.625 0.6 28.17 30.10
1.000000 0.000000 0.285714 0.000000 1.0 1.000000 0.625 0.6 28.63 30.19
0.777778 0.166667 0.428571 0.111111 1.0 0.000000 0.625 0.6 31.63 36.35
0.777778 0.166667 0.428571 0.111111 1.0 0.333333 0.625 0.6 32.40 35.10
0.777778 0.166667 0.428571 0.111111 1.0 0.666667 0.625 0.6 32.68 32.83
0.777778 0.166667 0.428571 0.111111 1.0 1.000000 0.625 0.6 32.29 32.46
0.666667 0.250000 0.285714 0.333333 1.0 0.000000 0.625 0.6 28.40 33.52
0.666667 0.250000 0.285714 0.333333 1.0 0.333333 0.625 0.6 29.40 32.93
0.666667 0.250000 0.285714 0.333333 1.0 0.666667 0.625 0.6 29.43 28.38
0.666667 0.250000 0.285714 0.333333 1.0 1.000000 0.625 0.6 29.07 29.82
0.555556 0.333333 0.428571 0.333333 1.0 0.000000 0.625 0.6 24.70 28.77
0.555556 0.333333 0.428571 0.333333 1.0 0.333333 0.625 0.6 25.48 27.76
0.555556 0.333333 0.428571 0.333333 1.0 0.666667 0.625 0.6 25.37 26.95
0.555556 0.333333 0.428571 0.333333 1.0 1.000000 0.625 0.6 25.17 26.41
0.472222 0.416667 0.571429 0.333333 1.0 0.000000 0.625 0.6 39.04 45.13
0.472222 0.416667 0.571429 0.333333 1.0 0.333333 0.625 0.6 38.35 43.66
0.472222 0.416667 0.571429 0.333333 1.0 0.666667 0.625 0.6 39.81 37.76
0.472222 0.416667 0.571429 0.333333 1.0 1.000000 0.625 0.6 39.83 36.87
0.388889 0.500000 1.000000 0.111111 1.0 0.000000 0.625 0.6 35.99 36.07
0.388889 0.500000 1.000000 0.111111 1.0 0.333333 0.625 0.6 36.59 36.44
0.388889 0.500000 1.000000 0.111111 1.0 0.666667 0.625 0.6 35.64 37.28
0.388889 0.500000 1.000000 0.111111 1.0 1.000000 0.625 0.6 36.52 37.29
0.333333 0.583333 0.000000 1.000000 0.0 0.000000 0.625 0.6 11.80 14.49
0.333333 0.583333 0.000000 1.000000 0.0 0.333333 0.625 0.6 12.03 13.79
0.333333 0.583333 0.000000 1.000000 0.0 0.666667 0.625 0.6 11.98 14.72
0.333333 0.583333 0.000000 1.000000 0.0 1.000000 0.625 0.6 11.69 14.76
0.250000 0.666667 0.142857 1.000000 0.0 0.000000 0.625 0.6 12.41 14.92
0.250000 0.666667 0.142857 1.000000 0.0 0.333333 0.625 0.6 12.28 14.74
0.250000 0.666667 0.142857 1.000000 0.0 0.666667 0.625 0.6 12.10 15.57
0.250000 0.666667 0.142857 1.000000 0.0 1.000000 0.625 0.6 12.19 14.94
0.194444 0.750000 0.285714 1.000000 0.0 0.000000 0.625 0.6 12.34 14.92
0.194444 0.750000 0.285714 1.000000 0.0 0.333333 0.625 0.6 12.46 14.38
0.194444 0.750000 0.285714 1.000000 0.0 0.666667 0.625 0.6 12.31 15.44
0.194444 0.750000 0.285714 1.000000 0.0 1.000000 0.625 0.6 12.12 15.17
0.111111 0.833333 0.428571 1.000000 0.0 0.000000 0.625 0.6 12.97 15.53
0.111111 0.833333 0.428571 1.000000 0.0 0.333333 0.625 0.6 13.01 15.80
0.111111 0.833333 0.428571 1.000000 0.0 0.666667 0.625 0.6 12.74 16.14
0.111111 0.833333 0.428571 1.000000 0.0 1.000000 0.625 0.6 12.84 16.26
0.055556 0.916667 0.571429 1.000000 0.0 0.000000 0.625 0.6 16.83 19.87
0.055556 0.916667 0.571429 1.000000 0.0 0.333333 0.625 0.6 16.93 20.03
0.055556 0.916667 0.571429 1.000000 0.0 0.666667 0.625 0.6 16.66 20.46
0.055556 0.916667 0.571429 1.000000 0.0 1.000000 0.625 0.6 16.86 20.28
0.000000 1.000000 0.714286 1.000000 0.0 0.000000 0.625 0.6 13.91 14.89
0.000000 1.000000 0.714286 1.000000 0.0 0.333333 0.625 0.6 14.34 14.96
0.000000 1.000000 0.714286 1.000000 0.0 0.666667 0.625 0.6 13.95 14.89
0.000000 1.000000 0.714286 1.000000 0.0 1.000000 0.625 0.6 13.99 14.35
1.000000 0.000000 0.285714 0.000000 1.0 0.000000 0.625 0.8 28.70 29.61
1.000000 0.000000 0.285714 0.000000 1.0 0.333333 0.625 0.8 28.55 29.59
1.000000 0.000000 0.285714 0.000000 1.0 0.666667 0.625 0.8 28.15 30.19
1.000000 0.000000 0.285714 0.000000 1.0 1.000000 0.625 0.8 28.62 30.12
0.777778 0.166667 0.428571 0.111111 1.0 0.000000 0.625 0.8 32.67 32.12
0.777778 0.166667 0.428571 0.111111 1.0 0.333333 0.625 0.8 31.69 37.12
0.777778 0.166667 0.428571 0.111111 1.0 0.666667 0.625 0.8 32.07 36.16
0.777778 0.166667 0.428571 0.111111 1.0 1.000000 0.625 0.8 33.28 33.16
0.666667 0.250000 0.285714 0.333333 1.0 0.000000 0.625 0.8 29.47 29.45
0.666667 0.250000 0.285714 0.333333 1.0 0.333333 0.625 0.8 28.42 34.19
0.666667 0.250000 0.285714 0.333333 1.0 0.666667 0.625 0.8 29.08 33.93
0.666667 0.250000 0.285714 0.333333 1.0 1.000000 0.625 0.8 29.88 28.31
0.555556 0.333333 0.428571 0.333333 1.0 0.000000 0.625 0.8 25.66 26.30
0.555556 0.333333 0.428571 0.333333 1.0 0.333333 0.625 0.8 24.96 29.43
0.555556 0.333333 0.428571 0.333333 1.0 0.666667 0.625 0.8 25.43 28.76
0.555556 0.333333 0.428571 0.333333 1.0 1.000000 0.625 0.8 26.00 27.34
0.472222 0.416667 0.571429 0.333333 1.0 0.000000 0.625 0.8 40.00 36.26
0.472222 0.416667 0.571429 0.333333 1.0 0.333333 0.625 0.8 38.84 45.48
0.472222 0.416667 0.571429 0.333333 1.0 0.666667 0.625 0.8 38.33 44.16
0.472222 0.416667 0.571429 0.333333 1.0 1.000000 0.625 0.8 40.12 37.26
0.388889 0.500000 1.000000 0.111111 1.0 0.000000 0.625 0.8 36.95 37.20
0.388889 0.500000 1.000000 0.111111 1.0 0.333333 0.625 0.8 36.45 36.76
0.388889 0.500000 1.000000 0.111111 1.0 0.666667 0.625 0.8 36.81 37.05
0.388889 0.500000 1.000000 0.111111 1.0 1.000000 0.625 0.8 36.26 37.51
0.333333 0.583333 0.000000 1.000000 0.0 0.000000 0.625 0.8 12.32 14.92
0.333333 0.583333 0.000000 1.000000 0.0 0.333333 0.625 0.8 12.30 15.24
0.333333 0.583333 0.000000 1.000000 0.0 0.666667 0.625 0.8 12.18 15.03
0.333333 0.583333 0.000000 1.000000 0.0 1.000000 0.625 0.8 12.43 15.35
0.250000 0.666667 0.142857 1.000000 0.0 0.000000 0.625 0.8 12.36 14.67
0.250000 0.666667 0.142857 1.000000 0.0 0.333333 0.625 0.8 12.49 15.09
0.250000 0.666667 0.142857 1.000000 0.0 0.666667 0.625 0.8 12.17 15.20
0.250000 0.666667 0.142857 1.000000 0.0 1.000000 0.625 0.8 12.28 15.64
0.194444 0.750000 0.285714 1.000000 0.0 0.000000 0.625 0.8 12.91 15.37
0.194444 0.750000 0.285714 1.000000 0.0 0.333333 0.625 0.8 12.95 15.73
0.194444 0.750000 0.285714 1.000000 0.0 0.666667 0.625 0.8 12.67 15.83
0.194444 0.750000 0.285714 1.000000 0.0 1.000000 0.625 0.8 12.86 16.13
0.111111 0.833333 0.428571 1.000000 0.0 0.000000 0.625 0.8 12.95 15.95
0.111111 0.833333 0.428571 1.000000 0.0 0.333333 0.625 0.8 13.00 15.59
0.111111 0.833333 0.428571 1.000000 0.0 0.666667 0.625 0.8 12.86 16.17
0.111111 0.833333 0.428571 1.000000 0.0 1.000000 0.625 0.8 12.92 16.14
0.055556 0.916667 0.571429 1.000000 0.0 0.000000 0.625 0.8 16.99 19.65
0.055556 0.916667 0.571429 1.000000 0.0 0.333333 0.625 0.8 16.69 19.76
0.055556 0.916667 0.571429 1.000000 0.0 0.666667 0.625 0.8 16.56 20.37
0.055556 0.916667 0.571429 1.000000 0.0 1.000000 0.625 0.8 16.62 19.90
0.000000 1.000000 0.714286 1.000000 0.0 0.000000 0.625 0.8 14.33 15.41
0.000000 1.000000 0.714286 1.000000 0.0 0.333333 0.625 0.8 14.61 15.56
0.000000 1.000000 0.714286 1.000000 0.0 0.666667 0.625 0.8 14.61 15.07
0.000000 1.000000 0.714286 1.000000 0.0 1.000000 0.625 0.8 14.65 15.38
1.000000 0.000000 0.285714 0.000000 1.0 0.000000 0.625 1.0 28.69 29.53
1.000000 0.000000 0.285714 0.000000 1.0 0.333333 0.625 1.0 28.58 29.77
1.000000 0.000000 0.285714 0.000000 1.0 0.666667 0.625 1.0 28.15 30.00
1.000000 0.000000 0.285714 0.000000 1.0 1.000000 0.625 1.0 28.61 30.20
0.777778 0.166667 0.428571 0.111111 1.0 0.000000 0.625 1.0 33.13 32.25
0.777778 0.166667 0.428571 0.111111 1.0 0.333333 0.625 1.0 32.31 32.00
0.777778 0.166667 0.428571 0.111111 1.0 0.666667 0.625 1.0 31.53 37.19
0.777778 0.166667 0.428571 0.111111 1.0 1.000000 0.625 1.0 32.46 35.62
0.666667 0.250000 0.285714 0.333333 1.0 0.000000 0.625 1.0 29.71 28.02
0.666667 0.250000 0.285714 0.333333 1.0 0.333333 0.625 1.0 29.09 29.43
0.666667 0.250000 0.285714 0.333333 1.0 0.666667 0.625 1.0 28.31 34.15
0.666667 0.250000 0.285714 0.333333 1.0 1.000000 0.625 1.0 29.39 33.47
0.555556 0.333333 0.428571 0.333333 1.0 0.000000 0.625 1.0 25.70 26.53
0.555556 0.333333 0.428571 0.333333 1.0 0.333333 0.625 1.0 25.17 26.08
0.555556 0.333333 0.428571 0.333333 1.0 0.666667 0.625 1.0 24.60 29.31
0.555556 0.333333 0.428571 0.333333 1.0 1.000000 0.625 1.0 25.49 28.14
0.472222 0.416667 0.571429 0.333333 1.0 0.000000 0.625 1.0 39.89 37.54
0.472222 0.416667 0.571429 0.333333 1.0 0.333333 0.625 1.0 39.83 36.66
0.472222 0.416667 0.571429 0.333333 1.0 0.666667 0.625 1.0 39.01 45.28
0.472222 0.416667 0.571429 0.333333 1.0 1.000000 0.625 1.0 38.65 43.73
0.388889 0.500000 1.000000 0.111111 1.0 0.000000 0.625 1.0 35.69 36.93
0.388889 0.500000 1.000000 0.111111 1.0 0.333333 0.625 1.0 36.64 37.01
0.388889 0.500000 1.000000 0.111111 1.0 0.666667 0.625 1.0 36.06 35.73
0.388889 0.500000 1.000000 0.111111 1.0 1.000000 0.625 1.0 36.70 36.15
0.333333 0.583333 0.000000 1.000000 0.0 0.000000 0.625 1.0 12.12 14.48
0.333333 0.583333 0.000000 1.000000 0.0 0.333333 0.625 1.0 11.67 14.58
0.333333 0.583333 0.000000 1.000000 0.0 0.666667 0.625 1.0 11.64 14.81
0.333333 0.583333 0.000000 1.000000 0.0 1.000000 0.625 1.0 12.02 14.03
0.250000 0.666667 0.142857 1.000000 0.0 0.000000 0.625 1.0 12.27 15.27
0.250000 0.666667 0.142857 1.000000 0.0 0.333333 0.625 1.0 12.19 14.71
0.250000 0.666667 0.142857 1.000000 0.0 0.666667 0.625 1.0 12.25 15.23
0.250000 0.666667 0.142857 1.000000 0.0 1.000000 0.625 1.0 12.27 14.97
0.194444 0.750000 0.285714 1.000000 0.0 0.000000 0.625 1.0 12.47 15.14
0.194444 0.750000 0.285714 1.000000 0.0 0.333333 0.625 1.0 12.12 14.97
0.194444 0.750000 0.285714 1.000000 0.0 0.666667 0.625 1.0 12.18 15.22
0.194444 0.750000 0.285714 1.000000 0.0 1.000000 0.625 1.0 12.47 14.60
0.111111 0.833333 0.428571 1.000000 0.0 0.000000 0.625 1.0 12.93 15.83
0.111111 0.833333 0.428571 1.000000 0.0 0.333333 0.625 1.0 12.82 16.03
0.111111 0.833333 0.428571 1.000000 0.0 0.666667 0.625 1.0 12.78 15.80
0.111111 0.833333 0.428571 1.000000 0.0 1.000000 0.625 1.0 13.02 16.06
0.055556 0.916667 0.571429 1.000000 0.0 0.000000 0.625 1.0 16.73 20.13
0.055556 0.916667 0.571429 1.000000 0.0 0.333333 0.625 1.0 16.86 20.01
0.055556 0.916667 0.571429 1.000000 0.0 0.666667 0.625 1.0 16.76 20.19
0.055556 0.916667 0.571429 1.000000 0.0 1.000000 0.625 1.0 16.92 20.29
0.000000 1.000000 0.714286 1.000000 0.0 0.000000 0.625 1.0 13.68 15.19
0.000000 1.000000 0.714286 1.000000 0.0 0.333333 0.625 1.0 13.99 14.61
0.000000 1.000000 0.714286 1.000000 0.0 0.666667 0.625 1.0 14.16 14.61
0.000000 1.000000 0.714286 1.000000 0.0 1.000000 0.625 1.0 13.86 14.75
1.000000 0.000000 0.285714 0.000000 1.0 0.000000 1.000 0.2 32.26 33.37
1.000000 0.000000 0.285714 0.000000 1.0 0.333333 1.000 0.2 32.26 33.34
1.000000 0.000000 0.285714 0.000000 1.0 0.666667 1.000 0.2 32.49 32.83
1.000000 0.000000 0.285714 0.000000 1.0 1.000000 1.000 0.2 32.53 33.04
0.777778 0.166667 0.428571 0.111111 1.0 0.000000 1.000 0.2 36.47 39.28
0.777778 0.166667 0.428571 0.111111 1.0 0.333333 1.000 0.2 37.24 36.38
0.777778 0.166667 0.428571 0.111111 1.0 0.666667 1.000 0.2 36.66 35.92
0.777778 0.166667 0.428571 0.111111 1.0 1.000000 1.000 0.2 35.96 40.99
0.666667 0.250000 0.285714 0.333333 1.0 0.000000 1.000 0.2 31.89 35.99
0.666667 0.250000 0.285714 0.333333 1.0 0.333333 1.000 0.2 32.39 30.66
0.666667 0.250000 0.285714 0.333333 1.0 0.666667 1.000 0.2 32.09 31.70
0.666667 0.250000 0.285714 0.333333 1.0 1.000000 1.000 0.2 31.29 36.73
0.555556 0.333333 0.428571 0.333333 1.0 0.000000 1.000 0.2 29.22 31.71
0.555556 0.333333 0.428571 0.333333 1.0 0.333333 1.000 0.2 29.91 29.13
0.555556 0.333333 0.428571 0.333333 1.0 0.666667 1.000 0.2 29.53 28.99
0.555556 0.333333 0.428571 0.333333 1.0 1.000000 1.000 0.2 28.65 33.54
0.472222 0.416667 0.571429 0.333333 1.0 0.000000 1.000 0.2 41.40 45.29
0.472222 0.416667 0.571429 0.333333 1.0 0.333333 1.000 0.2 42.62 39.07
0.472222 0.416667 0.571429 0.333333 1.0 0.666667 1.000 0.2 42.50 38.35
0.472222 0.416667 0.571429 0.333333 1.0 1.000000 1.000 0.2 41.67 46.94
0.388889 0.500000 1.000000 0.111111 1.0 0.000000 1.000 0.2 40.78 39.55
0.388889 0.500000 1.000000 0.111111 1.0 0.333333 1.000 0.2 39.97 40.85
0.388889 0.500000 1.000000 0.111111 1.0 0.666667 1.000 0.2 40.71 40.63
0.388889 0.500000 1.000000 0.111111 1.0 1.000000 1.000 0.2 40.43 39.48
0.333333 0.583333 0.000000 1.000000 0.0 0.000000 1.000 0.2 14.52 16.94
0.333333 0.583333 0.000000 1.000000 0.0 0.333333 1.000 0.2 14.61 17.25
0.333333 0.583333 0.000000 1.000000 0.0 0.666667 1.000 0.2 14.50 17.03
0.333333 0.583333 0.000000 1.000000 0.0 1.000000 1.000 0.2 14.55 17.25
0.250000 0.666667 0.142857 1.000000 0.0 0.000000 1.000 0.2 14.51 17.10
0.250000 0.666667 0.142857 1.000000 0.0 0.333333 1.000 0.2 14.60 17.51
0.250000 0.666667 0.142857 1.000000 0.0 0.666667 1.000 0.2 14.50 17.12
0.250000 0.666667 0.142857 1.000000 0.0 1.000000 1.000 0.2 14.58 17.47
0.194444 0.750000 0.285714 1.000000 0.0 0.000000 1.000 0.2 14.51 16.50
0.194444 0.750000 0.285714 1.000000 0.0 0.333333 1.000 0.2 14.70 17.00
0.194444 0.750000 0.285714 1.000000 0.0 0.666667 1.000 0.2 14.42 16.87
0.194444 0.750000 0.285714 1.000000 0.0 1.000000 1.000 0.2 14.42 17.20
0.111111 0.833333 0.428571 1.000000 0.0 0.000000 1.000 0.2 15.23 18.14
0.111111 0.833333 0.428571 1.000000 0.0 0.333333 1.000 0.2 15.23 18.03
0.111111 0.833333 0.428571 1.000000 0.0 0.666667 1.000 0.2 15.23 18.14
0.111111 0.833333 0.428571 1.000000 0.0 1.000000 1.000 0.2 15.23 17.95
0.055556 0.916667 0.571429 1.000000 0.0 0.000000 1.000 0.2 19.52 22.72
0.055556 0.916667 0.571429 1.000000 0.0 0.333333 1.000 0.2 19.36 22.73
0.055556 0.916667 0.571429 1.000000 0.0 0.666667 1.000 0.2 19.48 22.72
0.055556 0.916667 0.571429 1.000000 0.0 1.000000 1.000 0.2 19.42 22.53
0.000000 1.000000 0.714286 1.000000 0.0 0.000000 1.000 0.2 15.09 17.20
0.000000 1.000000 0.714286 1.000000 0.0 0.333333 1.000 0.2 17.17 17.21
0.000000 1.000000 0.714286 1.000000 0.0 0.666667 1.000 0.2 17.14 17.15
0.000000 1.000000 0.714286 1.000000 0.0 1.000000 1.000 0.2 17.14 17.20
1.000000 0.000000 0.285714 0.000000 1.0 0.000000 1.000 0.4 32.82 32.96
1.000000 0.000000 0.285714 0.000000 1.0 0.333333 1.000 0.4 32.71 33.13
1.000000 0.000000 0.285714 0.000000 1.0 0.666667 1.000 0.4 32.24 33.94
1.000000 0.000000 0.285714 0.000000 1.0 1.000000 1.000 0.4 32.72 33.78
0.777778 0.166667 0.428571 0.111111 1.0 0.000000 1.000 0.4 35.84 38.35
0.777778 0.166667 0.428571 0.111111 1.0 0.333333 1.000 0.4 36.57 35.39
0.777778 0.166667 0.428571 0.111111 1.0 0.666667 1.000 0.4 36.06 34.94
0.777778 0.166667 0.428571 0.111111 1.0 1.000000 1.000 0.4 35.69 40.66
0.666667 0.250000 0.285714 0.333333 1.0 0.000000 1.000 0.4 32.48 35.48
0.666667 0.250000 0.285714 0.333333 1.0 0.333333 1.000 0.4 32.74 30.53
0.666667 0.250000 0.285714 0.333333 1.0 0.666667 1.000 0.4 32.13 32.28
0.666667 0.250000 0.285714 0.333333 1.0 1.000000 1.000 0.4 31.64 36.86
0.555556 0.333333 0.428571 0.333333 1.0 0.000000 1.000 0.4 28.95 30.34
0.555556 0.333333 0.428571 0.333333 1.0 0.333333 1.000 0.4 29.49 27.93
0.555556 0.333333 0.428571 0.333333 1.0 0.666667 1.000 0.4 28.64 28.95
0.555556 0.333333 0.428571 0.333333 1.0 1.000000 1.000 0.4 28.01 32.92
0.472222 0.416667 0.571429 0.333333 1.0 0.000000 1.000 0.4 41.64 45.59
0.472222 0.416667 0.571429 0.333333 1.0 0.333333 1.000 0.4 43.10 39.41
0.472222 0.416667 0.571429 0.333333 1.0 0.666667 1.000 0.4 42.74 38.84
0.472222 0.416667 0.571429 0.333333 1.0 1.000000 1.000 0.4 41.92 48.03
0.388889 0.500000 1.000000 0.111111 1.0 0.000000 1.000 0.4 40.78 39.48
0.388889 0.500000 1.000000 0.111111 1.0 0.333333 1.000 0.4 40.15 40.40
0.388889 0.500000 1.000000 0.111111 1.0 0.666667 1.000 0.4 40.57 40.47
0.388889 0.500000 1.000000 0.111111 1.0 1.000000 1.000 0.4 40.42 39.70
0.333333 0.583333 0.000000 1.000000 0.0 0.000000 1.000 0.4 14.54 16.43
0.333333 0.583333 0.000000 1.000000 0.0 0.333333 1.000 0.4 14.45 16.93
0.333333 0.583333 0.000000 1.000000 0.0 0.666667 1.000 0.4 14.18 16.99
0.333333 0.583333 0.000000 1.000000 0.0 1.000000 1.000 0.4 14.50 17.03
0.250000 0.666667 0.142857 1.000000 0.0 0.000000 1.000 0.4 14.70 16.77
0.250000 0.666667 0.142857 1.000000 0.0 0.333333 1.000 0.4 14.66 17.37
0.250000 0.666667 0.142857 1.000000 0.0 0.666667 1.000 0.4 14.40 17.27
0.250000 0.666667 0.142857 1.000000 0.0 1.000000 1.000 0.4 14.71 17.51
0.194444 0.750000 0.285714 1.000000 0.0 0.000000 1.000 0.4 14.75 16.44
0.194444 0.750000 0.285714 1.000000 0.0 0.333333 1.000 0.4 14.71 17.01
0.194444 0.750000 0.285714 1.000000 0.0 0.666667 1.000 0.4 14.33 17.23
0.194444 0.750000 0.285714 1.000000 0.0 1.000000 1.000 0.4 14.62 17.22
0.111111 0.833333 0.428571 1.000000 0.0 0.000000 1.000 0.4 15.34 17.85
0.111111 0.833333 0.428571 1.000000 0.0 0.333333 1.000 0.4 15.29 17.89
0.111111 0.833333 0.428571 1.000000 0.0 0.666667 1.000 0.4 15.09 18.36
0.111111 0.833333 0.428571 1.000000 0.0 1.000000 1.000 0.4 15.30 18.15
0.055556 0.916667 0.571429 1.000000 0.0 0.000000 1.000 0.4 19.20 21.72
0.055556 0.916667 0.571429 1.000000 0.0 0.333333 1.000 0.4 18.88 22.07
0.055556 0.916667 0.571429 1.000000 0.0 0.666667 1.000 0.4 18.90 22.09
0.055556 0.916667 0.571429 1.000000 0.0 1.000000 1.000 0.4 19.12 21.93
0.000000 1.000000 0.714286 1.000000 0.0 0.000000 1.000 0.4 16.76 17.36
0.000000 1.000000 0.714286 1.000000 0.0 0.333333 1.000 0.4 17.23 17.38
0.000000 1.000000 0.714286 1.000000 0.0 0.666667 1.000 0.4 17.26 16.86
0.000000 1.000000 0.714286 1.000000 0.0 1.000000 1.000 0.4 17.15 16.99
1.000000 0.000000 0.285714 0.000000 1.0 0.000000 1.000 0.6 32.82 32.78
1.000000 0.000000 0.285714 0.000000 1.0 0.333333 1.000 0.6 32.69 33.24
1.000000 0.000000 0.285714 0.000000 1.0 0.666667 1.000 0.6 32.23 33.86
1.000000 0.000000 0.285714 0.000000 1.0 1.000000 1.000 0.6 32.75 34.00
0.777778 0.166667 0.428571 0.111111 1.0 0.000000 1.000 0.6 34.24 37.26
0.777778 0.166667 0.428571 0.111111 1.0 0.333333 1.000 0.6 34.95 35.04
0.777778 0.166667 0.428571 0.111111 1.0 0.666667 1.000 0.6 35.05 33.82
0.777778 0.166667 0.428571 0.111111 1.0 1.000000 1.000 0.6 34.29 33.31
0.666667 0.250000 0.285714 0.333333 1.0 0.000000 1.000 0.6 31.28 35.22
0.666667 0.250000 0.285714 0.333333 1.0 0.333333 1.000 0.6 32.12 34.70
0.666667 0.250000 0.285714 0.333333 1.0 0.666667 1.000 0.6 32.05 30.11
0.666667 0.250000 0.285714 0.333333 1.0 1.000000 1.000 0.6 31.84 31.60
0.555556 0.333333 0.428571 0.333333 1.0 0.000000 1.000 0.6 28.67 32.43
0.555556 0.333333 0.428571 0.333333 1.0 0.333333 1.000 0.6 29.67 30.65
0.555556 0.333333 0.428571 0.333333 1.0 0.666667 1.000 0.6 29.47 29.77
0.555556 0.333333 0.428571 0.333333 1.0 1.000000 1.000 0.6 28.91 29.64
0.472222 0.416667 0.571429 0.333333 1.0 0.000000 1.000 0.6 41.26 46.44
0.472222 0.416667 0.571429 0.333333 1.0 0.333333 1.000 0.6 41.30 44.18
0.472222 0.416667 0.571429 0.333333 1.0 0.666667 1.000 0.6 42.49 38.81
0.472222 0.416667 0.571429 0.333333 1.0 1.000000 1.000 0.6 42.08 38.23
0.388889 0.500000 1.000000 0.111111 1.0 0.000000 1.000 0.6 39.32 38.17
0.388889 0.500000 1.000000 0.111111 1.0 0.333333 1.000 0.6 39.84 38.48
0.388889 0.500000 1.000000 0.111111 1.0 0.666667 1.000 0.6 38.89 39.66
0.388889 0.500000 1.000000 0.111111 1.0 1.000000 1.000 0.6 39.68 40.10
0.333333 0.583333 0.000000 1.000000 0.0 0.000000 1.000 0.6 13.97 16.08
0.333333 0.583333 0.000000 1.000000 0.0 0.333333 1.000 0.6 14.22 15.39
0.333333 0.583333 0.000000 1.000000 0.0 0.666667 1.000 0.6 14.10 16.57
0.333333 0.583333 0.000000 1.000000 0.0 1.000000 1.000 0.6 13.78 16.60
0.250000 0.666667 0.142857 1.000000 0.0 0.000000 1.000 0.6 14.07 16.11
0.250000 0.666667 0.142857 1.000000 0.0 0.333333 1.000 0.6 14.03 15.47
0.250000 0.666667 0.142857 1.000000 0.0 0.666667 1.000 0.6 13.94 16.70
0.250000 0.666667 0.142857 1.000000 0.0 1.000000 1.000 0.6 13.86 16.10
0.194444 0.750000 0.285714 1.000000 0.0 0.000000 1.000 0.6 14.32 16.35
0.194444 0.750000 0.285714 1.000000 0.0 0.333333 1.000 0.6 14.56 15.84
0.194444 0.750000 0.285714 1.000000 0.0 0.666667 1.000 0.6 14.33 16.99
0.194444 0.750000 0.285714 1.000000 0.0 1.000000 1.000 0.6 14.08 17.02
0.111111 0.833333 0.428571 1.000000 0.0 0.000000 1.000 0.6 15.16 17.04
0.111111 0.833333 0.428571 1.000000 0.0 0.333333 1.000 0.6 15.18 17.63
0.111111 0.833333 0.428571 1.000000 0.0 0.666667 1.000 0.6 14.72 18.10
0.111111 0.833333 0.428571 1.000000 0.0 1.000000 1.000 0.6 14.90 18.22
0.055556 0.916667 0.571429 1.000000 0.0 0.000000 1.000 0.6 18.48 20.78
0.055556 0.916667 0.571429 1.000000 0.0 0.333333 1.000 0.6 18.71 20.72
0.055556 0.916667 0.571429 1.000000 0.0 0.666667 1.000 0.6 18.48 21.54
0.055556 0.916667 0.571429 1.000000 0.0 1.000000 1.000 0.6 18.46 21.53
0.000000 1.000000 0.714286 1.000000 0.0 0.000000 1.000 0.6 16.47 16.90
0.000000 1.000000 0.714286 1.000000 0.0 0.333333 1.000 0.6 16.35 17.14
0.000000 1.000000 0.714286 1.000000 0.0 0.666667 1.000 0.6 16.55 16.56
0.000000 1.000000 0.714286 1.000000 0.0 1.000000 1.000 0.6 16.74 16.00
1.000000 0.000000 0.285714 0.000000 1.0 0.000000 1.000 0.8 32.85 32.95
1.000000 0.000000 0.285714 0.000000 1.0 0.333333 1.000 0.8 32.67 33.06
1.000000 0.000000 0.285714 0.000000 1.0 0.666667 1.000 0.8 32.21 33.95
1.000000 0.000000 0.285714 0.000000 1.0 1.000000 1.000 0.8 32.74 33.88
0.777778 0.166667 0.428571 0.111111 1.0 0.000000 1.000 0.8 36.45 33.98
0.777778 0.166667 0.428571 0.111111 1.0 0.333333 1.000 0.8 35.73 39.92
0.777778 0.166667 0.428571 0.111111 1.0 0.666667 1.000 0.8 35.40 39.22
0.777778 0.166667 0.428571 0.111111 1.0 1.000000 1.000 0.8 36.57 36.10
0.666667 0.250000 0.285714 0.333333 1.0 0.000000 1.000 0.8 32.38 31.53
0.666667 0.250000 0.285714 0.333333 1.0 0.333333 1.000 0.8 31.66 36.20
0.666667 0.250000 0.285714 0.333333 1.0 0.666667 1.000 0.8 32.15 36.21
0.666667 0.250000 0.285714 0.333333 1.0 1.000000 1.000 0.8 32.75 31.00
0.555556 0.333333 0.428571 0.333333 1.0 0.000000 1.000 0.8 28.93 28.20
0.555556 0.333333 0.428571 0.333333 1.0 0.333333 1.000 0.8 28.05 32.35
0.555556 0.333333 0.428571 0.333333 1.0 0.666667 1.000 0.8 28.64 31.14
0.555556 0.333333 0.428571 0.333333 1.0 1.000000 1.000 0.8 29.52 28.43
0.472222 0.416667 0.571429 0.333333 1.0 0.000000 1.000 0.8 42.77 38.33
0.472222 0.416667 0.571429 0.333333 1.0 0.333333 1.000 0.8 41.73 47.59
0.472222 0.416667 0.571429 0.333333 1.0 0.666667 1.000 0.8 41.32 46.23
0.472222 0.416667 0.571429 0.333333 1.0 1.000000 1.000 0.8 42.96 39.56
0.388889 0.500000 1.000000 0.111111 1.0 0.000000 1.000 0.8 40.68 40.36
0.388889 0.500000 1.000000 0.111111 1.0 0.333333 1.000 0.8 40.40 39.67
0.388889 0.500000 1.000000 0.111111 1.0 0.666667 1.000 0.8 40.60 39.85
0.388889 0.500000 1.000000 0.111111 1.0 1.000000 1.000 0.8 40.11 40.77
0.333333 0.583333 0.000000 1.000000 0.0 0.000000 1.000 0.8 14.37 16.61
0.333333 0.583333 0.000000 1.000000 0.0 0.333333 1.000 0.8 14.48 16.74
0.333333 0.583333 0.000000 1.000000 0.0 0.666667 1.000 0.8 14.32 16.90
0.333333 0.583333 0.000000 1.000000 0.0 1.000000 1.000 0.8 14.44 17.32
0.250000 0.666667 0.142857 1.000000 0.0 0.000000 1.000 0.8 14.60 16.85
0.250000 0.666667 0.142857 1.000000 0.0 0.333333 1.000 0.8 14.70 17.20
0.250000 0.666667 0.142857 1.000000 0.0 0.666667 1.000 0.8 14.47 17.23
0.250000 0.666667 0.142857 1.000000 0.0 1.000000 1.000 0.8 14.66 17.74
0.194444 0.750000 0.285714 1.000000 0.0 0.000000 1.000 0.8 14.54 16.81
0.194444 0.750000 0.285714 1.000000 0.0 0.333333 1.000 0.8 14.62 16.88
0.194444 0.750000 0.285714 1.000000 0.0 0.666667 1.000 0.8 14.53 16.90
0.194444 0.750000 0.285714 1.000000 0.0 1.000000 1.000 0.8 14.71 17.39
0.111111 0.833333 0.428571 1.000000 0.0 0.000000 1.000 0.8 15.34 17.86
0.111111 0.833333 0.428571 1.000000 0.0 0.333333 1.000 0.8 15.29 17.82
0.111111 0.833333 0.428571 1.000000 0.0 0.666667 1.000 0.8 15.09 18.36
0.111111 0.833333 0.428571 1.000000 0.0 1.000000 1.000 0.8 15.30 18.24
0.055556 0.916667 0.571429 1.000000 0.0 0.000000 1.000 0.8 19.06 21.68
0.055556 0.916667 0.571429 1.000000 0.0 0.333333 1.000 0.8 19.13 21.54
0.055556 0.916667 0.571429 1.000000 0.0 0.666667 1.000 0.8 19.00 22.25
0.055556 0.916667 0.571429 1.000000 0.0 1.000000 1.000 0.8 18.84 22.49
0.000000 1.000000 0.714286 1.000000 0.0 0.000000 1.000 0.8 16.44 17.10
0.000000 1.000000 0.714286 1.000000 0.0 0.333333 1.000 0.8 16.90 16.79
0.000000 1.000000 0.714286 1.000000 0.0 0.666667 1.000 0.8 16.94 16.58
0.000000 1.000000 0.714286 1.000000 0.0 1.000000 1.000 0.8 16.77 16.79
1.000000 0.000000 0.285714 0.000000 1.0 0.000000 1.000 1.0 32.84 32.88
1.000000 0.000000 0.285714 0.000000 1.0 0.333333 1.000 1.0 32.72 33.23
1.000000 0.000000 0.285714 0.000000 1.0 0.666667 1.000 1.0 32.21 33.76
1.000000 0.000000 0.285714 0.000000 1.0 1.000000 1.000 1.0 32.73 34.01
0.777778 0.166667 0.428571 0.111111 1.0 0.000000 1.000 1.0 35.67 33.94
0.777778 0.166667 0.428571 0.111111 1.0 0.333333 1.000 1.0 35.01 33.14
0.777778 0.166667 0.428571 0.111111 1.0 0.666667 1.000 1.0 34.72 38.79
0.777778 0.166667 0.428571 0.111111 1.0 1.000000 1.000 1.0 35.24 37.27
0.666667 0.250000 0.285714 0.333333 1.0 0.000000 1.000 1.0 32.31 29.69
0.666667 0.250000 0.285714 0.333333 1.0 0.333333 1.000 1.0 31.81 31.20
0.666667 0.250000 0.285714 0.333333 1.0 0.666667 1.000 1.0 31.12 36.26
0.666667 0.250000 0.285714 0.333333 1.0 1.000000 1.000 1.0 32.06 35.71
0.555556 0.333333 0.428571 0.333333 1.0 0.000000 1.000 1.0 30.00 29.93
0.555556 0.333333 0.428571 0.333333 1.0 0.333333 1.000 1.0 29.50 29.56
0.555556 0.333333 0.428571 0.333333 1.0 0.666667 1.000 1.0 29.06 33.84
0.555556 0.333333 0.428571 0.333333 1.0 1.000000 1.000 1.0 29.92 32.54
0.472222 0.416667 0.571429 0.333333 1.0 0.000000 1.000 1.0 42.11 38.56
0.472222 0.416667 0.571429 0.333333 1.0 0.333333 1.000 1.0 41.96 37.70
0.472222 0.416667 0.571429 0.333333 1.0 0.666667 1.000 1.0 41.09 47.01
0.472222 0.416667 0.571429 0.333333 1.0 1.000000 1.000 1.0 40.79 44.87
0.388889 0.500000 1.000000 0.111111 1.0 0.000000 1.000 1.0 38.82 39.37
0.388889 0.500000 1.000000 0.111111 1.0 0.333333 1.000 1.0 39.72 39.80
0.388889 0.500000 1.000000 0.111111 1.0 0.666667 1.000 1.0 39.31 37.79
0.388889 0.500000 1.000000 0.111111 1.0 1.000000 1.000 1.0 39.86 38.18
0.333333 0.583333 0.000000 1.000000 0.0 0.000000 1.000 1.0 14.41 16.69
0.333333 0.583333 0.000000 1.000000 0.0 0.333333 1.000 1.0 14.19 16.62
0.333333 0.583333 0.000000 1.000000 0.0 0.666667 1.000 1.0 14.17 16.94
0.333333 0.583333 0.000000 1.000000 0.0 1.000000 1.000 1.0 14.39 16.70
0.250000 0.666667 0.142857 1.000000 0.0 0.000000 1.000 1.0 12.43 15.59
0.250000 0.666667 0.142857 1.000000 0.0 0.333333 1.000 1.0 12.63 14.58
0.250000 0.666667 0.142857 1.000000 0.0 0.666667 1.000 1.0 12.76 15.33
0.250000 0.666667 0.142857 1.000000 0.0 1.000000 1.000 1.0 12.42 15.31
0.194444 0.750000 0.285714 1.000000 0.0 0.000000 1.000 1.0 14.12 16.63
0.194444 0.750000 0.285714 1.000000 0.0 0.333333 1.000 1.0 14.28 15.87
0.194444 0.750000 0.285714 1.000000 0.0 0.666667 1.000 1.0 14.37 16.54
0.194444 0.750000 0.285714 1.000000 0.0 1.000000 1.000 1.0 14.21 16.74
0.111111 0.833333 0.428571 1.000000 0.0 0.000000 1.000 1.0 14.96 17.64
0.111111 0.833333 0.428571 1.000000 0.0 0.333333 1.000 1.0 14.92 17.79
0.111111 0.833333 0.428571 1.000000 0.0 0.666667 1.000 1.0 14.92 17.55
0.111111 0.833333 0.428571 1.000000 0.0 1.000000 1.000 1.0 15.16 18.06
0.055556 0.916667 0.571429 1.000000 0.0 0.000000 1.000 1.0 17.69 20.82
0.055556 0.916667 0.571429 1.000000 0.0 0.333333 1.000 1.0 18.19 20.21
0.055556 0.916667 0.571429 1.000000 0.0 0.666667 1.000 1.0 18.16 20.71
0.055556 0.916667 0.571429 1.000000 0.0 1.000000 1.000 1.0 17.88 21.40
0.000000 1.000000 0.714286 1.000000 0.0 0.000000 1.000 1.0 16.54 16.88
0.000000 1.000000 0.714286 1.000000 0.0 0.333333 1.000 1.0 16.44 17.11
0.000000 1.000000 0.714286 1.000000 0.0 0.666667 1.000 1.0 16.48 16.61
0.000000 1.000000 0.714286 1.000000 0.0 1.000000 1.000 1.0 16.64 16.03
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="modelling">
<h2>Modelling<a class="headerlink" href="#modelling" title="Link to this heading">#</a></h2>
<p>Modeling adalah proses membangun sebuah model matematis atau algoritmik berdasarkan data yang tersedia, dengan tujuan untuk memahami pola atau hubungan antar variabel dalam data, serta untuk membuat prediksi atau keputusan otomatis pada data baru.</p>
<p>Dalam konteks machine learning, modeling merupakan inti dari keseluruhan proses pembelajaran mesin, yaitu membuat model prediktif atau deskriptif dari data latih (training data), dan kemudian mengujinya pada data uji (testing data) untuk menilai kinerjanya.</p>
</section>
<section id="pemodelan-prediksi-efisiensi-energi-bangunan-menggunakan-decision-tree">
<h2>Pemodelan Prediksi Efisiensi Energi Bangunan Menggunakan Decision Tree<a class="headerlink" href="#pemodelan-prediksi-efisiensi-energi-bangunan-menggunakan-decision-tree" title="Link to this heading">#</a></h2>
<p>Pemodelan dalam penelitian ini bertujuan untuk mengklasifikasikan tingkat kebutuhan energi bangunan, baik dalam bentuk beban pemanasan (Heating Load / Y1) maupun beban pendinginan (Cooling Load / Y2). Proses klasifikasi dilakukan berdasarkan sejumlah fitur arsitektural dan fisik bangunan, seperti luas permukaan, tinggi bangunan, area dinding, area kaca, hingga orientasi bangunan.</p>
<p>Variabel target yang digunakan adalah Y1 dan Y2, yang telah diklasifikasikan ke dalam tiga kelas, yaitu Rendah, Sedang, dan Tinggi, berdasarkan rentang nilai aslinya menggunakan metode <em>binning</em>.</p>
<ol class="arabic simple">
<li><p>Pembagian Dataset: Data Latih dan Data Uji</p></li>
</ol>
<p>Tahap awal dalam proses pemodelan melibatkan pembagian data menjadi dua bagian utama:</p>
<ul class="simple">
<li><p>Data Latih (Training Set): digunakan untuk melatih model agar memahami pola hubungan antara fitur desain bangunan dengan kebutuhan energi.</p></li>
<li><p>Data Uji (Testing Set): digunakan untuk mengevaluasi performa model terhadap data baru yang belum pernah dilihat sebelumnya.</p></li>
</ul>
<p>Dataset dibagi dengan proporsi 80% untuk pelatihan dan 20% untuk pengujian, mengikuti praktik umum dalam eksperimen machine learning.</p>
<ol class="arabic simple" start="2">
<li><p>Pra-pemrosesan Data</p></li>
</ol>
<p>Sebelum dilakukan pelatihan model, beberapa langkah pra-pemrosesan dilakukan untuk mempersiapkan data:</p>
<ul class="simple">
<li><p>Normalisasi fitur numerik: Semua fitur input (seperti <code class="docutils literal notranslate"><span class="pre">Relative</span> <span class="pre">Compactness</span></code>, <code class="docutils literal notranslate"><span class="pre">Surface</span> <span class="pre">Area</span></code>, <code class="docutils literal notranslate"><span class="pre">Wall</span> <span class="pre">Area</span></code>, dll.) dinormalisasi menggunakan MinMaxScaler untuk memastikan skala data seragam antara 0 hingga 1.</p></li>
<li><p>Transformasi target ke bentuk klasifikasi: Nilai kontinu <code class="docutils literal notranslate"><span class="pre">Y1</span></code> dan <code class="docutils literal notranslate"><span class="pre">Y2</span></code> dikonversi menjadi kelas kategori (Rendah, Sedang, Tinggi) agar sesuai untuk pemodelan klasifikasi.</p></li>
<li><p>Pemisahan fitur dan target: Fitur-fitur prediktor (<code class="docutils literal notranslate"><span class="pre">X1</span></code> hingga <code class="docutils literal notranslate"><span class="pre">X8</span></code>) dipisahkan dari variabel target (<code class="docutils literal notranslate"><span class="pre">Y1_class</span></code>, <code class="docutils literal notranslate"><span class="pre">Y2_class</span></code>).</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p>Pelatihan Model Decision Tree</p></li>
</ol>
<p>Pemodelan dilakukan menggunakan algoritma Decision Tree Classifier, yaitu model pembelajaran terawasi yang meniru pola pengambilan keputusan secara bertingkat. Dalam proses ini:</p>
<ul class="simple">
<li><p>Setiap cabang pohon akan membagi data berdasarkan fitur yang paling mengurangi ketidakmurnian data (biasanya diukur menggunakan Gini Index).</p></li>
<li><p>Model dibatasi kedalamannya (<code class="docutils literal notranslate"><span class="pre">max_depth</span> <span class="pre">=</span> <span class="pre">5</span></code>) untuk menghindari overfitting dan menjaga generalisasi model.</p></li>
</ul>
<ol class="arabic simple" start="4">
<li><p>Evaluasi Model</p></li>
</ol>
<p>Setelah model dilatih, performanya dievaluasi menggunakan berbagai metrik:</p>
<ul class="simple">
<li><p>Akurasi: mengukur sejauh mana prediksi model sesuai dengan kelas sebenarnya.</p></li>
<li><p>Classification Report: memberikan gambaran precision, recall, dan F1-score untuk masing-masing kelas (Rendah/Sedang/Tinggi).</p></li>
<li><p>Confusion Matrix: menggambarkan distribusi hasil prediksi terhadap label aktual.</p></li>
<li><p>Visualisasi Pohon Keputusan: digunakan untuk memahami fitur mana yang paling memengaruhi klasifikasi serta logika pembentukan keputusan model.</p></li>
</ul>
<ol class="arabic simple" start="5">
<li><p>Prediksi Kebutuhan Energi Bangunan Baru</p></li>
</ol>
<p>Model yang telah dilatih dapat digunakan untuk:</p>
<ul class="simple">
<li><p>Memprediksi tingkat efisiensi energi (pemanasan dan pendinginan) dari desain bangunan baru,</p></li>
<li><p>Mengidentifikasi fitur arsitektural paling berpengaruh terhadap kebutuhan energi, seperti luas permukaan, area kaca, atau tinggi bangunan.</p></li>
</ul>
<p>Kesimpulan</p>
<p>Decision Tree terbukti sebagai metode klasifikasi yang sesuai untuk memodelkan efisiensi energi bangunan karena:</p>
<ul class="simple">
<li><p>Mampu menangani fitur numerik tanpa asumsi distribusi,</p></li>
<li><p>Mudah dipahami dan divisualisasikan,</p></li>
<li><p>Memberikan transparansi dalam proses pengambilan keputusan,</p></li>
<li><p>Cocok untuk penggunaan praktis dalam desain bangunan hemat energi.</p></li>
</ul>
<p>Model ini dapat dimanfaatkan sebagai alat bantu dalam merancang bangunan yang efisien secara energi berdasarkan parameter-parameter teknisnya.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span><span class="p">,</span> <span class="n">plot_tree</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">classification_report</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># === 1. Klasifikasikan Y1 menjadi kelas (Rendah, Sedang, Tinggi)</span>
<span class="c1"># Menentukan bins berdasarkan nilai minimum dan maksimum Y1</span>
<span class="n">min_y1</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="s1">&#39;Y1&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
<span class="n">max_y1</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="s1">&#39;Y1&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="n">bins</span> <span class="o">=</span> <span class="p">[</span><span class="n">min_y1</span><span class="p">,</span> <span class="n">min_y1</span> <span class="o">+</span> <span class="p">(</span><span class="n">max_y1</span> <span class="o">-</span> <span class="n">min_y1</span><span class="p">)</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_y1</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">max_y1</span> <span class="o">-</span> <span class="n">min_y1</span><span class="p">)</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_y1</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Rendah&#39;</span><span class="p">,</span> <span class="s1">&#39;Sedang&#39;</span><span class="p">,</span> <span class="s1">&#39;Tinggi&#39;</span><span class="p">]</span>
<span class="n">y_class</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="s1">&#39;Y1&#39;</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">include_lowest</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="c1"># === 2. Split ulang target klasifikasi</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X_train_cls</span><span class="p">,</span> <span class="n">X_test_cls</span><span class="p">,</span> <span class="n">y_train_cls</span><span class="p">,</span> <span class="n">y_test_cls</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_class</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># === 3. Latih model klasifikasi</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;gini&#39;</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_cls</span><span class="p">,</span> <span class="n">y_train_cls</span><span class="p">)</span>

<span class="c1"># === 4. Evaluasi klasifikasi</span>
<span class="n">y_pred_cls</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_cls</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Akurasi:&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test_cls</span><span class="p">,</span> <span class="n">y_pred_cls</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== Classification Report ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test_cls</span><span class="p">,</span> <span class="n">y_pred_cls</span><span class="p">))</span>

<span class="c1"># === 5. Visualisasi pohon keputusan</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">plot_tree</span><span class="p">(</span>
    <span class="n">clf</span><span class="p">,</span>
    <span class="n">feature_names</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span>
    <span class="n">class_names</span><span class="o">=</span><span class="n">clf</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span>
    <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">rounded</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Pohon Keputusan - Klasifikasi Y1 (Heating Load)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Akurasi: 0.9805194805194806

=== Classification Report ===
              precision    recall  f1-score   support

      Rendah       1.00      0.99      0.99        69
      Sedang       0.93      1.00      0.97        42
      Tinggi       1.00      0.95      0.98        43

    accuracy                           0.98       154
   macro avg       0.98      0.98      0.98       154
weighted avg       0.98      0.98      0.98       154
</pre></div>
</div>
<img alt="_images/b4352bcb805048c1bc865a963aabb4fab00bc8fa4320851b14164653bb697f48.png" src="_images/b4352bcb805048c1bc865a963aabb4fab00bc8fa4320851b14164653bb697f48.png" />
</div>
</div>
</section>
<section id="pemodelan-kebutuhan-energi-bangunan-menggunakan-k-nearest-neighbors-knn">
<h2>Pemodelan Kebutuhan Energi Bangunan Menggunakan K-Nearest Neighbors (KNN)<a class="headerlink" href="#pemodelan-kebutuhan-energi-bangunan-menggunakan-k-nearest-neighbors-knn" title="Link to this heading">#</a></h2>
<p>Model K-Nearest Neighbors (KNN) digunakan dalam penelitian ini untuk memprediksi tingkat kebutuhan energi bangunan, khususnya pada dua indikator utama: beban pemanasan (Heating Load/Y1) dan beban pendinginan (Cooling Load/Y2). Prediksi dilakukan berdasarkan parameter desain bangunan seperti luas permukaan, tinggi bangunan, area dinding dan atap, luas jendela, serta orientasi bangunan.</p>
<p>KNN merupakan metode pembelajaran berbasis instance (<em>instance-based learning</em>), di mana proses prediksi dilakukan dengan mencari beberapa “tetangga” terdekat dari data baru dalam ruang fitur, lalu melakukan klasifikasi berdasarkan mayoritas label dari tetangga tersebut.</p>
<ol class="arabic simple">
<li><p>Pembagian Dataset: Data Latih dan Data Uji</p></li>
</ol>
<p>Sebagaimana pendekatan supervised learning lainnya, dataset dibagi menjadi dua bagian utama:</p>
<ul class="simple">
<li><p>Data Latih (Training Set): digunakan untuk menyimpan contoh bangunan yang sudah diketahui kebutuhan energinya.</p></li>
<li><p>Data Uji (Testing Set): digunakan untuk mengukur akurasi model dalam memprediksi tingkat kebutuhan energi dari desain bangunan baru.</p></li>
</ul>
<p>Pada penelitian ini, proporsi yang digunakan adalah 80% untuk pelatihan dan 20% untuk pengujian. Rasio ini umum diterapkan dalam banyak eksperimen machine learning.</p>
<ol class="arabic simple" start="2">
<li><p>Pra-pemrosesan Data</p></li>
</ol>
<p>Agar algoritma KNN dapat bekerja optimal, dilakukan beberapa tahap pra-pemrosesan sebagai berikut:</p>
<ul class="simple">
<li><p>Normalisasi fitur numerik: Semua fitur numerik, seperti <code class="docutils literal notranslate"><span class="pre">Relative</span> <span class="pre">Compactness</span></code>, <code class="docutils literal notranslate"><span class="pre">Surface</span> <span class="pre">Area</span></code>, <code class="docutils literal notranslate"><span class="pre">Overall</span> <span class="pre">Height</span></code>, dan sebagainya, dinormalisasi menggunakan MinMaxScaler. Hal ini penting karena KNN mengandalkan perhitungan jarak, dan fitur dengan skala lebih besar bisa mendominasi perhitungan jika tidak dinormalisasi terlebih dahulu.</p></li>
<li><p>Transformasi target: Nilai kontinu dari <code class="docutils literal notranslate"><span class="pre">Y1</span></code> dan <code class="docutils literal notranslate"><span class="pre">Y2</span></code> dikonversi ke dalam kategori Rendah, Sedang, dan Tinggi menggunakan metode binning, agar sesuai untuk klasifikasi.</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p>Mekanisme Kerja KNN</p></li>
</ol>
<p>Berbeda dengan model yang membentuk aturan eksplisit, KNN menyimpan semua data pelatihan, lalu melakukan prediksi berdasarkan kedekatan (jarak) antara data uji dan seluruh data latih. Umumnya, jarak Euclidean digunakan untuk mengukur kedekatan.</p>
<p>Langkah-langkahnya meliputi:</p>
<ol class="arabic simple">
<li><p>Menghitung jarak antara satu desain bangunan baru dengan semua data latih.</p></li>
<li><p>Memilih <strong>k</strong> tetangga terdekat berdasarkan jarak terpendek.</p></li>
<li><p>Menentukan kelas prediksi (Rendah/Sedang/Tinggi) berdasarkan mayoritas label dari k tetangga tersebut.</p></li>
</ol>
<p>Misalnya, jika <code class="docutils literal notranslate"><span class="pre">k</span> <span class="pre">=</span> <span class="pre">5</span></code> dan dari 5 tetangga, 3 di antaranya memiliki kategori “Tinggi” untuk kebutuhan pemanasan, maka bangunan baru tersebut akan diklasifikasikan sebagai “Tinggi”.</p>
<ol class="arabic simple" start="4">
<li><p>Evaluasi Model</p></li>
</ol>
<p>Setelah proses prediksi selesai, performa model dievaluasi menggunakan beberapa metrik:</p>
<ul class="simple">
<li><p>Akurasi: mengukur persentase prediksi yang benar dari keseluruhan data uji.</p></li>
<li><p>Classification Report: memberikan detail metrik seperti precision, recall, dan F1-score untuk masing-masing kelas.</p></li>
<li><p>Confusion Matrix: menunjukkan persebaran prediksi yang benar maupun salah terhadap nilai sebenarnya.</p></li>
</ul>
<p>Eksperimen juga dapat dilakukan dengan berbagai nilai k (misalnya 3, 5, atau 7) untuk mencari kombinasi terbaik yang memberikan akurasi tertinggi.</p>
<p>Kesimpulan</p>
<p>K-Nearest Neighbors merupakan algoritma klasifikasi yang sederhana namun efektif, khususnya dalam kasus prediksi berbasis kemiripan seperti ini. Setelah melalui proses normalisasi dan transformasi target, KNN dapat memberikan hasil yang cukup akurat untuk memprediksi kebutuhan energi bangunan.</p>
<p>Metode ini sangat cocok digunakan untuk studi desain bangunan efisien energi, karena dapat membantu mengidentifikasi pola-pola kebutuhan energi berdasarkan fitur-fitur fisik bangunan secara transparan dan intuitif.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.multioutput</span> <span class="kn">import</span> <span class="n">MultiOutputClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">confusion_matrix</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># === 1. Ubah target Y1 dan Y2 ke kelas kategorikal ===</span>
<span class="n">min_y1</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="s1">&#39;Y1&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
<span class="n">max_y1</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="s1">&#39;Y1&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="n">min_y2</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="s1">&#39;Y2&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
<span class="n">max_y2</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="s1">&#39;Y2&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

<span class="n">bins_y1</span> <span class="o">=</span> <span class="p">[</span><span class="n">min_y1</span><span class="p">,</span> <span class="n">min_y1</span> <span class="o">+</span> <span class="p">(</span><span class="n">max_y1</span> <span class="o">-</span> <span class="n">min_y1</span><span class="p">)</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_y1</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">max_y1</span> <span class="o">-</span> <span class="n">min_y1</span><span class="p">)</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_y1</span><span class="p">]</span>
<span class="n">bins_y2</span> <span class="o">=</span> <span class="p">[</span><span class="n">min_y2</span><span class="p">,</span> <span class="n">min_y2</span> <span class="o">+</span> <span class="p">(</span><span class="n">max_y2</span> <span class="o">-</span> <span class="n">min_y2</span><span class="p">)</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_y2</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">max_y2</span> <span class="o">-</span> <span class="n">min_y2</span><span class="p">)</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_y2</span><span class="p">]</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Rendah&#39;</span><span class="p">,</span> <span class="s1">&#39;Sedang&#39;</span><span class="p">,</span> <span class="s1">&#39;Tinggi&#39;</span><span class="p">]</span>

<span class="n">y_class_multi</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">y_class_multi</span><span class="p">[</span><span class="s1">&#39;Y1_class&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="s1">&#39;Y1&#39;</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins_y1</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">include_lowest</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y_class_multi</span><span class="p">[</span><span class="s1">&#39;Y2_class&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="s1">&#39;Y2&#39;</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins_y2</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">include_lowest</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># === 2. Split data untuk multi-output classification ===</span>
<span class="n">X_train_multi</span><span class="p">,</span> <span class="n">X_test_multi</span><span class="p">,</span> <span class="n">y_train_multi</span><span class="p">,</span> <span class="n">y_test_multi</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y_class_multi</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="c1"># === 3. Buat model KNN multi-output ===</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">multi_knn</span> <span class="o">=</span> <span class="n">MultiOutputClassifier</span><span class="p">(</span><span class="n">knn</span><span class="p">)</span>
<span class="n">multi_knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_multi</span><span class="p">,</span> <span class="n">y_train_multi</span><span class="p">)</span>

<span class="c1"># === 4. Prediksi</span>
<span class="n">y_pred_multi</span> <span class="o">=</span> <span class="n">multi_knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_multi</span><span class="p">)</span>
<span class="n">y_pred_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">y_pred_multi</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Y1_pred&#39;</span><span class="p">,</span> <span class="s1">&#39;Y2_pred&#39;</span><span class="p">])</span>

<span class="c1"># === 5. Evaluasi per target ===</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Evaluasi Y1 (Heating Load):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Akurasi:&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test_multi</span><span class="p">[</span><span class="s1">&#39;Y1_class&#39;</span><span class="p">],</span> <span class="n">y_pred_df</span><span class="p">[</span><span class="s1">&#39;Y1_pred&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test_multi</span><span class="p">[</span><span class="s1">&#39;Y1_class&#39;</span><span class="p">],</span> <span class="n">y_pred_df</span><span class="p">[</span><span class="s1">&#39;Y1_pred&#39;</span><span class="p">]))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Evaluasi Y2 (Cooling Load):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Akurasi:&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test_multi</span><span class="p">[</span><span class="s1">&#39;Y2_class&#39;</span><span class="p">],</span> <span class="n">y_pred_df</span><span class="p">[</span><span class="s1">&#39;Y2_pred&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test_multi</span><span class="p">[</span><span class="s1">&#39;Y2_class&#39;</span><span class="p">],</span> <span class="n">y_pred_df</span><span class="p">[</span><span class="s1">&#39;Y2_pred&#39;</span><span class="p">]))</span>

<span class="c1"># === 6. Confusion Matrix ===</span>
<span class="c1"># Y1</span>
<span class="n">cm_y1</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test_multi</span><span class="p">[</span><span class="s1">&#39;Y1_class&#39;</span><span class="p">],</span> <span class="n">y_pred_df</span><span class="p">[</span><span class="s1">&#39;Y1_pred&#39;</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cm_y1</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix - Y1 (Heating Load)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Predicted&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Actual&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Y2</span>
<span class="n">cm_y2</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test_multi</span><span class="p">[</span><span class="s1">&#39;Y2_class&#39;</span><span class="p">],</span> <span class="n">y_pred_df</span><span class="p">[</span><span class="s1">&#39;Y2_pred&#39;</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cm_y2</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Greens&#39;</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix - Y2 (Cooling Load)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Predicted&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Actual&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Evaluasi Y1 (Heating Load):
Akurasi: 0.9155844155844156
              precision    recall  f1-score   support

      Rendah       0.99      1.00      0.99        69
      Sedang       0.85      0.83      0.84        42
      Tinggi       0.86      0.86      0.86        43

    accuracy                           0.92       154
   macro avg       0.90      0.90      0.90       154
weighted avg       0.91      0.92      0.92       154


Evaluasi Y2 (Cooling Load):
Akurasi: 0.9090909090909091
              precision    recall  f1-score   support

      Rendah       1.00      0.99      0.99        73
      Sedang       0.86      0.89      0.88        57
      Tinggi       0.74      0.71      0.72        24

    accuracy                           0.91       154
   macro avg       0.87      0.86      0.87       154
weighted avg       0.91      0.91      0.91       154
</pre></div>
</div>
<img alt="_images/1a5261c64344cdb553a651f0fac2ff054ae87989ff691c5d362024dfd704b276.png" src="_images/1a5261c64344cdb553a651f0fac2ff054ae87989ff691c5d362024dfd704b276.png" />
<img alt="_images/2a7fd5783c9fd62c1f7fb13a6dad7b8334710b482e84b74e4c281b4a2e65a49a.png" src="_images/2a7fd5783c9fd62c1f7fb13a6dad7b8334710b482e84b74e4c281b4a2e65a49a.png" />
</div>
</div>
</section>
<section id="pemodelan-prediksi-kebutuhan-energi-bangunan-dengan-gaussian-naive-bayes">
<h2>Pemodelan Prediksi Kebutuhan Energi Bangunan dengan Gaussian Naive Bayes<a class="headerlink" href="#pemodelan-prediksi-kebutuhan-energi-bangunan-dengan-gaussian-naive-bayes" title="Link to this heading">#</a></h2>
<p>Pemodelan ini bertujuan untuk mengklasifikasikan tingkat kebutuhan energi bangunan, baik dalam bentuk beban pemanasan (Heating Load / Y1) maupun beban pendinginan (Cooling Load / Y2). Prediksi dilakukan berdasarkan delapan fitur fisik bangunan seperti kompaksi relatif, luas permukaan, tinggi keseluruhan, orientasi, hingga distribusi area kaca.</p>
<p>Model klasifikasi yang digunakan adalah Gaussian Naive Bayes, sebuah metode probabilistik yang mengasumsikan bahwa setiap fitur input memiliki distribusi normal dan bersifat independen terhadap fitur lainnya. Model ini dikenal sederhana, cepat, dan efisien, terutama ketika fitur numerik digunakan.</p>
<ol class="arabic simple">
<li><p>Pembagian Data: Data Latih dan Data Uji</p></li>
</ol>
<p>Langkah awal melibatkan pembagian dataset menjadi dua bagian utama:</p>
<ul class="simple">
<li><p>Data Latih (Training Set): digunakan untuk menghitung parameter distribusi dari setiap fitur berdasarkan kelas target.</p></li>
<li><p>Data Uji (Testing Set): digunakan untuk menguji kemampuan model dalam mengklasifikasikan desain bangunan yang belum pernah dianalisis sebelumnya.</p></li>
</ul>
<p>Proporsi yang digunakan adalah 80% untuk pelatihan dan 20% untuk pengujian, sebagaimana umum dilakukan dalam eksperimen machine learning.</p>
<ol class="arabic simple" start="2">
<li><p>Menghitung Probabilitas Awal (Prior Probability)</p></li>
</ol>
<p>Model akan terlebih dahulu menghitung distribusi awal kelas target — misalnya berapa banyak desain bangunan dalam kategori “Rendah”, “Sedang”, atau “Tinggi” untuk masing-masing kebutuhan energi. Ini memberikan gambaran awal tentang sebaran kelas sebelum fitur dipertimbangkan.</p>
<ol class="arabic simple" start="3">
<li><p>Menghitung Rata-rata dan Standar Deviasi Setiap Fitur</p></li>
</ol>
<p>Untuk setiap kelas (<code class="docutils literal notranslate"><span class="pre">Rendah</span></code>, <code class="docutils literal notranslate"><span class="pre">Sedang</span></code>, dan <code class="docutils literal notranslate"><span class="pre">Tinggi</span></code>), dihitung:</p>
<ul class="simple">
<li><p>Rata-rata (mean) dari setiap fitur bangunan (misalnya kompaksi relatif, luas permukaan, dan lainnya).</p></li>
<li><p>Standar deviasi (std) untuk mengukur penyebaran data pada tiap kelas.</p></li>
</ul>
<p>Langkah ini penting karena Gaussian Naive Bayes mengasumsikan setiap fitur mengikuti distribusi normal, sehingga dua parameter ini diperlukan untuk menghitung probabilitas fitur dalam masing-masing kelas.</p>
<ol class="arabic simple" start="4">
<li><p>Menghitung Probabilitas Gaussian</p></li>
</ol>
<p>Dengan menggunakan rumus distribusi Gaussian, model menghitung probabilitas suatu nilai fitur muncul dalam masing-masing kelas target berdasarkan nilai rata-rata dan standar deviasi yang telah dihitung sebelumnya. Ini dilakukan untuk semua fitur dan semua kelas.</p>
<ol class="arabic simple" start="5">
<li><p>Menghitung Probabilitas Posterior</p></li>
</ol>
<p>Model akan mengalikan semua probabilitas fitur (berdasarkan distribusi Gaussian) dengan probabilitas awal kelas (prior), untuk menghasilkan probabilitas posterior dari masing-masing kelas.</p>
<p>Kelas dengan probabilitas posterior tertinggi akan dipilih sebagai prediksi akhir model untuk bangunan tertentu.</p>
<ol class="arabic simple" start="6">
<li><p>Menentukan Kategori Kebutuhan Energi</p></li>
</ol>
<p>Hasil akhir dari model ini adalah kategori kelas prediksi untuk kebutuhan energi bangunan — baik untuk Heating Load maupun Cooling Load — yang diklasifikasikan ke dalam kelas Rendah, Sedang, atau Tinggi, berdasarkan kombinasi semua fitur desain bangunan.</p>
<p>Kesimpulan</p>
<p>Model Gaussian Naive Bayes sangat cocok untuk data numerik yang telah dinormalisasi, seperti dalam kasus prediksi kebutuhan energi bangunan. Dengan asumsi fitur-fitur input bersifat independen dan berdistribusi normal, model ini mampu memberikan prediksi cepat dan efisien tanpa memerlukan proses pelatihan yang rumit. Dalam konteks ini, model dapat membantu arsitek dan perancang bangunan memahami secara awal estimasi kebutuhan energi dari desain yang dibuat, sebagai dasar dalam upaya efisiensi energi.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="nn">sklearn.multioutput</span> <span class="kn">import</span> <span class="n">MultiOutputClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">confusion_matrix</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># === 1. Ubah target Y1 dan Y2 ke kelas kategorikal ===</span>
<span class="n">min_y1</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="s1">&#39;Y1&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
<span class="n">max_y1</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="s1">&#39;Y1&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="n">min_y2</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="s1">&#39;Y2&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
<span class="n">max_y2</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="s1">&#39;Y2&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

<span class="n">bins_y1</span> <span class="o">=</span> <span class="p">[</span><span class="n">min_y1</span><span class="p">,</span> <span class="n">min_y1</span> <span class="o">+</span> <span class="p">(</span><span class="n">max_y1</span> <span class="o">-</span> <span class="n">min_y1</span><span class="p">)</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_y1</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">max_y1</span> <span class="o">-</span> <span class="n">min_y1</span><span class="p">)</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_y1</span><span class="p">]</span>
<span class="n">bins_y2</span> <span class="o">=</span> <span class="p">[</span><span class="n">min_y2</span><span class="p">,</span> <span class="n">min_y2</span> <span class="o">+</span> <span class="p">(</span><span class="n">max_y2</span> <span class="o">-</span> <span class="n">min_y2</span><span class="p">)</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_y2</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">max_y2</span> <span class="o">-</span> <span class="n">min_y2</span><span class="p">)</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_y2</span><span class="p">]</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Rendah&#39;</span><span class="p">,</span> <span class="s1">&#39;Sedang&#39;</span><span class="p">,</span> <span class="s1">&#39;Tinggi&#39;</span><span class="p">]</span>

<span class="n">y_class_multi</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">y_class_multi</span><span class="p">[</span><span class="s1">&#39;Y1_class&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="s1">&#39;Y1&#39;</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins_y1</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">include_lowest</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y_class_multi</span><span class="p">[</span><span class="s1">&#39;Y2_class&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="s1">&#39;Y2&#39;</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins_y2</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">include_lowest</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># === 2. Split data untuk klasifikasi ===</span>
<span class="n">X_train_nb</span><span class="p">,</span> <span class="n">X_test_nb</span><span class="p">,</span> <span class="n">y_train_nb</span><span class="p">,</span> <span class="n">y_test_nb</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y_class_multi</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="c1"># === 3. Buat dan latih model Naive Bayes multi-output ===</span>
<span class="n">nb</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">multi_nb</span> <span class="o">=</span> <span class="n">MultiOutputClassifier</span><span class="p">(</span><span class="n">nb</span><span class="p">)</span>
<span class="n">multi_nb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_nb</span><span class="p">,</span> <span class="n">y_train_nb</span><span class="p">)</span>

<span class="c1"># === 4. Prediksi</span>
<span class="n">y_pred_nb</span> <span class="o">=</span> <span class="n">multi_nb</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_nb</span><span class="p">)</span>
<span class="n">y_pred_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">y_pred_nb</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Y1_pred&#39;</span><span class="p">,</span> <span class="s1">&#39;Y2_pred&#39;</span><span class="p">])</span>

<span class="c1"># === 5. Evaluasi Y1</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Evaluasi Y1 (Heating Load):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Akurasi:&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test_nb</span><span class="p">[</span><span class="s1">&#39;Y1_class&#39;</span><span class="p">],</span> <span class="n">y_pred_df</span><span class="p">[</span><span class="s1">&#39;Y1_pred&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test_nb</span><span class="p">[</span><span class="s1">&#39;Y1_class&#39;</span><span class="p">],</span> <span class="n">y_pred_df</span><span class="p">[</span><span class="s1">&#39;Y1_pred&#39;</span><span class="p">]))</span>

<span class="c1"># Confusion matrix untuk Y1</span>
<span class="n">cm_y1</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test_nb</span><span class="p">[</span><span class="s1">&#39;Y1_class&#39;</span><span class="p">],</span> <span class="n">y_pred_df</span><span class="p">[</span><span class="s1">&#39;Y1_pred&#39;</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cm_y1</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Oranges&#39;</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix - Naive Bayes Y1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Predicted&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Actual&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># === 6. Evaluasi Y2</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Evaluasi Y2 (Cooling Load):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Akurasi:&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test_nb</span><span class="p">[</span><span class="s1">&#39;Y2_class&#39;</span><span class="p">],</span> <span class="n">y_pred_df</span><span class="p">[</span><span class="s1">&#39;Y2_pred&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test_nb</span><span class="p">[</span><span class="s1">&#39;Y2_class&#39;</span><span class="p">],</span> <span class="n">y_pred_df</span><span class="p">[</span><span class="s1">&#39;Y2_pred&#39;</span><span class="p">]))</span>

<span class="c1"># Confusion matrix untuk Y2</span>
<span class="n">cm_y2</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test_nb</span><span class="p">[</span><span class="s1">&#39;Y2_class&#39;</span><span class="p">],</span> <span class="n">y_pred_df</span><span class="p">[</span><span class="s1">&#39;Y2_pred&#39;</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cm_y2</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Purples&#39;</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix - Naive Bayes Y2&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Predicted&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Actual&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Evaluasi Y1 (Heating Load):
Akurasi: 0.7207792207792207
              precision    recall  f1-score   support

      Rendah       0.94      0.99      0.96        69
      Sedang       0.00      0.00      0.00        42
      Tinggi       0.52      1.00      0.69        43

    accuracy                           0.72       154
   macro avg       0.49      0.66      0.55       154
weighted avg       0.57      0.72      0.62       154
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f&quot;{metric.capitalize()} is&quot;, len(result))
/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f&quot;{metric.capitalize()} is&quot;, len(result))
/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f&quot;{metric.capitalize()} is&quot;, len(result))
</pre></div>
</div>
<img alt="_images/bcdf010d177a67a3e0fe150790a65b865c13df63ad7cbe21ec0ed88a63daeb37.png" src="_images/bcdf010d177a67a3e0fe150790a65b865c13df63ad7cbe21ec0ed88a63daeb37.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Evaluasi Y2 (Cooling Load):
Akurasi: 0.8831168831168831
              precision    recall  f1-score   support

      Rendah       1.00      0.99      0.99        73
      Sedang       0.83      0.86      0.84        57
      Tinggi       0.65      0.62      0.64        24

    accuracy                           0.88       154
   macro avg       0.83      0.82      0.83       154
weighted avg       0.88      0.88      0.88       154
</pre></div>
</div>
<img alt="_images/fd464dbe81b10a2c951eab189b2d411520442e41bba4ff208843d243a65f9037.png" src="_images/fd464dbe81b10a2c951eab189b2d411520442e41bba4ff208843d243a65f9037.png" />
</div>
</div>
</section>
<section id="evaluasi">
<h2>Evaluasi<a class="headerlink" href="#evaluasi" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">f1_score</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="nn">sklearn.multioutput</span> <span class="kn">import</span> <span class="n">MultiOutputClassifier</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># --- Model ---</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;Decision Tree&#39;</span><span class="p">:</span> <span class="n">MultiOutputClassifier</span><span class="p">(</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)),</span>
    <span class="s1">&#39;KNN&#39;</span><span class="p">:</span> <span class="n">MultiOutputClassifier</span><span class="p">(</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)),</span>
    <span class="s1">&#39;Naive Bayes&#39;</span><span class="p">:</span> <span class="n">MultiOutputClassifier</span><span class="p">(</span><span class="n">GaussianNB</span><span class="p">())</span>
<span class="p">}</span>

<span class="c1"># --- Evaluasi per model ---</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_multi</span><span class="p">,</span> <span class="n">y_train_multi</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_multi</span><span class="p">)</span>
    <span class="n">y_pred_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Y1_pred&#39;</span><span class="p">,</span> <span class="s1">&#39;Y2_pred&#39;</span><span class="p">])</span>

    <span class="c1"># Evaluasi per target</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="s1">&#39;Y1_class&#39;</span><span class="p">,</span> <span class="s1">&#39;Y2_class&#39;</span><span class="p">]):</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test_multi</span><span class="p">[</span><span class="n">target</span><span class="p">],</span> <span class="n">y_pred_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span>
        <span class="n">precision</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y_test_multi</span><span class="p">[</span><span class="n">target</span><span class="p">],</span> <span class="n">y_pred_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">recall</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y_test_multi</span><span class="p">[</span><span class="n">target</span><span class="p">],</span> <span class="n">y_pred_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">f1</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y_test_multi</span><span class="p">[</span><span class="n">target</span><span class="p">],</span> <span class="n">y_pred_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
            <span class="s1">&#39;Model&#39;</span><span class="p">:</span> <span class="n">model_name</span><span class="p">,</span>
            <span class="s1">&#39;Target&#39;</span><span class="p">:</span> <span class="n">target</span><span class="p">,</span>
            <span class="s1">&#39;Akurasi&#39;</span><span class="p">:</span> <span class="n">accuracy</span><span class="p">,</span>
            <span class="s1">&#39;Precision&#39;</span><span class="p">:</span> <span class="n">precision</span><span class="p">,</span>
            <span class="s1">&#39;Recall&#39;</span><span class="p">:</span> <span class="n">recall</span><span class="p">,</span>
            <span class="s1">&#39;F1 Score&#39;</span><span class="p">:</span> <span class="n">f1</span>
        <span class="p">})</span>

<span class="c1"># --- Buat DataFrame hasil evaluasi ---</span>
<span class="n">evaluation_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="c1"># Tampilkan Hasil</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== Hasil Evaluasi Multi-Output Model (Y1 dan Y2) ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">evaluation_df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>=== Hasil Evaluasi Multi-Output Model (Y1 dan Y2) ===
           Model    Target   Akurasi  Precision    Recall  F1 Score
0  Decision Tree  Y1_class  0.980519   0.977778  0.979665  0.978136
1  Decision Tree  Y2_class  0.935065   0.933013  0.869152  0.889344
2            KNN  Y1_class  0.915584   0.899946  0.897933  0.898881
3            KNN  Y2_class  0.909091   0.867846  0.863124  0.865273
4    Naive Bayes  Y1_class  0.720779   0.489612  0.661836  0.550846
5    Naive Bayes  Y2_class  0.883117   0.827561  0.823650  0.825410
</pre></div>
</div>
</div>
</div>
<p>Berdasarkan hasil evaluasi terhadap tiga model klasifikasi multi-output (Decision Tree, K-Nearest Neighbors, dan Naive Bayes) untuk memprediksi kategori beban pemanasan (Y1_class) dan beban pendinginan (Y2_class), dapat disimpulkan bahwa model <strong>Decision Tree</strong> menunjukkan kinerja terbaik secara keseluruhan. Model ini menghasilkan akurasi tinggi untuk kedua target, dengan nilai F1 Score sebesar 97,81% untuk Y1 dan 88,93% untuk Y2. Hal ini menunjukkan bahwa Decision Tree mampu mengenali pola secara konsisten dan akurat.</p>
<p>Sementara itu, model <strong>K-Nearest Neighbors (KNN)</strong> juga menunjukkan performa yang cukup baik, terutama dalam memprediksi beban pendinginan (Y2), dengan akurasi tertinggi mencapai 99,09%. Namun, meskipun akurasi tinggi, nilai precision dan recall-nya sedikit lebih rendah dibandingkan Decision Tree, menandakan bahwa prediksi KNN kurang merata pada seluruh kelas.</p>
<p>Di sisi lain, model <strong>Naive Bayes</strong> memperlihatkan performa yang paling rendah, khususnya dalam memprediksi Y1, di mana akurasinya hanya mencapai 72,08% dan F1 Score berada di angka 55,08%. Hal ini mengindikasikan bahwa asumsi dasar model Naive Bayes (independensi antar fitur dan distribusi Gaussian) kurang cocok diterapkan pada karakteristik data ini.</p>
<p>Secara keseluruhan, <strong>Decision Tree direkomendasikan sebagai model utama</strong> untuk klasifikasi kebutuhan energi bangunan karena mampu memberikan hasil yang stabil, akurat, dan seimbang antara kedua target variabel.</p>
</section>
<section id="deploy">
<h2>Deploy<a class="headerlink" href="#deploy" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">joblib</span>
<span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">files</span>

<span class="c1"># Misalnya model terbaik Anda adalah Decision Tree</span>
<span class="c1"># dan Anda sudah melatihnya dengan nama clf_dt</span>
<span class="c1"># serta menggunakan MinMaxScaler bernama scaler</span>

<span class="c1"># Simpan model dan scaler</span>
<span class="n">joblib</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="s1">&#39;model_energy_efficiency.pkl&#39;</span><span class="p">)</span>   <span class="c1"># Simpan model Decision Tree</span>
<span class="n">joblib</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">scaler</span><span class="p">,</span> <span class="s1">&#39;scaler_energy.pkl&#39;</span><span class="p">)</span>             <span class="c1"># Simpan scaler MinMax</span>

<span class="c1"># Unduh file dari Colab</span>
<span class="n">files</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;model_energy_efficiency.pkl&#39;</span><span class="p">)</span>        <span class="c1"># Unduh model</span>
<span class="n">files</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;scaler_energy.pkl&#39;</span><span class="p">)</span>                  <span class="c1"># Unduh scaler</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">14</span><span class="p">],</span> <span class="n">line</span> <span class="mi">2</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="kn">import</span> <span class="nn">joblib</span>
<span class="ne">----&gt; </span><span class="mi">2</span> <span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">files</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="c1"># Misalnya model terbaik Anda adalah Decision Tree</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="c1"># dan Anda sudah melatihnya dengan nama clf_dt</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> <span class="c1"># serta menggunakan MinMaxScaler bernama scaler</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span> 
<span class="g g-Whitespace">      </span><span class="mi">8</span> <span class="c1"># Simpan model dan scaler</span>
<span class="g g-Whitespace">      </span><span class="mi">9</span> <span class="n">joblib</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="s1">&#39;model_energy_efficiency.pkl&#39;</span><span class="p">)</span>   <span class="c1"># Simpan model Decision Tree</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;google&#39;
</pre></div>
</div>
</div>
</div>
</section>
<section id="hasil-web">
<h2>Hasil Web<a class="headerlink" href="#hasil-web" title="Link to this heading">#</a></h2>
<p>link web: <a class="reference external" href="https://j65zedxyhq6yjlz88xsl2w.streamlit.app/">https://j65zedxyhq6yjlz88xsl2w.streamlit.app/</a></p>
<p>berikut adalah hasil webnya terdapat form untuk memprediksinya.</p>
<p><img alt="image.png" src="data:image/png;base64,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" /></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="uaspra.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Pra UAS</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">UAS</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#efisiensi-energi">Efisiensi Energi</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-understanding">Data Understanding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sumber-data-set">Sumber Data Set</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#penjelasan-fitur-dan-variabel">Penjelasan Fitur dan Variabel</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#integrasi-data">Integrasi Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visulaisasi-data">Visulaisasi Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#missing-value">Missing Value</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocessing-data">Preprocessing Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalisasi-data">Normalisasi Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#split-data">Split Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reduksi-dimensi-dengan-pca">Reduksi Dimensi dengan PCA</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hasil-pre-processing">Hasil Pre Processing</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modelling">Modelling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pemodelan-prediksi-efisiensi-energi-bangunan-menggunakan-decision-tree">Pemodelan Prediksi Efisiensi Energi Bangunan Menggunakan Decision Tree</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pemodelan-kebutuhan-energi-bangunan-menggunakan-k-nearest-neighbors-knn">Pemodelan Kebutuhan Energi Bangunan Menggunakan K-Nearest Neighbors (KNN)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pemodelan-prediksi-kebutuhan-energi-bangunan-dengan-gaussian-naive-bayes">Pemodelan Prediksi Kebutuhan Energi Bangunan dengan Gaussian Naive Bayes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluasi">Evaluasi</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deploy">Deploy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hasil-web">Hasil Web</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Muhammad Umar Faruq - 008
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>