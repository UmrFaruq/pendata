
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Pra UAS &#8212; Penambangan Data</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'uaspra';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Teknik Binning" href="Binning.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/foto.png" class="logo__image only-light" alt="Penambangan Data - Home"/>
    <script>document.write(`<img src="_static/foto.png" class="logo__image only-dark" alt="Penambangan Data - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Penambangan Data
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Tugas1.html">Tugas 1 Pendata</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Tugas2.html">Penjelasan Outlier Deteksi</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="knn.html">Deteksi Outlier dengan K-Nearest Neighbors (KNN) dalam Data Understanding</a></li>
<li class="toctree-l2"><a class="reference internal" href="LOF.html">Deteksi Outlier dengan metode Local Outlier Factor (LOF) dalam Data Understanding</a></li>

</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="Uts.html">UTS Pendata</a></li>
<li class="toctree-l1"><a class="reference internal" href="k-means%20%281%29.html"><strong>Algoritma <em>K-Means</em></strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="Fuzzy_CMeans.html"><strong>FUZZY C-MEANS</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="DecisionnTree.html">Decision Tree</a></li>
<li class="toctree-l1"><a class="reference internal" href="Binning.html">Teknik Binning</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Pra UAS</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fuaspra.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/uaspra.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Pra UAS</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kekambuhan-kanker-tiroid-terdiferensiasi">Kekambuhan Kanker Tiroid Terdiferensiasi</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-understanding">Data Understanding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sumber-data-set">Sumber Data Set</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#penjelasan-fitur-dan-variabel">Penjelasan Fitur dan Variabel</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#integrasi-data">Integrasi Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualisasi-data">Visualisasi Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#missing-value">Missing Value</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocessing-data">Preprocessing Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transformasi-data">Transformasi Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalisasi-data">Normalisasi Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#split-data">Split Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hasil-data-setelah-di-preprocessing">Hasil Data setelah di Preprocessing</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modeling">Modeling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pemodelan-prediksi-kekambuhan-kanker-tiroid-menggunakan-decision-tree">Pemodelan Prediksi Kekambuhan Kanker Tiroid Menggunakan Decision Tree</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluasi-model">4. Evaluasi Model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pemodelan-kekambuhan-kanker-tiroid-dengan-k-nearest-neighbors-knn">Pemodelan Kekambuhan Kanker Tiroid dengan K-Nearest Neighbors (KNN)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pemodelan-prediksi-kekambuhan-kanker-tiroid-dengan-gaussian-naive-bayes">Pemodelan Prediksi Kekambuhan Kanker Tiroid dengan Gaussian Naive Bayes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kesimpulan">Kesimpulan</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluasi">Evaluasi</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="pra-uas">
<h1>Pra UAS<a class="headerlink" href="#pra-uas" title="Link to this heading">#</a></h1>
<section id="kekambuhan-kanker-tiroid-terdiferensiasi">
<h2>Kekambuhan Kanker Tiroid Terdiferensiasi<a class="headerlink" href="#kekambuhan-kanker-tiroid-terdiferensiasi" title="Link to this heading">#</a></h2>
</section>
<section id="data-understanding">
<h2>Data Understanding<a class="headerlink" href="#data-understanding" title="Link to this heading">#</a></h2>
</section>
<section id="sumber-data-set">
<h2>Sumber Data Set<a class="headerlink" href="#sumber-data-set" title="Link to this heading">#</a></h2>
<p>Data Set Diambil dari :</p>
<p><a class="reference external" href="https://archive.ics.uci.edu/dataset/915/differentiated+thyroid+cancer+recurrence">https://archive.ics.uci.edu/dataset/915/differentiated+thyroid+cancer+recurrence</a></p>
<p>Borzooei, S. &amp; Tarokhian, A. (2023). Differentiated Thyroid Cancer Recurrence [Dataset].UCI Machine Learning Repository.</p>
<p><a class="reference external" href="https://doi.org/10.24432/C5632J">https://doi.org/10.24432/C5632J</a>.</p>
</section>
<section id="penjelasan-fitur-dan-variabel">
<h2>Penjelasan Fitur dan Variabel<a class="headerlink" href="#penjelasan-fitur-dan-variabel" title="Link to this heading">#</a></h2>
<p>Dataset ini digunakan untuk memprediksi apakah pasien kanker tiroid mengalami kekambuhan setelah menjalani pengobatan. Setiap baris berisi data satu pasien, dengan sejumlah fitur klinis dan riwayat terapi.</p>
<p>Fitur-fitur yang digunakan antara lain:</p>
<ul class="simple">
<li><p>Usia (Age) dan Jenis kelamin (Gender) pasien,</p></li>
<li><p>Stadium kanker (T_stage, N_stage, M_stage) sesuai sistem TNM,</p></li>
<li><p>Lokasi utama tumor (Primary_site) dan tipe histologis kanker (Histologic_type),</p></li>
<li><p>Jenis operasi (Surgery_type) serta riwayat terapi radiasi dan kemoterapi,</p></li>
<li><p>Stadium patologi (Pathologic_stage),</p></li>
<li><p>Ukuran tumor (Tumor_size), jumlah kelenjar getah bening yang diperiksa (Lymph_node_examined_count) dan yang positif kanker (Positive_LN_count),</p></li>
<li><p>Serta penyebaran tumor ke jaringan sekitarnya (Extension).</p></li>
</ul>
<p>Sementara itu, variabel target adalah Recurrence, yaitu status kekambuhan kanker (1 = kambuh, 0 = tidak kambuh).</p>
</section>
<section id="integrasi-data">
<h2>Integrasi Data<a class="headerlink" href="#integrasi-data" title="Link to this heading">#</a></h2>
<p>untuk mengambil data agar dapat diolah, perlu untuk menginstall package yang telah disediakan oleh UCI Dataset. Instalasi dilakukan berguna untuk menarik data yang berasal dari UCI dataset agar dapat diolah. peritah untuk mengambil data dari UCI dataset dapat di lihat ketika menekan tombol import in python pada datase yang diinginkan dan ikuti perintah tersebut agar data dapat diambil dari UCI dataset. Contoh pengambilan data dari UCI dataset dapat dilihat pada gambar dan perintah berikut:</p>
<p><img alt="image.png" src="data:image/png;base64,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" /></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">ucimlrepo</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: ucimlrepo in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.0.7)
Requirement already satisfied: pandas&gt;=1.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from ucimlrepo) (2.2.3)
Requirement already satisfied: certifi&gt;=2020.12.5 in /home/codespace/.local/lib/python3.12/site-packages (from ucimlrepo) (2024.8.30)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: numpy&gt;=1.26.0 in /home/codespace/.local/lib/python3.12/site-packages (from pandas&gt;=1.0.0-&gt;ucimlrepo) (2.2.0)
Requirement already satisfied: python-dateutil&gt;=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas&gt;=1.0.0-&gt;ucimlrepo) (2.9.0.post0)
Requirement already satisfied: pytz&gt;=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas&gt;=1.0.0-&gt;ucimlrepo) (2024.2)
Requirement already satisfied: tzdata&gt;=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas&gt;=1.0.0-&gt;ucimlrepo) (2024.2)
Requirement already satisfied: six&gt;=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas&gt;=1.0.0-&gt;ucimlrepo) (1.17.0)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">[</span><span class=" -Color -Color-Blue">notice</span><span class=" -Color -Color-Bold">]</span> A new release of pip is available: <span class=" -Color -Color-Red">24.3.1</span> -&gt; <span class=" -Color -Color-Green">25.1.1</span>
<span class=" -Color -Color-Bold">[</span><span class=" -Color -Color-Blue">notice</span><span class=" -Color -Color-Bold">]</span> To update, run: <span class=" -Color -Color-Green">python3 -m pip install --upgrade pip</span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Note: you may need to restart the kernel to use updated packages.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ucimlrepo</span> <span class="kn">import</span> <span class="n">fetch_ucirepo</span>

<span class="c1"># fetch dataset</span>
<span class="n">differentiated_thyroid_cancer_recurrence</span> <span class="o">=</span> <span class="n">fetch_ucirepo</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">915</span><span class="p">)</span>

<span class="c1"># data (as pandas dataframes)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">differentiated_thyroid_cancer_recurrence</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">features</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">differentiated_thyroid_cancer_recurrence</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">targets</span>

 <span class="c1">## data (as pandas dataframes)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">differentiated_thyroid_cancer_recurrence</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">original</span>
  <span class="c1"># menyimpan hasil komputasi ke dalam csv</span>
<span class="n">data</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s2">&quot;differentiated_thyroid_cancer_recurrence.csv&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">info</span><span class="p">())</span> <span class="c1">#untuk menampilkan info fitur-fitur yang ada di tabel</span>

<span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">())</span> <span class="c1">#untuk menampilkan 5 baris pertama</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 383 entries, 0 to 382
Data columns (total 17 columns):
 #   Column                Non-Null Count  Dtype 
---  ------                --------------  ----- 
 0   Age                   383 non-null    int64 
 1   Gender                383 non-null    object
 2   Smoking               383 non-null    object
 3   Hx Smoking            383 non-null    object
 4   Hx Radiothreapy       383 non-null    object
 5   Thyroid Function      383 non-null    object
 6   Physical Examination  383 non-null    object
 7   Adenopathy            383 non-null    object
 8   Pathology             383 non-null    object
 9   Focality              383 non-null    object
 10  Risk                  383 non-null    object
 11  T                     383 non-null    object
 12  N                     383 non-null    object
 13  M                     383 non-null    object
 14  Stage                 383 non-null    object
 15  Response              383 non-null    object
 16  Recurred              383 non-null    object
dtypes: int64(1), object(16)
memory usage: 51.0+ KB
None
   Age Gender Smoking Hx Smoking Hx Radiothreapy Thyroid Function  \
0   27      F      No         No              No        Euthyroid   
1   34      F      No        Yes              No        Euthyroid   
2   30      F      No         No              No        Euthyroid   
3   62      F      No         No              No        Euthyroid   
4   62      F      No         No              No        Euthyroid   

          Physical Examination Adenopathy       Pathology     Focality Risk  \
0   Single nodular goiter-left         No  Micropapillary    Uni-Focal  Low   
1          Multinodular goiter         No  Micropapillary    Uni-Focal  Low   
2  Single nodular goiter-right         No  Micropapillary    Uni-Focal  Low   
3  Single nodular goiter-right         No  Micropapillary    Uni-Focal  Low   
4          Multinodular goiter         No  Micropapillary  Multi-Focal  Low   

     T   N   M Stage       Response Recurred  
0  T1a  N0  M0     I  Indeterminate       No  
1  T1a  N0  M0     I      Excellent       No  
2  T1a  N0  M0     I      Excellent       No  
3  T1a  N0  M0     I      Excellent       No  
4  T1a  N0  M0     I      Excellent       No  
</pre></div>
</div>
</div>
</div>
</section>
<section id="visualisasi-data">
<h2>Visualisasi Data<a class="headerlink" href="#visualisasi-data" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="c1">#display dataset</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Gender</th>
      <th>Smoking</th>
      <th>Hx Smoking</th>
      <th>Hx Radiothreapy</th>
      <th>Thyroid Function</th>
      <th>Physical Examination</th>
      <th>Adenopathy</th>
      <th>Pathology</th>
      <th>Focality</th>
      <th>Risk</th>
      <th>T</th>
      <th>N</th>
      <th>M</th>
      <th>Stage</th>
      <th>Response</th>
      <th>Recurred</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>27</td>
      <td>F</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>Euthyroid</td>
      <td>Single nodular goiter-left</td>
      <td>No</td>
      <td>Micropapillary</td>
      <td>Uni-Focal</td>
      <td>Low</td>
      <td>T1a</td>
      <td>N0</td>
      <td>M0</td>
      <td>I</td>
      <td>Indeterminate</td>
      <td>No</td>
    </tr>
    <tr>
      <th>1</th>
      <td>34</td>
      <td>F</td>
      <td>No</td>
      <td>Yes</td>
      <td>No</td>
      <td>Euthyroid</td>
      <td>Multinodular goiter</td>
      <td>No</td>
      <td>Micropapillary</td>
      <td>Uni-Focal</td>
      <td>Low</td>
      <td>T1a</td>
      <td>N0</td>
      <td>M0</td>
      <td>I</td>
      <td>Excellent</td>
      <td>No</td>
    </tr>
    <tr>
      <th>2</th>
      <td>30</td>
      <td>F</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>Euthyroid</td>
      <td>Single nodular goiter-right</td>
      <td>No</td>
      <td>Micropapillary</td>
      <td>Uni-Focal</td>
      <td>Low</td>
      <td>T1a</td>
      <td>N0</td>
      <td>M0</td>
      <td>I</td>
      <td>Excellent</td>
      <td>No</td>
    </tr>
    <tr>
      <th>3</th>
      <td>62</td>
      <td>F</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>Euthyroid</td>
      <td>Single nodular goiter-right</td>
      <td>No</td>
      <td>Micropapillary</td>
      <td>Uni-Focal</td>
      <td>Low</td>
      <td>T1a</td>
      <td>N0</td>
      <td>M0</td>
      <td>I</td>
      <td>Excellent</td>
      <td>No</td>
    </tr>
    <tr>
      <th>4</th>
      <td>62</td>
      <td>F</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>Euthyroid</td>
      <td>Multinodular goiter</td>
      <td>No</td>
      <td>Micropapillary</td>
      <td>Multi-Focal</td>
      <td>Low</td>
      <td>T1a</td>
      <td>N0</td>
      <td>M0</td>
      <td>I</td>
      <td>Excellent</td>
      <td>No</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>378</th>
      <td>72</td>
      <td>M</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>Euthyroid</td>
      <td>Single nodular goiter-right</td>
      <td>Right</td>
      <td>Papillary</td>
      <td>Uni-Focal</td>
      <td>High</td>
      <td>T4b</td>
      <td>N1b</td>
      <td>M1</td>
      <td>IVB</td>
      <td>Biochemical Incomplete</td>
      <td>Yes</td>
    </tr>
    <tr>
      <th>379</th>
      <td>81</td>
      <td>M</td>
      <td>Yes</td>
      <td>No</td>
      <td>Yes</td>
      <td>Euthyroid</td>
      <td>Multinodular goiter</td>
      <td>Extensive</td>
      <td>Papillary</td>
      <td>Multi-Focal</td>
      <td>High</td>
      <td>T4b</td>
      <td>N1b</td>
      <td>M1</td>
      <td>IVB</td>
      <td>Structural Incomplete</td>
      <td>Yes</td>
    </tr>
    <tr>
      <th>380</th>
      <td>72</td>
      <td>M</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>No</td>
      <td>Euthyroid</td>
      <td>Multinodular goiter</td>
      <td>Bilateral</td>
      <td>Papillary</td>
      <td>Multi-Focal</td>
      <td>High</td>
      <td>T4b</td>
      <td>N1b</td>
      <td>M1</td>
      <td>IVB</td>
      <td>Structural Incomplete</td>
      <td>Yes</td>
    </tr>
    <tr>
      <th>381</th>
      <td>61</td>
      <td>M</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>Clinical Hyperthyroidism</td>
      <td>Multinodular goiter</td>
      <td>Extensive</td>
      <td>Hurthel cell</td>
      <td>Multi-Focal</td>
      <td>High</td>
      <td>T4b</td>
      <td>N1b</td>
      <td>M0</td>
      <td>IVA</td>
      <td>Structural Incomplete</td>
      <td>Yes</td>
    </tr>
    <tr>
      <th>382</th>
      <td>67</td>
      <td>M</td>
      <td>Yes</td>
      <td>No</td>
      <td>No</td>
      <td>Euthyroid</td>
      <td>Multinodular goiter</td>
      <td>Bilateral</td>
      <td>Papillary</td>
      <td>Multi-Focal</td>
      <td>High</td>
      <td>T4b</td>
      <td>N1b</td>
      <td>M0</td>
      <td>IVA</td>
      <td>Structural Incomplete</td>
      <td>Yes</td>
    </tr>
  </tbody>
</table>
<p>383 rows  17 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="c1"># Loop semua kolom</span>
<span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">data</span><span class="p">[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="s1">&#39;object&#39;</span> <span class="ow">or</span> <span class="n">data</span><span class="p">[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">nunique</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">:</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">countplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">column</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Distribusi Kategori: </span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">dropna</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;skyblue&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Distribusi Numerik: </span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/8054fb64ecb1ba3d803f7f659852ad8fd48b79d7feb81de31789c69844127a09.png" src="_images/8054fb64ecb1ba3d803f7f659852ad8fd48b79d7feb81de31789c69844127a09.png" />
<img alt="_images/27fd196dce4e5d7ee97b14f8735a6acfb23aafb1e5309d40b46c773d1c584320.png" src="_images/27fd196dce4e5d7ee97b14f8735a6acfb23aafb1e5309d40b46c773d1c584320.png" />
<img alt="_images/93bde189c43a3df471034f1abfd8d2127feeaf44bd24f9d0f13c298fa4956f1d.png" src="_images/93bde189c43a3df471034f1abfd8d2127feeaf44bd24f9d0f13c298fa4956f1d.png" />
<img alt="_images/41fbc2b53dfc889c534807b3319c5d7023a77091bdcf36b0aa314449285cd2cc.png" src="_images/41fbc2b53dfc889c534807b3319c5d7023a77091bdcf36b0aa314449285cd2cc.png" />
<img alt="_images/ab2ece1d53c3a2be247f4582a18f7413b0205ba809867a2887facd0716082599.png" src="_images/ab2ece1d53c3a2be247f4582a18f7413b0205ba809867a2887facd0716082599.png" />
<img alt="_images/693cf797518ad1c65d28d17160547d499a10312fc8aed3609ced3802e7aa7db0.png" src="_images/693cf797518ad1c65d28d17160547d499a10312fc8aed3609ced3802e7aa7db0.png" />
<img alt="_images/8cd7d0b890285bc8798dc63146de4a7b0ce09799178b7983c51b2d65a2935da5.png" src="_images/8cd7d0b890285bc8798dc63146de4a7b0ce09799178b7983c51b2d65a2935da5.png" />
<img alt="_images/811ecacdece96843de8a0da470ea8bca5bd14ff383d14963937a125a851242c8.png" src="_images/811ecacdece96843de8a0da470ea8bca5bd14ff383d14963937a125a851242c8.png" />
<img alt="_images/2450b956bd607dc080f780b09b5ba50888c8a26e858cccff8eff350a1c5d5894.png" src="_images/2450b956bd607dc080f780b09b5ba50888c8a26e858cccff8eff350a1c5d5894.png" />
<img alt="_images/12fb49dfba8fc86269e3cf2b26206caea52aed2789cba741f6056cd6303bacb5.png" src="_images/12fb49dfba8fc86269e3cf2b26206caea52aed2789cba741f6056cd6303bacb5.png" />
<img alt="_images/0b6bfad85edd0ce1e4f8eb5a1c8540af6263c091777849b161c1ad9948324c8c.png" src="_images/0b6bfad85edd0ce1e4f8eb5a1c8540af6263c091777849b161c1ad9948324c8c.png" />
<img alt="_images/238efb77ca3ee31625496ea0612d9d9cafa326a97d03ef0366f59be983a64d2f.png" src="_images/238efb77ca3ee31625496ea0612d9d9cafa326a97d03ef0366f59be983a64d2f.png" />
<img alt="_images/7cc75901aa6f430e440dfe37d8705ef094f69bf5fc93d0d6e3ff23b2ad483d55.png" src="_images/7cc75901aa6f430e440dfe37d8705ef094f69bf5fc93d0d6e3ff23b2ad483d55.png" />
<img alt="_images/9ffc2eded4abcc6c7af63ae8ce87384a6e40af8350402a5b74ef08fc37261cd8.png" src="_images/9ffc2eded4abcc6c7af63ae8ce87384a6e40af8350402a5b74ef08fc37261cd8.png" />
<img alt="_images/0c66e40f0aa1c1f48bdd16dd040e02dacd053b2d8275ba3816ce1ab2ea333ceb.png" src="_images/0c66e40f0aa1c1f48bdd16dd040e02dacd053b2d8275ba3816ce1ab2ea333ceb.png" />
<img alt="_images/6dbb3fb04097f31141edd24acb18d357084fdabdd1edff76662dc9d6a0d749d5.png" src="_images/6dbb3fb04097f31141edd24acb18d357084fdabdd1edff76662dc9d6a0d749d5.png" />
<img alt="_images/13d62d5396723b54b6e488057e707c90856c847b6c3d550dac7f0ed3c40dffc8.png" src="_images/13d62d5396723b54b6e488057e707c90856c847b6c3d550dac7f0ed3c40dffc8.png" />
</div>
</div>
<section id="missing-value">
<h3>Missing Value<a class="headerlink" href="#missing-value" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Detect rows with missing values</span>
<span class="n">rows_with_missing</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span>

<span class="c1"># Tampilkan baris dengan nilai yang hilang beserta ID, fitur, dan labelnya</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Rows with Missing Values:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rows_with_missing</span><span class="p">)</span>

<span class="c1"># Detect missing values</span>
<span class="n">missing_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span>

<span class="c1"># Hitung nilai yang hilang di setiap kolom</span>
<span class="n">missing_counts</span> <span class="o">=</span> <span class="n">missing_values</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="c1"># Display missing value counts</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Missing Value Counts:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">missing_counts</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Rows with Missing Values:
Empty DataFrame
Columns: [Age, Gender, Smoking, Hx Smoking, Hx Radiothreapy, Thyroid Function, Physical Examination, Adenopathy, Pathology, Focality, Risk, T, N, M, Stage, Response, Recurred]
Index: []
Missing Value Counts:
Age                     0
Gender                  0
Smoking                 0
Hx Smoking              0
Hx Radiothreapy         0
Thyroid Function        0
Physical Examination    0
Adenopathy              0
Pathology               0
Focality                0
Risk                    0
T                       0
N                       0
M                       0
Stage                   0
Response                0
Recurred                0
dtype: int64
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="preprocessing-data">
<h2>Preprocessing Data<a class="headerlink" href="#preprocessing-data" title="Link to this heading">#</a></h2>
<section id="transformasi-data">
<h3>Transformasi Data<a class="headerlink" href="#transformasi-data" title="Link to this heading">#</a></h3>
<p>Mengimpor LabelEncoder dari Scikit-learn, yaitu kelas untuk mengubah nilai teks menjadi angka</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>

<span class="c1"># Salin data agar tidak mengubah yang asli</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1"># Inisialisasi LabelEncoder</span>
<span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>

<span class="c1"># Encode semua kolom kategorikal</span>
<span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="s1">&#39;object&#39;</span><span class="p">:</span>
        <span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Transformasi kategorikal selesai.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Transformasi kategorikal selesai.
</pre></div>
</div>
</div>
</div>
</section>
<section id="normalisasi-data">
<h3>Normalisasi Data<a class="headerlink" href="#normalisasi-data" title="Link to this heading">#</a></h3>
<p>Melakukan normalisasi data numerik menggunakan Min-Max Scaling, yaitu mengubah nilai-nilai angka agar berada dalam rentang 0 hingga 1.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>

<span class="c1"># Normalisasi hanya kolom numerik</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">numerical_cols</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;int64&#39;</span><span class="p">,</span> <span class="s1">&#39;float64&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">columns</span>

<span class="n">df</span><span class="p">[</span><span class="n">numerical_cols</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">numerical_cols</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Normalisasi fitur numerik selesai.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Normalisasi fitur numerik selesai.
</pre></div>
</div>
</div>
</div>
</section>
<section id="split-data">
<h3>Split Data<a class="headerlink" href="#split-data" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Misalnya targetnya kolom &#39;Recurred&#39;</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Recurred&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Recurred&#39;</span><span class="p">]</span>

<span class="c1"># Split 80% data latih, 20% data uji</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Data berhasil di-split.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Train shape: </span><span class="si">{</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, Test shape: </span><span class="si">{</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Data berhasil di-split.
Train shape: (306, 16), Test shape: (77, 16)
</pre></div>
</div>
</div>
</div>
</section>
<section id="hasil-data-setelah-di-preprocessing">
<h3>Hasil Data setelah di Preprocessing<a class="headerlink" href="#hasil-data-setelah-di-preprocessing" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span><span class="p">,</span> <span class="n">MinMaxScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Load dataset dari file (jika belum dimuat)</span>
<span class="c1"># data = pd.read_csv(&quot;differentiated_thyroid_cancer_recurrence.csv&quot;)</span>

<span class="c1"># Tangani missing value</span>
<span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">data</span><span class="p">[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">data</span><span class="p">[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="s1">&#39;object&#39;</span><span class="p">:</span>
            <span class="n">data</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">mode</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">data</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">median</span><span class="p">())</span>

<span class="c1"># Copy dan transformasi data</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="s1">&#39;object&#39;</span><span class="p">:</span>
        <span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">])</span>

<span class="c1"># Normalisasi fitur numerik</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">numerical_cols</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;int64&#39;</span><span class="p">,</span> <span class="s1">&#39;float64&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">columns</span>
<span class="n">df</span><span class="p">[</span><span class="n">numerical_cols</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">numerical_cols</span><span class="p">])</span>

<span class="c1"># Split fitur dan target</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Recurred&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Recurred&#39;</span><span class="p">]</span>

<span class="c1"># Gabungkan kembali fitur dan target</span>
<span class="n">df_processed</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Tampilkan hasil akhir</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Total baris dataset ini adalah =&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">df_processed</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Visualisasi dataset setelah preprocessing lengkap:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_processed</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>  <span class="c1"># Menampilkan seluruh isi (hati-hati kalau datanya besar!)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total baris dataset ini adalah = 383
Visualisasi dataset setelah preprocessing lengkap:
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>     Age  Gender  Smoking  Hx Smoking  Hx Radiothreapy  Thyroid Function  Physical Examination  Adenopathy  Pathology  Focality  Risk        T   N   M  Stage  Response  Recurred
0.179104     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.666667       0.0
0.283582     0.0      0.0         1.0              0.0              0.50                  0.25         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.223881     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.701493     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.701493     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   0.666667       0.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.552239     1.0      1.0         0.0              0.0              0.50                  0.25         0.6   0.666667       0.0   1.0 0.000000 0.0 0.0   0.00  0.666667       0.0
0.388060     0.0      0.0         1.0              0.0              0.00                  1.00         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.462687     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.537313     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.373134     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.895522     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.656716     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.507463     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.522388     0.0      0.0         0.0              0.0              0.00                  0.25         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.910448     0.0      0.0         0.0              0.0              0.25                  1.00         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.402985     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.666667       0.0
0.373134     0.0      0.0         1.0              0.0              0.50                  1.00         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.432836     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.666667       0.0
0.417910     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.552239     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   0.666667       0.0   1.0 0.000000 0.0 0.0   0.00  0.666667       0.0
0.388060     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.432836     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.313433     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.820896     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.671642     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   0.666667       0.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.268657     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.417910     1.0      0.0         0.0              0.0              0.75                  0.75         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.164179     1.0      0.0         0.0              0.0              0.50                  0.75         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.388060     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.328358     0.0      0.0         1.0              0.0              1.00                  0.50         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.328358     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.223881     0.0      0.0         0.0              0.0              0.00                  0.50         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.313433     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.597015     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.552239     1.0      1.0         0.0              0.0              0.50                  0.75         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.328358     0.0      0.0         0.0              0.0              1.00                  0.75         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.238806     0.0      0.0         0.0              0.0              0.00                  0.00         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.417910     1.0      0.0         0.0              0.0              0.50                  0.25         0.6   0.666667       0.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.373134     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.283582     0.0      0.0         0.0              0.0              0.50                  0.00         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.537313     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.447761     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.074627     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.343284     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.343284     0.0      0.0         0.0              0.0              0.00                  0.00         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.268657     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   0.666667       1.0   1.0 0.000000 0.0 0.0   0.00  0.333333       0.0
0.238806     0.0      0.0         0.0              0.0              0.25                  0.25         0.6   0.666667       1.0   0.5 0.000000 0.0 0.0   0.00  0.333333       0.0
0.238806     0.0      0.0         0.0              0.0              0.50                  0.25         1.0   1.000000       0.0   0.5 0.000000 1.0 0.0   0.00  0.333333       0.0
0.164179     0.0      0.0         0.0              0.0              0.50                  0.50         0.2   1.000000       1.0   0.5 0.000000 1.0 0.0   0.00  1.000000       1.0
0.208955     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   0.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.417910     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   0.666667       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.223881     0.0      0.0         0.0              0.0              0.75                  0.25         0.6   1.000000       0.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.149254     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       0.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.179104     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.149254     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.089552     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.666667       0.0
0.417910     0.0      0.0         1.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.119403     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.119403     0.0      0.0         0.0              0.0              0.00                  0.75         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.417910     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.666667       0.0
0.134328     1.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.298507     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.582090     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.582090     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.666667       0.0
0.104478     0.0      0.0         0.0              0.0              0.75                  1.00         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.343284     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       0.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.298507     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.537313     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.104478     1.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       0.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.537313     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.373134     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.805970     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.238806     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.208955     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.223881     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.194030     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.104478     0.0      0.0         0.0              0.0              0.25                  1.00         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.208955     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.298507     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.522388     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       0.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.179104     0.0      0.0         0.0              0.0              0.00                  0.00         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.029851     0.0      0.0         1.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.179104     0.0      0.0         0.0              0.0              0.75                  1.00         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.373134     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   0.5 0.166667 0.0 0.0   0.00  0.333333       0.0
0.268657     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   0.5 0.166667 1.0 0.0   0.00  0.666667       0.0
0.149254     0.0      0.0         1.0              1.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.865672     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.166667 0.0 0.0   0.00  0.333333       0.0
0.313433     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       0.0   1.0 0.166667 0.0 0.0   0.00  0.666667       1.0
0.298507     0.0      0.0         0.0              0.0              0.50                  0.25         1.0   1.000000       0.0   0.5 0.166667 1.0 0.0   0.00  1.000000       1.0
0.238806     1.0      1.0         0.0              0.0              0.50                  0.25         1.0   1.000000       0.0   0.5 0.166667 1.0 0.0   0.00  1.000000       1.0
0.044776     0.0      0.0         0.0              0.0              0.50                  1.00         1.0   1.000000       1.0   0.5 0.166667 1.0 0.0   0.00  1.000000       1.0
0.701493     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   0.000000       0.0   0.5 0.166667 0.0 0.0   0.00  1.000000       1.0
0.701493     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   0.333333       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.358209     0.0      0.0         0.0              0.0              0.50                  0.00         0.6   0.333333       1.0   1.0 0.333333 0.0 0.0   0.00  0.666667       0.0
0.328358     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   0.333333       0.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.164179     0.0      0.0         1.0              0.0              0.75                  1.00         0.6   0.333333       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.238806     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   0.333333       1.0   1.0 0.333333 0.0 0.0   0.00  0.666667       0.0
0.134328     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   0.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.626866     0.0      1.0         0.0              0.0              0.50                  0.25         0.6   0.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.194030     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   0.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.432836     1.0      1.0         0.0              0.0              0.50                  1.00         0.6   0.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.402985     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   0.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.666667       0.0
0.179104     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   0.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.537313     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.268657     0.0      0.0         0.0              0.0              0.25                  1.00         1.0   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.402985     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.164179     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.134328     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.671642     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       0.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.671642     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.000000       0.0
0.238806     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.761194     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.432836     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.253731     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.164179     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       0.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.328358     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.268657     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.119403     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       0.0   1.0 0.333333 0.0 0.0   0.00  0.666667       0.0
0.477612     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.194030     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.328358     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.164179     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.194030     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.194030     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.432836     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.238806     0.0      0.0         1.0              0.0              0.50                  1.00         1.0   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.179104     0.0      0.0         0.0              0.0              0.25                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.611940     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.000000       0.0
0.716418     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.134328     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.000000       0.0
0.223881     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.238806     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.253731     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.194030     0.0      0.0         0.0              0.0              0.00                  0.25         0.6   1.000000       0.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.537313     0.0      0.0         1.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.074627     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.089552     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.253731     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.164179     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.402985     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.000000       0.0
0.194030     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.388060     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.402985     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.507463     0.0      0.0         1.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.208955     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.208955     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.149254     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.666667       0.0
0.388060     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.666667       0.0
0.268657     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.179104     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.522388     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.059701     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.298507     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.716418     0.0      0.0         0.0              0.0              0.50                  0.50         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.000000       0.0
0.134328     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.313433     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.238806     1.0      1.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.134328     1.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.268657     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.134328     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.194030     0.0      0.0         0.0              0.0              0.25                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.104478     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.179104     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.194030     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.666667       0.0
0.208955     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       0.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.373134     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       0.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.238806     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.597015     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.089552     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.373134     1.0      1.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.343284     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.089552     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.238806     1.0      1.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.223881     0.0      0.0         0.0              0.0              0.50                  0.50         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.666667       0.0
0.522388     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.666667       0.0
0.283582     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.447761     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.552239     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.343284     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.776119     1.0      1.0         0.0              0.0              0.50                  0.25         0.6   1.000000       0.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.850746     0.0      0.0         0.0              0.0              1.00                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.666667       0.0
0.447761     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       0.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.447761     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       0.0   1.0 0.333333 0.0 0.0   0.00  0.000000       0.0
0.164179     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.776119     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.611940     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.223881     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.283582     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.522388     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.402985     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.298507     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       0.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.119403     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       0.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.432836     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       0.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.119403     0.0      0.0         0.0              0.0              0.50                  0.75         0.4   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.164179     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.686567     1.0      0.0         0.0              0.0              1.00                  0.75         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.104478     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.238806     0.0      0.0         0.0              0.0              0.00                  0.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.791045     0.0      0.0         0.0              0.0              0.00                  0.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.626866     1.0      0.0         0.0              0.0              0.00                  0.75         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.333333       0.0
0.179104     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       0.0   1.0 0.333333 1.0 0.0   0.00  0.333333       0.0
0.149254     0.0      0.0         0.0              0.0              0.50                  1.00         1.0   1.000000       0.0   1.0 0.333333 1.0 0.0   0.00  0.333333       0.0
0.074627     1.0      0.0         0.0              0.0              1.00                  0.25         0.6   1.000000       0.0   0.5 0.333333 1.0 0.0   0.00  0.333333       0.0
0.268657     1.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 1.0 0.0   0.00  0.333333       0.0
0.313433     0.0      0.0         0.0              0.0              0.50                  1.00         1.0   1.000000       1.0   1.0 0.333333 1.0 0.0   0.00  0.333333       0.0
0.313433     0.0      0.0         0.0              0.0              0.50                  1.00         1.0   1.000000       1.0   1.0 0.333333 1.0 0.0   0.00  0.666667       0.0
0.373134     0.0      0.0         0.0              0.0              0.50                  0.25         0.4   1.000000       1.0   0.5 0.333333 1.0 0.0   0.00  0.666667       0.0
0.029851     0.0      0.0         0.0              0.0              0.50                  0.25         1.0   1.000000       1.0   0.5 0.333333 1.0 0.0   0.00  0.666667       0.0
0.134328     0.0      0.0         0.0              0.0              0.25                  0.25         0.0   1.000000       0.0   0.5 0.333333 1.0 0.0   0.00  0.333333       0.0
0.343284     0.0      0.0         0.0              0.0              0.50                  1.00         1.0   1.000000       0.0   0.5 0.333333 1.0 0.0   0.00  0.333333       0.0
0.194030     0.0      0.0         0.0              0.0              0.50                  0.25         1.0   1.000000       1.0   0.5 0.333333 1.0 0.0   0.00  0.333333       0.0
0.313433     0.0      0.0         0.0              0.0              0.50                  1.00         1.0   1.000000       1.0   0.5 0.333333 1.0 0.0   0.00  0.666667       0.0
0.522388     0.0      0.0         0.0              0.0              0.50                  1.00         1.0   1.000000       1.0   0.5 0.333333 1.0 0.0   0.00  0.666667       0.0
0.537313     1.0      0.0         0.0              0.0              0.00                  1.00         1.0   1.000000       1.0   0.5 0.333333 1.0 0.0   0.00  0.666667       0.0
0.597015     0.0      0.0         0.0              0.0              1.00                  0.25         0.0   1.000000       0.0   0.5 0.333333 1.0 0.0   0.25  0.666667       0.0
0.238806     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.5 0.0   0.00  0.333333       0.0
0.268657     0.0      0.0         0.0              0.0              0.25                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.5 0.0   0.00  0.666667       0.0
0.194030     0.0      1.0         0.0              0.0              0.00                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.5 0.0   0.00  0.666667       0.0
0.492537     1.0      0.0         0.0              0.0              0.50                  0.75         0.4   1.000000       1.0   0.5 0.333333 0.5 0.0   0.00  0.666667       0.0
0.373134     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.5 0.0   0.00  0.333333       0.0
0.208955     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.333333 0.5 0.0   0.00  0.666667       0.0
0.074627     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   0.5 0.333333 0.5 0.0   0.00  0.333333       0.0
0.298507     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.666667       0.0
0.611940     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  1.000000       1.0
0.074627     1.0      0.0         0.0              0.0              0.50                  1.00         1.0   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  1.000000       1.0
0.701493     0.0      0.0         0.0              0.0              0.25                  0.25         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.000000       1.0
0.029851     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   0.0 0.333333 0.0 0.0   0.00  1.000000       1.0
0.089552     0.0      0.0         0.0              0.0              0.50                  1.00         1.0   1.000000       1.0   1.0 0.333333 1.0 0.0   0.00  1.000000       1.0
0.074627     1.0      0.0         0.0              0.0              0.50                  1.00         1.0   1.000000       1.0   0.5 0.333333 1.0 0.0   0.00  1.000000       1.0
0.373134     1.0      1.0         0.0              0.0              0.50                  0.25         0.0   1.000000       0.0   0.5 0.333333 1.0 0.0   0.00  1.000000       1.0
0.343284     1.0      1.0         1.0              0.0              0.50                  0.25         1.0   1.000000       0.0   0.5 0.333333 1.0 0.0   0.00  1.000000       1.0
0.089552     0.0      0.0         0.0              0.0              0.50                  0.75         0.4   1.000000       1.0   0.5 0.333333 1.0 0.0   0.00  1.000000       1.0
0.238806     0.0      0.0         0.0              0.0              0.50                  0.75         0.4   1.000000       1.0   0.5 0.333333 1.0 0.0   0.00  1.000000       1.0
0.283582     0.0      0.0         0.0              0.0              0.50                  1.00         1.0   1.000000       1.0   0.5 0.333333 1.0 0.0   0.00  1.000000       1.0
0.671642     0.0      0.0         0.0              0.0              0.50                  0.25         0.0   1.000000       0.0   0.5 0.333333 1.0 0.0   0.00  1.000000       1.0
0.671642     0.0      0.0         0.0              0.0              0.50                  1.00         1.0   1.000000       0.0   0.5 0.333333 1.0 0.0   0.25  1.000000       1.0
0.701493     0.0      0.0         0.0              0.0              0.50                  1.00         1.0   1.000000       0.0   0.5 0.333333 1.0 0.0   0.25  1.000000       1.0
0.313433     1.0      0.0         0.0              0.0              0.00                  0.25         0.6   1.000000       1.0   0.5 0.333333 0.5 0.0   0.00  0.666667       1.0
0.208955     0.0      1.0         0.0              0.0              0.50                  0.75         0.6   0.000000       1.0   1.0 0.333333 0.0 0.0   0.00  1.000000       1.0
0.268657     1.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  1.000000       1.0
0.895522     1.0      1.0         0.0              1.0              0.50                  0.75         0.6   0.000000       0.0   0.0 0.333333 0.0 1.0   1.00  1.000000       1.0
0.701493     1.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  1.000000       1.0
0.611940     1.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.333333 0.0 0.0   0.00  0.000000       1.0
0.552239     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   0.000000       1.0   1.0 0.500000 0.0 0.0   0.00  0.000000       0.0
0.298507     0.0      0.0         0.0              0.0              0.00                  0.25         0.6   1.000000       1.0   1.0 0.500000 0.0 0.0   0.00  0.333333       0.0
0.283582     0.0      0.0         0.0              0.0              0.50                  0.25         0.0   1.000000       1.0   0.5 0.500000 1.0 0.0   0.00  0.666667       0.0
0.253731     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   0.333333       0.0   1.0 0.500000 0.0 0.0   0.00  0.333333       0.0
0.179104     0.0      0.0         0.0              0.0              0.25                  1.00         0.6   0.333333       0.0   1.0 0.500000 0.0 0.0   0.00  0.333333       0.0
0.552239     0.0      1.0         0.0              0.0              0.50                  0.75         0.0   0.333333       0.0   1.0 0.500000 0.0 0.0   0.00  1.000000       0.0
0.462687     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   0.333333       0.0   1.0 0.500000 0.0 0.0   0.00  0.333333       0.0
0.223881     0.0      0.0         0.0              0.0              1.00                  0.50         1.0   0.000000       1.0   1.0 0.500000 0.0 0.0   0.00  0.333333       0.0
0.253731     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   0.000000       0.0   1.0 0.500000 0.0 0.0   0.00  0.666667       0.0
0.149254     0.0      0.0         0.0              0.0              0.00                  0.25         0.6   0.000000       0.0   1.0 0.500000 0.0 0.0   0.00  0.333333       0.0
0.343284     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   0.000000       0.0   1.0 0.500000 0.0 0.0   0.00  0.000000       0.0
0.238806     1.0      1.0         1.0              0.0              0.50                  0.75         0.6   0.000000       1.0   1.0 0.500000 0.0 0.0   0.00  0.666667       0.0
0.328358     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       0.0   1.0 0.500000 0.0 0.0   0.00  0.333333       0.0
0.089552     1.0      1.0         1.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.500000 0.0 0.0   0.00  0.000000       0.0
0.283582     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.500000 0.0 0.0   0.00  0.333333       0.0
0.223881     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       0.0   1.0 0.500000 0.0 0.0   0.00  0.000000       0.0
0.492537     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       0.0   1.0 0.500000 0.0 0.0   0.00  0.666667       0.0
0.238806     1.0      1.0         0.0              0.0              0.50                  1.00         0.6   1.000000       0.0   1.0 0.500000 0.0 0.0   0.00  0.333333       0.0
0.552239     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       0.0   1.0 0.500000 0.0 0.0   0.00  0.666667       0.0
0.343284     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.500000 0.0 0.0   0.00  0.333333       0.0
0.388060     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       0.0   1.0 0.500000 0.0 0.0   0.00  0.333333       0.0
0.388060     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       0.0   1.0 0.500000 0.0 0.0   0.00  0.000000       0.0
0.820896     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       0.0   1.0 0.500000 0.0 0.0   0.00  0.666667       0.0
0.059701     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       0.0   1.0 0.500000 0.0 0.0   0.00  0.333333       0.0
0.388060     1.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.500000 0.0 0.0   0.00  0.333333       0.0
0.253731     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.500000 0.0 0.0   0.00  0.333333       0.0
0.298507     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       0.0   1.0 0.500000 0.0 0.0   0.00  0.666667       0.0
0.358209     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.500000 0.0 0.0   0.00  0.666667       0.0
0.447761     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.500000 0.0 0.0   0.00  0.333333       0.0
0.462687     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       0.0   1.0 0.500000 0.0 0.0   0.00  0.666667       0.0
0.447761     0.0      0.0         0.0              0.0              0.25                  0.75         0.6   1.000000       0.0   1.0 0.500000 0.0 0.0   0.00  0.666667       0.0
0.194030     1.0      1.0         0.0              0.0              0.50                  0.75         0.6   1.000000       1.0   1.0 0.500000 0.0 0.0   0.00  0.666667       0.0
0.238806     1.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   1.0 0.500000 0.0 0.0   0.00  0.666667       0.0
0.985075     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       0.0   1.0 0.500000 0.0 0.0   0.25  0.333333       0.0
0.388060     1.0      1.0         0.0              0.0              0.50                  1.00         0.6   0.333333       0.0   0.5 0.500000 0.0 0.0   0.00  0.333333       0.0
0.611940     1.0      1.0         0.0              0.0              0.50                  0.25         0.6   0.333333       0.0   0.5 0.500000 0.0 0.0   0.00  0.666667       0.0
0.477612     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   0.000000       1.0   0.5 0.500000 0.0 0.0   0.00  0.666667       0.0
0.328358     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   0.000000       1.0   0.5 0.500000 0.0 0.0   0.00  0.333333       0.0
0.253731     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       0.0   0.5 0.500000 0.0 0.0   0.00  0.000000       0.0
0.567164     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   0.5 0.500000 0.0 0.0   0.00  0.666667       0.0
0.223881     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   1.000000       0.0   0.5 0.500000 0.0 0.0   0.00  0.333333       0.0
0.283582     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   0.5 0.500000 0.0 0.0   0.00  0.333333       0.0
0.701493     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   0.333333       0.0   0.5 0.500000 0.0 0.0   0.25  0.666667       0.0
0.641791     0.0      0.0         0.0              0.0              0.00                  0.25         0.6   0.000000       0.0   0.5 0.500000 0.0 0.0   0.25  0.666667       0.0
0.597015     1.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       0.0   0.5 0.500000 0.0 0.0   0.25  0.666667       0.0
0.089552     0.0      0.0         0.0              0.0              0.50                  0.75         1.0   1.000000       0.0   1.0 0.500000 1.0 0.0   0.00  0.333333       0.0
0.179104     0.0      0.0         0.0              0.0              0.50                  0.25         1.0   1.000000       0.0   0.5 0.500000 1.0 0.0   0.00  0.333333       0.0
0.462687     0.0      0.0         0.0              0.0              0.50                  0.25         0.0   1.000000       0.0   0.5 0.500000 1.0 0.0   0.00  0.666667       0.0
0.432836     0.0      0.0         0.0              0.0              0.50                  0.25         1.0   1.000000       0.0   0.5 0.500000 1.0 0.0   0.00  0.333333       0.0
0.208955     0.0      0.0         0.0              0.0              0.50                  0.75         0.4   1.000000       1.0   1.0 0.500000 0.5 0.0   0.00  0.333333       0.0
0.164179     0.0      0.0         0.0              0.0              1.00                  0.25         1.0   1.000000       1.0   0.5 0.500000 0.5 0.0   0.00  0.666667       0.0
0.402985     1.0      0.0         0.0              0.0              1.00                  1.00         0.6   1.000000       0.0   0.5 0.500000 0.5 0.0   0.00  0.666667       0.0
0.611940     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       0.0   0.5 0.500000 0.5 0.0   0.25  0.333333       0.0
0.537313     0.0      0.0         1.0              0.0              0.50                  1.00         1.0   0.333333       1.0   1.0 0.500000 0.0 0.0   0.00  0.666667       0.0
0.686567     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   0.5 0.500000 0.0 0.0   0.25  0.333333       0.0
0.402985     1.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       0.0   1.0 0.500000 0.0 0.0   0.00  1.000000       1.0
0.283582     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   1.0 0.500000 0.0 0.0   0.00  1.000000       1.0
0.776119     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   0.333333       1.0   0.5 0.500000 0.0 0.0   0.25  0.666667       1.0
0.716418     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   0.000000       1.0   0.5 0.500000 0.0 0.0   0.25  0.666667       1.0
0.776119     1.0      1.0         1.0              0.0              0.50                  0.25         0.0   1.000000       1.0   0.5 0.500000 0.0 0.0   0.25  0.000000       1.0
0.865672     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       1.0   0.5 0.500000 0.0 0.0   0.25  1.000000       1.0
0.164179     0.0      0.0         0.0              0.0              0.50                  0.75         0.4   1.000000       1.0   0.5 0.500000 1.0 1.0   0.00  1.000000       1.0
0.223881     0.0      0.0         0.0              0.0              0.50                  0.25         1.0   1.000000       1.0   0.5 0.500000 1.0 0.0   0.00  1.000000       1.0
0.313433     0.0      0.0         0.0              0.0              0.50                  0.75         0.4   1.000000       0.0   0.5 0.500000 1.0 0.0   0.00  0.666667       1.0
0.238806     0.0      0.0         0.0              0.0              0.50                  0.25         0.0   1.000000       0.0   0.5 0.500000 1.0 0.0   0.00  1.000000       1.0
0.373134     0.0      0.0         0.0              0.0              0.50                  1.00         1.0   1.000000       0.0   0.5 0.500000 1.0 0.0   0.00  1.000000       1.0
0.507463     0.0      0.0         0.0              0.0              0.50                  1.00         1.0   1.000000       1.0   0.5 0.500000 1.0 0.0   0.00  1.000000       1.0
0.343284     1.0      1.0         0.0              0.0              0.50                  0.25         0.0   1.000000       1.0   0.5 0.500000 1.0 0.0   0.00  1.000000       1.0
0.179104     0.0      0.0         0.0              0.0              0.50                  1.00         0.0   1.000000       1.0   0.5 0.500000 1.0 0.0   0.00  1.000000       1.0
0.179104     1.0      0.0         0.0              0.0              0.50                  0.25         0.0   1.000000       0.0   0.5 0.500000 1.0 0.0   0.00  1.000000       1.0
0.268657     0.0      0.0         0.0              0.0              0.50                  1.00         1.0   1.000000       0.0   0.5 0.500000 1.0 0.0   0.00  0.000000       1.0
0.253731     0.0      0.0         0.0              0.0              0.50                  0.75         0.4   1.000000       0.0   0.5 0.500000 1.0 0.0   0.00  1.000000       1.0
0.208955     0.0      0.0         0.0              0.0              0.50                  0.25         1.0   1.000000       0.0   0.5 0.500000 1.0 0.0   0.00  1.000000       1.0
0.328358     1.0      0.0         0.0              0.0              0.50                  0.25         0.0   1.000000       0.0   0.5 0.500000 1.0 0.0   0.00  1.000000       1.0
0.492537     0.0      0.0         0.0              0.0              0.50                  0.25         0.0   1.000000       0.0   0.5 0.500000 1.0 0.0   0.00  1.000000       1.0
0.223881     0.0      0.0         0.0              0.0              0.50                  0.25         0.0   1.000000       0.0   0.5 0.500000 1.0 0.0   0.00  1.000000       1.0
0.268657     1.0      0.0         0.0              0.0              0.50                  0.25         0.0   1.000000       1.0   0.5 0.500000 1.0 0.0   0.00  1.000000       1.0
0.970149     0.0      1.0         0.0              0.0              0.50                  1.00         1.0   1.000000       0.0   0.5 0.500000 1.0 0.0   0.25  1.000000       1.0
0.701493     0.0      0.0         0.0              0.0              0.50                  0.25         1.0   1.000000       0.0   0.5 0.500000 1.0 0.0   0.25  1.000000       1.0
0.716418     1.0      1.0         0.0              0.0              0.50                  1.00         1.0   1.000000       0.0   0.5 0.500000 1.0 0.0   0.25  1.000000       1.0
0.671642     1.0      0.0         0.0              0.0              0.50                  1.00         0.0   1.000000       0.0   0.5 0.500000 1.0 0.0   0.25  1.000000       1.0
0.955224     1.0      1.0         0.0              0.0              0.50                  0.25         0.0   1.000000       0.0   0.5 0.500000 1.0 0.0   0.25  0.666667       1.0
0.746269     0.0      0.0         1.0              0.0              0.50                  0.25         0.0   1.000000       0.0   0.5 0.500000 1.0 0.0   0.25  0.000000       1.0
0.298507     0.0      0.0         0.0              0.0              0.50                  1.00         1.0   1.000000       0.0   0.5 0.500000 0.5 0.0   0.00  0.000000       1.0
0.641791     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       0.0   0.5 0.500000 0.5 0.0   0.25  0.666667       1.0
0.283582     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       1.0   0.0 0.500000 0.5 0.0   0.00  0.000000       1.0
0.611940     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   0.000000       0.0   1.0 0.500000 0.0 0.0   0.00  1.000000       1.0
0.552239     1.0      1.0         0.0              0.0              0.50                  0.75         1.0   0.000000       0.0   0.5 0.500000 0.0 0.0   0.00  0.000000       1.0
0.537313     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   0.000000       1.0   0.5 0.500000 0.0 0.0   0.00  1.000000       1.0
0.238806     0.0      0.0         0.0              0.0              0.50                  0.75         0.6   0.000000       1.0   0.5 0.500000 0.5 0.0   0.00  0.000000       1.0
0.432836     1.0      1.0         0.0              0.0              0.50                  1.00         1.0   1.000000       0.0   0.0 0.500000 1.0 1.0   0.25  1.000000       1.0
0.000000     0.0      0.0         0.0              0.0              0.50                  0.50         0.0   1.000000       0.0   0.5 0.500000 1.0 0.0   0.00  1.000000       1.0
0.208955     1.0      0.0         0.0              0.0              0.50                  0.25         0.2   1.000000       0.0   0.5 0.500000 1.0 0.0   0.00  1.000000       1.0
0.567164     1.0      0.0         0.0              0.0              0.50                  0.25         0.0   1.000000       0.0   0.5 0.500000 1.0 0.0   0.00  1.000000       1.0
0.447761     0.0      0.0         0.0              0.0              0.50                  0.75         0.4   1.000000       0.0   0.5 0.500000 1.0 0.0   0.00  1.000000       1.0
0.343284     0.0      0.0         0.0              0.0              0.50                  0.75         0.8   1.000000       0.0   0.0 0.500000 1.0 1.0   0.25  1.000000       1.0
0.492537     0.0      0.0         0.0              0.0              0.00                  0.25         0.4   0.333333       0.0   0.5 0.666667 0.5 0.0   0.00  1.000000       0.0
0.402985     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       0.0   0.5 0.666667 0.0 0.0   0.00  0.666667       0.0
0.119403     0.0      0.0         1.0              0.0              0.50                  1.00         1.0   1.000000       1.0   0.5 0.666667 0.5 0.0   0.00  1.000000       1.0
0.104478     0.0      0.0         0.0              0.0              0.50                  0.25         1.0   1.000000       0.0   0.5 0.666667 0.5 0.0   0.00  1.000000       1.0
0.432836     0.0      0.0         0.0              0.0              0.50                  0.75         0.4   1.000000       0.0   0.5 0.666667 1.0 0.0   0.00  1.000000       1.0
0.238806     0.0      0.0         0.0              0.0              0.50                  0.75         0.4   1.000000       1.0   0.5 0.666667 1.0 0.0   0.00  0.333333       1.0
0.149254     0.0      0.0         0.0              0.0              0.50                  0.25         0.4   1.000000       0.0   0.5 0.666667 1.0 0.0   0.00  1.000000       1.0
0.253731     1.0      0.0         1.0              0.0              0.50                  0.25         0.0   1.000000       0.0   0.5 0.666667 1.0 0.0   0.00  1.000000       1.0
1.000000     1.0      1.0         0.0              0.0              0.50                  0.75         0.4   1.000000       1.0   0.5 0.666667 1.0 0.0   0.25  1.000000       1.0
0.641791     0.0      0.0         0.0              0.0              1.00                  0.25         0.2   1.000000       0.0   0.5 0.666667 1.0 0.0   0.25  1.000000       1.0
0.791045     1.0      1.0         1.0              0.0              1.00                  0.75         0.0   1.000000       0.0   0.5 0.666667 1.0 0.0   0.50  1.000000       1.0
0.328358     0.0      0.0         0.0              0.0              1.00                  0.75         0.6   1.000000       0.0   0.5 0.666667 0.5 1.0   0.25  1.000000       1.0
0.656716     1.0      1.0         1.0              0.0              0.25                  0.25         0.6   0.333333       0.0   0.5 0.666667 0.0 0.0   0.00  1.000000       1.0
0.089552     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       0.0   0.5 0.666667 0.0 0.0   0.00  1.000000       1.0
0.865672     0.0      0.0         0.0              0.0              0.50                  1.00         1.0   1.000000       0.0   0.0 0.666667 0.5 1.0   1.00  1.000000       1.0
0.298507     1.0      1.0         0.0              0.0              0.50                  0.75         0.4   1.000000       0.0   0.0 0.666667 1.0 1.0   0.25  1.000000       1.0
0.253731     0.0      0.0         0.0              0.0              0.50                  1.00         0.6   1.000000       0.0   0.5 0.833333 0.0 0.0   0.00  0.333333       0.0
0.582090     1.0      1.0         0.0              0.0              0.50                  0.75         1.0   0.333333       0.0   0.5 0.833333 1.0 0.0   0.25  1.000000       1.0
0.164179     0.0      1.0         0.0              0.0              0.50                  0.75         0.0   0.333333       0.0   0.0 0.833333 1.0 0.0   0.00  1.000000       1.0
0.567164     0.0      0.0         0.0              0.0              0.50                  0.25         1.0   1.000000       1.0   0.0 0.833333 1.0 0.0   0.00  1.000000       1.0
0.298507     0.0      0.0         0.0              0.0              0.50                  0.25         0.2   1.000000       0.0   0.0 0.833333 1.0 0.0   0.00  1.000000       1.0
0.507463     1.0      0.0         0.0              0.0              0.50                  0.25         0.0   1.000000       0.0   0.0 0.833333 1.0 0.0   0.00  1.000000       1.0
0.283582     0.0      0.0         0.0              0.0              0.50                  0.25         1.0   1.000000       1.0   0.0 0.833333 1.0 0.0   0.00  1.000000       1.0
0.970149     0.0      1.0         1.0              0.0              0.50                  0.25         1.0   1.000000       1.0   0.0 0.833333 1.0 0.0   0.50  1.000000       1.0
0.776119     0.0      0.0         0.0              0.0              0.50                  1.00         1.0   1.000000       0.0   0.0 0.833333 1.0 0.0   0.50  1.000000       1.0
0.791045     0.0      0.0         0.0              0.0              0.50                  0.25         0.0   1.000000       0.0   0.0 0.833333 1.0 1.0   1.00  1.000000       1.0
0.835821     0.0      1.0         0.0              0.0              0.50                  0.75         0.6   0.000000       0.0   0.0 0.833333 0.0 1.0   1.00  1.000000       1.0
0.731343     0.0      0.0         1.0              0.0              0.50                  0.25         0.6   0.000000       0.0   0.0 0.833333 0.0 1.0   1.00  1.000000       1.0
0.970149     1.0      1.0         0.0              0.0              0.50                  0.75         0.6   0.333333       0.0   0.5 0.833333 0.0 0.0   0.25  1.000000       1.0
0.611940     0.0      0.0         0.0              0.0              0.50                  0.25         0.8   1.000000       0.0   0.0 0.833333 1.0 0.0   0.25  1.000000       1.0
0.835821     1.0      1.0         1.0              0.0              1.00                  0.25         0.0   1.000000       0.0   0.0 0.833333 1.0 0.0   0.50  1.000000       1.0
0.940299     1.0      1.0         1.0              1.0              0.00                  0.25         0.6   0.000000       0.0   0.0 0.833333 0.0 1.0   1.00  1.000000       1.0
0.537313     0.0      0.0         0.0              0.0              0.50                  0.25         0.6   1.000000       0.0   0.0 0.833333 0.5 1.0   0.25  1.000000       1.0
0.776119     0.0      1.0         0.0              0.0              1.00                  0.25         0.6   1.000000       0.0   0.0 0.833333 0.0 0.0   0.75  0.000000       1.0
0.238806     1.0      1.0         0.0              1.0              0.50                  0.75         0.2   1.000000       0.0   0.0 0.833333 1.0 1.0   0.25  1.000000       1.0
0.701493     1.0      1.0         0.0              0.0              0.50                  0.75         0.4   1.000000       0.0   0.0 0.833333 1.0 1.0   1.00  1.000000       1.0
0.656716     0.0      0.0         0.0              0.0              0.50                  0.25         0.0   1.000000       0.0   0.0 1.000000 1.0 0.0   1.00  1.000000       1.0
0.373134     1.0      1.0         0.0              0.0              0.50                  0.25         0.0   1.000000       0.0   0.0 1.000000 1.0 0.0   0.00  1.000000       1.0
0.462687     1.0      1.0         0.0              0.0              0.50                  0.75         0.0   0.000000       1.0   0.0 1.000000 1.0 1.0   0.25  1.000000       1.0
0.850746     1.0      1.0         1.0              1.0              0.50                  1.00         1.0   1.000000       1.0   0.0 1.000000 1.0 1.0   1.00  0.000000       1.0
0.985075     1.0      1.0         0.0              1.0              0.50                  0.25         0.2   1.000000       0.0   0.0 1.000000 1.0 1.0   1.00  1.000000       1.0
0.850746     1.0      1.0         1.0              0.0              0.50                  0.25         0.0   1.000000       0.0   0.0 1.000000 1.0 1.0   1.00  1.000000       1.0
0.686567     1.0      1.0         1.0              1.0              0.00                  0.25         0.2   0.333333       0.0   0.0 1.000000 1.0 0.0   0.75  1.000000       1.0
0.776119     1.0      1.0         0.0              0.0              0.50                  0.25         0.0   1.000000       0.0   0.0 1.000000 1.0 0.0   0.75  1.000000       1.0
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="modeling">
<h2>Modeling<a class="headerlink" href="#modeling" title="Link to this heading">#</a></h2>
<p>Modeling adalah proses membangun sebuah model matematis atau algoritmik berdasarkan data yang tersedia, dengan tujuan untuk memahami pola atau hubungan antar variabel dalam data, serta untuk membuat prediksi atau keputusan otomatis pada data baru.</p>
<p>Dalam konteks machine learning, modeling merupakan inti dari keseluruhan proses pembelajaran mesin, yaitu membuat model prediktif atau deskriptif dari data latih (training data), dan kemudian mengujinya pada data uji (testing data) untuk menilai kinerjanya.</p>
</section>
<section id="pemodelan-prediksi-kekambuhan-kanker-tiroid-menggunakan-decision-tree">
<h2>Pemodelan Prediksi Kekambuhan Kanker Tiroid Menggunakan Decision Tree<a class="headerlink" href="#pemodelan-prediksi-kekambuhan-kanker-tiroid-menggunakan-decision-tree" title="Link to this heading">#</a></h2>
<p>Pemodelan dalam penelitian ini bertujuan untuk <strong>mengklasifikasikan kemungkinan kekambuhan (recurrence)</strong> pada pasien kanker tiroid berdiferensiasi berdasarkan sejumlah fitur klinis, seperti usia, jenis kelamin, ukuran tumor, stadium kanker, jenis terapi yang diterima, serta karakteristik jaringan kanker. Variabel target yang digunakan terdiri dari dua kelas, yaitu <strong>kambuh (1)</strong> dan <strong>tidak kambuh (0)</strong>.</p>
<p>Model klasifikasi yang digunakan adalah <strong>Decision Tree Classifier</strong>, yaitu metode supervised learning yang bekerja berdasarkan struktur bercabang seperti pohon. Setiap percabangan menggambarkan keputusan yang diambil berdasarkan nilai dari fitur tertentu, sedangkan ujung cabangnya (daun) mewakili hasil klasifikasi.</p>
<ol class="arabic simple">
<li><p>Pembagian Dataset: Data Latih dan Data Uji</p></li>
</ol>
<p>Tahap awal pemodelan melibatkan pembagian dataset menjadi dua bagian, yaitu:</p>
<ul class="simple">
<li><p>Data Latih (Training Set): digunakan untuk melatih model agar mampu mengenali pola dari data historis.</p></li>
<li><p>Data Uji (Testing Set): digunakan untuk menguji kinerja model terhadap data baru yang belum pernah dilihat sebelumnya.</p></li>
</ul>
<p>Pada eksperimen ini digunakan proporsi 80% untuk data latih dan 20% untuk data uji, yang merupakan rasio umum dalam banyak eksperimen machine learning.</p>
<ol class="arabic simple" start="2">
<li><p>Pra-pemrosesan Data**</p></li>
</ol>
<p>Sebelum model dilatih, dilakukan beberapa tahap <strong>pra-pemrosesan data</strong>, di antaranya:</p>
<ul class="simple">
<li><p>Transformasi kategori ke numerik: Seluruh fitur kategorikal seperti jenis kelamin, tipe histologis, dan jenis terapi dikonversi ke bentuk numerik menggunakan Label Encoding agar bisa diproses oleh algoritma.</p></li>
<li><p>Normalisasi fitur numerik: Fitur numerik seperti <code class="docutils literal notranslate"><span class="pre">Age</span></code>, <code class="docutils literal notranslate"><span class="pre">Tumor_size</span></code>, dan <code class="docutils literal notranslate"><span class="pre">Positive_LN_count</span></code> dinormalisasi menggunakan MinMaxScaler agar memiliki skala yang seragam, terutama untuk algoritma yang sensitif terhadap perbedaan skala seperti KNN.</p></li>
<li><p>Pemisahan fitur dan target: Fitur-fitur prediktor dipisahkan dari target <code class="docutils literal notranslate"><span class="pre">Recurrence</span></code>.</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p>Pelatihan Model Decision Tree</p></li>
</ol>
<p>Model dilatih menggunakan algoritma Decision Tree, di mana:</p>
<ul class="simple">
<li><p>Setiap node akan membagi data berdasarkan fitur yang paling mengurangi impuritas (biasanya diukur dengan Gini Index).</p></li>
<li><p>Proses pembagian berlangsung secara bertingkat hingga mencapai batas tertentu, misalnya kedalaman maksimum (<code class="docutils literal notranslate"><span class="pre">max_depth</span> <span class="pre">=</span> <span class="pre">5</span></code>) untuk mencegah overfitting.</p></li>
</ul>
<section id="evaluasi-model">
<h3>4. Evaluasi Model<a class="headerlink" href="#evaluasi-model" title="Link to this heading">#</a></h3>
<p>Setelah pelatihan, model dievaluasi menggunakan:</p>
<ul class="simple">
<li><p>Akurasi: mengukur persentase prediksi yang benar dari keseluruhan data uji.</p></li>
<li><p>Classification Report: memberikan metrik tambahan seperti precision, recall, dan F1-score untuk masing-masing kelas.</p></li>
<li><p>Confusion Matrix: menampilkan bagaimana prediksi model tersebar pada kelas sebenarnya.</p></li>
<li><p>Visualisasi pohon keputusan: membantu memahami fitur apa yang paling memengaruhi keputusan model, urutan pengecekan fitur, dan logika klasifikasi.</p></li>
</ul>
<ol class="arabic simple" start="5">
<li><p>Prediksi Kekambuhan Pasien Baru</p></li>
</ol>
<p>Setelah proses pelatihan dan evaluasi selesai, model dapat digunakan untuk:</p>
<ul class="simple">
<li><p>Memprediksi kemungkinan kekambuhan pada pasien baru berdasarkan data klinis yang tersedia.</p></li>
<li><p>Mengidentifikasi fitur-fitur penting yang paling berpengaruh terhadap risiko kekambuhan, seperti stadium tumor, ukuran tumor, atau status metastasis.</p></li>
</ul>
<p>Kesimpulan</p>
<p>Decision Tree merupakan metode yang efektif untuk kasus klasifikasi seperti prediksi kekambuhan kanker tiroid karena:</p>
<ul class="simple">
<li><p>Dapat menangani kombinasi data numerik dan kategorikal,</p></li>
<li><p>Tidak memerlukan normalisasi khusus untuk data numerik (meskipun tetap disarankan),</p></li>
<li><p>Menyediakan interpretasi visual yang memudahkan pemahaman logika model.</p></li>
</ul>
<p>Model ini dapat menjadi alat bantu pengambilan keputusan klinis berbasis data, terutama dalam menentukan langkah lanjutan terhadap pasien yang berpotensi mengalami kekambuhan kanker.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span><span class="p">,</span> <span class="n">plot_tree</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">classification_report</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># === 7. Training Decision Tree ===</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;gini&#39;</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># === 8. Evaluasi Model ===</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Akurasi:&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== Classification Report ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>

<span class="c1"># === 9. Visualisasi Struktur Pohon ===</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">plot_tree</span><span class="p">(</span>
    <span class="n">clf</span><span class="p">,</span>
    <span class="n">feature_names</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span>
    <span class="n">class_names</span><span class="o">=</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">clf</span><span class="o">.</span><span class="n">classes_</span><span class="p">],</span>
    <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">rounded</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Pohon Keputusan - Decision Tree Classifier&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Akurasi: 0.961038961038961

=== Classification Report ===
              precision    recall  f1-score   support

         0.0       0.95      1.00      0.97        55
         1.0       1.00      0.86      0.93        22

    accuracy                           0.96        77
   macro avg       0.97      0.93      0.95        77
weighted avg       0.96      0.96      0.96        77
</pre></div>
</div>
<img alt="_images/83aa6030197d0bcc11f95e0d2dc2cb849086afb3ec44c7fa167cb0d1a1ea5298.png" src="_images/83aa6030197d0bcc11f95e0d2dc2cb849086afb3ec44c7fa167cb0d1a1ea5298.png" />
</div>
</div>
<p>Berikut adalah <strong>parafrase penjelasan</strong> yang sudah <strong>disesuaikan dengan data kanker tiroid</strong> milikmu menggunakan metode <strong>K-Nearest Neighbors (KNN)</strong>:</p>
</section>
</section>
<hr class="docutils" />
<section id="pemodelan-kekambuhan-kanker-tiroid-dengan-k-nearest-neighbors-knn">
<h2>Pemodelan Kekambuhan Kanker Tiroid dengan K-Nearest Neighbors (KNN)<a class="headerlink" href="#pemodelan-kekambuhan-kanker-tiroid-dengan-k-nearest-neighbors-knn" title="Link to this heading">#</a></h2>
<p>Model K-Nearest Neighbors (KNN) digunakan dalam pemodelan ini untuk memprediksi apakah pasien kanker tiroid mengalami kekambuhan atau tidak, berdasarkan sejumlah fitur klinis seperti usia, jenis kelamin, ukuran tumor, status limfosit, jenis terapi, dan karakteristik kanker lainnya.</p>
<p>KNN merupakan metode pembelajaran berbasis instance (instance-based learning), di mana prediksi terhadap data baru dilakukan dengan mengidentifikasi beberapa tetangga terdekat dalam ruang fitur, kemudian mengklasifikasikannya berdasarkan mayoritas label tetangga tersebut.</p>
<ol class="arabic simple">
<li><p>Pembagian Dataset: Data Latih dan Data Uji</p></li>
</ol>
<p>Sama seperti pendekatan supervised learning lainnya, data pasien dibagi menjadi dua kelompok utama:</p>
<ul class="simple">
<li><p>Data Latih (Training Set): digunakan sebagai referensi untuk proses pencocokan (tetangga).</p></li>
<li><p>Data Uji (Testing Set): digunakan untuk mengukur kemampuan model dalam mengklasifikasikan pasien baru.</p></li>
</ul>
<p>Dalam percobaan ini digunakan proporsi 80% data latih dan 20% data uji, namun proporsi ini bisa disesuaikan tergantung pada kebutuhan dan ukuran dataset.</p>
<ol class="arabic simple" start="2">
<li><p>Pra-pemrosesan Data</p></li>
</ol>
<p>Agar algoritma KNN bekerja secara optimal, beberapa langkah pra-pemrosesan diterapkan terlebih dahulu:</p>
<ul class="simple">
<li><p>Label Encoding untuk fitur kategorikal seperti jenis kelamin, tipe histologi, terapi, dan hasil patologi, agar bisa dibaca oleh algoritma KNN yang hanya memahami nilai numerik.</p></li>
<li><p>Normalisasi fitur numerik seperti usia, ukuran tumor, dan jumlah kelenjar getah bening positif menggunakan metode MinMaxScaler. Normalisasi sangat penting karena KNN menghitung jarak antar data, dan fitur dengan skala besar dapat memengaruhi hasil jika tidak disamakan.</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p>Cara Kerja KNN</p></li>
</ol>
<p>Tidak seperti algoritma yang membentuk aturan eksplisit, KNN menyimpan semua data latih dan membuat prediksi berdasarkan jarak antara data uji dan seluruh data latih. Biasanya digunakan jarak Euclidean untuk mengukur kedekatan.</p>
<p>Langkah-langkahnya:</p>
<ol class="arabic simple">
<li><p>Hitung jarak antara satu pasien baru dan seluruh data latih.</p></li>
<li><p>Ambil k tetangga terdekat berdasarkan jarak terpendek.</p></li>
<li><p>Lakukan voting: label (kambuh/tidak kambuh) yang paling sering muncul di antara tetangga tersebut akan menjadi prediksi akhir.</p></li>
</ol>
<p>Contohnya, jika <code class="docutils literal notranslate"><span class="pre">k</span> <span class="pre">=</span> <span class="pre">5</span></code> dan dari lima tetangga terdekat terdapat 3 pasien dengan status kambuh, maka model akan memprediksi pasien baru tersebut juga berpotensi kambuh.</p>
<ol class="arabic simple" start="4">
<li><p>Evaluasi Model</p></li>
</ol>
<p>Setelah model melakukan prediksi, kinerjanya diukur menggunakan:</p>
<ul class="simple">
<li><p>Akurasi: rasio prediksi yang benar terhadap seluruh prediksi.</p></li>
<li><p>Classification Report: berisi nilai precision, recall, dan F1-score untuk setiap kelas.</p></li>
<li><p>Confusion Matrix: menunjukkan distribusi klasifikasi benar dan salah terhadap data aktual.</p></li>
</ul>
<p>Eksperimen juga dapat dilakukan dengan mencoba beberapa nilai k (misalnya k = 3, 5, 7) untuk mencari nilai k terbaik yang menghasilkan performa paling optimal.</p>
<p>Kesimpulan</p>
<p>Model K-Nearest Neighbors merupakan metode yang sederhana namun efektif dalam memprediksi kekambuhan kanker tiroid, terutama bila data telah melalui tahap encoding dan normalisasi yang tepat. KNN sangat cocok digunakan saat dibutuhkan pemodelan cepat dan transparan, karena mudah dipahami serta tidak memerlukan pelatihan model yang rumit.</p>
<p>Dalam konteks medis, KNN dapat digunakan untuk membantu dokter mengidentifikasi pasien dengan risiko kekambuhan tinggi, berdasarkan kemiripan dengan pasien-pasien sebelumnya.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>

<span class="c1"># Melatih model</span>
<span class="n">knn_model</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">knn_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Prediksi</span>
<span class="n">knn_preds</span> <span class="o">=</span> <span class="n">knn_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Evaluasi</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model: K-Nearest Neighbors (K=5)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Akurasi:&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">knn_preds</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Classification Report:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">knn_preds</span><span class="p">))</span>

<span class="c1"># Confusion Matrix</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">knn_preds</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Greens&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix - KNN&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Predicted&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Actual&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: K-Nearest Neighbors (K=5)
Akurasi: 0.8961038961038961
Classification Report:
              precision    recall  f1-score   support

         0.0       0.89      0.98      0.93        55
         1.0       0.94      0.68      0.79        22

    accuracy                           0.90        77
   macro avg       0.91      0.83      0.86        77
weighted avg       0.90      0.90      0.89        77
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">11</span><span class="p">],</span> <span class="n">line</span> <span class="mi">17</span>
<span class="g g-Whitespace">     </span><span class="mi">14</span> <span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">knn_preds</span><span class="p">))</span>
<span class="g g-Whitespace">     </span><span class="mi">16</span> <span class="c1"># Confusion Matrix</span>
<span class="ne">---&gt; </span><span class="mi">17</span> <span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">knn_preds</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">18</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Greens&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">19</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix - KNN&quot;</span><span class="p">)</span>

<span class="ne">NameError</span>: name &#39;confusion_matrix&#39; is not defined
</pre></div>
</div>
</div>
</div>
</section>
<section id="pemodelan-prediksi-kekambuhan-kanker-tiroid-dengan-gaussian-naive-bayes">
<h2>Pemodelan Prediksi Kekambuhan Kanker Tiroid dengan Gaussian Naive Bayes<a class="headerlink" href="#pemodelan-prediksi-kekambuhan-kanker-tiroid-dengan-gaussian-naive-bayes" title="Link to this heading">#</a></h2>
<p>Pada proses pemodelan kali ini, tujuan utamanya adalah untuk mengklasifikasikan kemungkinan kekambuhan (recurrence) kanker tiroid berdasarkan sejumlah fitur klinis dan karakteristik pasien, seperti usia, jenis kelamin, ukuran tumor, tipe kanker, status limfosit, dan jenis pengobatan. Kelas target terdiri dari dua kategori, yaitu: Tidak Kambuh (0) dan Kambuh (1).</p>
<p>Untuk membangun model prediksi ini, digunakan algoritma Gaussian Naive Bayes, yaitu salah satu metode klasifikasi berbasis probabilitas yang mengasumsikan bahwa fitur-fitur data berdistribusi normal dan saling independen antar satu sama lain.</p>
<ol class="arabic simple">
<li><p>Pembagian Data: Data Latih dan Data Uji</p></li>
</ol>
<p>Langkah pertama dalam pemodelan adalah membagi data menjadi dua bagian:</p>
<ul class="simple">
<li><p>Data Latih (Training Set): digunakan untuk melatih model agar dapat mengenali pola dari data historis.</p></li>
<li><p>Data Uji (Testing Set): digunakan untuk mengevaluasi kemampuan model memprediksi data baru yang belum pernah dilihat.</p></li>
</ul>
<p>Dalam eksperimen ini, digunakan rasio pembagian data sebesar 80% data latih dan 20% data uji, yang merupakan salah satu praktik umum dalam pemodelan. Rasio ini bisa disesuaikan tergantung pada jumlah data dan kebutuhan.</p>
<ol class="arabic simple" start="2">
<li><p>Menghitung Probabilitas Awal (Prior Probability)</p></li>
</ol>
<p>Setelah data terbagi, langkah selanjutnya adalah menghitung probabilitas awal (prior) untuk masing-masing kelas, yaitu berapa peluang kekambuhan atau tidak berdasarkan distribusi kelas dalam data latih:</p>
<ul class="simple">
<li><p>Hitung jumlah kasus kambuh dan tidak kambuh.</p></li>
<li><p>Bagi masing-masing jumlah tersebut dengan total data latih.</p></li>
</ul>
<p>Hasilnya akan menunjukkan seberapa besar kemungkinan kekambuhan terjadi tanpa mempertimbangkan fitur lainnya.</p>
<ol class="arabic simple" start="3">
<li><p>Menghitung Rata-rata dan Standar Deviasi Tiap Fitur</p></li>
</ol>
<p>Untuk setiap kelas (<code class="docutils literal notranslate"><span class="pre">Kambuh</span></code> dan <code class="docutils literal notranslate"><span class="pre">Tidak</span> <span class="pre">Kambuh</span></code>), dihitung:</p>
<ul class="simple">
<li><p><strong>Rata-rata</strong> dari masing-masing fitur numerik seperti usia pasien, ukuran tumor, dan jumlah kelenjar getah bening positif.</p></li>
<li><p><strong>Standar deviasi</strong> dari fitur-fitur tersebut untuk mengetahui sebaran nilainya.</p></li>
</ul>
<p>Langkah ini penting karena <strong>Gaussian Naive Bayes</strong> menggunakan asumsi bahwa nilai-nilai fitur mengikuti distribusi Gaussian (normal) dalam tiap kelas.</p>
<ol class="arabic simple" start="4">
<li><p>Menghitung Probabilitas Gaussian</p></li>
</ol>
<p>Dengan menggunakan rumus distribusi Gaussian, probabilitas kemunculan suatu nilai fitur dalam masing-masing kelas dihitung. Rumusnya:</p>
<div class="math notranslate nohighlight">
\[
P(x|\mu,\sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(x - \mu)^2}{2\sigma^2} \right)
\]</div>
<p>dimana:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x\)</span> adalah nilai fitur pasien,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mu\)</span> adalah rata-rata fitur untuk kelas tertentu,</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma\)</span> adalah standar deviasi fitur tersebut.</p></li>
</ul>
<hr class="docutils" />
<ol class="arabic simple" start="5">
<li><p>Menghitung Probabilitas Posterior</p></li>
</ol>
<p>Untuk masing-masing kelas, probabilitas gabungan dari semua fitur dihitung dengan mengalikan probabilitas Gaussian tiap fitur dan prior dari kelas tersebut:</p>
<div class="math notranslate nohighlight">
\[
P(C_i|X) = P(x_1|C_i) \cdot P(x_2|C_i) \cdot \ldots \cdot P(x_n|C_i) \cdot P(C_i)
\]</div>
<p>dimana:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(x_k|C_i)\)</span> adalah probabilitas fitur ke-k pada kelas ke-i,</p></li>
<li><p><span class="math notranslate nohighlight">\(P(C_i)\)</span> adalah probabilitas awal dari kelas tersebut.</p></li>
</ul>
<ol class="arabic simple" start="6">
<li><p>Menentukan Kelas Prediksi</p></li>
</ol>
<p>Langkah akhir adalah memilih kelas dengan nilai probabilitas posterior terbesar. Kelas tersebut dianggap sebagai hasil prediksi model untuk kasus pasien yang diberikan.</p>
<section id="kesimpulan">
<h3>Kesimpulan<a class="headerlink" href="#kesimpulan" title="Link to this heading">#</a></h3>
<p>Model Gaussian Naive Bayes bekerja dengan mengasumsikan independensi antar fitur serta distribusi normal, yang membuatnya sangat cepat dan efisien. Dalam kasus prediksi kekambuhan kanker tiroid, model ini mampu memberikan hasil prediktif yang cukup baik, sekaligus mudah diinterpretasi. Dengan menggunakan fitur klinis dasar, model ini dapat membantu dokter atau pihak medis dalam memperkirakan risiko kekambuhan pasien secara otomatis.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>

<span class="c1"># Melatih model</span>
<span class="n">nb_model</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">nb_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Prediksi</span>
<span class="n">nb_preds</span> <span class="o">=</span> <span class="n">nb_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Evaluasi</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model: Gaussian Naive Bayes&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Akurasi:&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">nb_preds</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Classification Report:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">nb_preds</span><span class="p">))</span>

<span class="c1"># Confusion Matrix</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">nb_preds</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Oranges&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix - Naive Bayes&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Predicted&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Actual&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: Gaussian Naive Bayes
Akurasi: 0.9090909090909091
Classification Report:
              precision    recall  f1-score   support

         0.0       0.90      0.98      0.94        55
         1.0       0.94      0.73      0.82        22

    accuracy                           0.91        77
   macro avg       0.92      0.85      0.88        77
weighted avg       0.91      0.91      0.91        77
</pre></div>
</div>
<img alt="_images/895b2bfd002a862853d2d2f16dd44757addeb4498e89573aa32aa5c02cba464a.png" src="_images/895b2bfd002a862853d2d2f16dd44757addeb4498e89573aa32aa5c02cba464a.png" />
</div>
</div>
</section>
</section>
<section id="evaluasi">
<h2>Evaluasi<a class="headerlink" href="#evaluasi" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">f1_score</span>

<span class="c1"># --- Decision Tree ---</span>
<span class="n">clf_dt</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;gini&#39;</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">clf_dt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred_dt</span> <span class="o">=</span> <span class="n">clf_dt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># --- KNN ---</span>
<span class="n">clf_knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">clf_knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred_knn</span> <span class="o">=</span> <span class="n">clf_knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># --- Gaussian Naive Bayes ---</span>
<span class="n">clf_nb</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">clf_nb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred_nb</span> <span class="o">=</span> <span class="n">clf_nb</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># --- Buat Ringkasan Evaluasi ---</span>
<span class="n">evaluation_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;Model&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Decision Tree&#39;</span><span class="p">,</span> <span class="s1">&#39;KNN&#39;</span><span class="p">,</span> <span class="s1">&#39;Naive Bayes&#39;</span><span class="p">],</span>
    <span class="s1">&#39;Akurasi&#39;</span><span class="p">:</span> <span class="p">[</span>
        <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_dt</span><span class="p">),</span>
        <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_knn</span><span class="p">),</span>
        <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_nb</span><span class="p">)</span>
    <span class="p">],</span>
    <span class="s1">&#39;Precision&#39;</span><span class="p">:</span> <span class="p">[</span>
        <span class="n">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_dt</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">),</span>
        <span class="n">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_knn</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">),</span>
        <span class="n">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_nb</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>
    <span class="p">],</span>
    <span class="s1">&#39;Recall&#39;</span><span class="p">:</span> <span class="p">[</span>
        <span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_dt</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">),</span>
        <span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_knn</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">),</span>
        <span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_nb</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>
    <span class="p">],</span>
    <span class="s1">&#39;F1 Score&#39;</span><span class="p">:</span> <span class="p">[</span>
        <span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_dt</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">),</span>
        <span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_knn</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">),</span>
        <span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_nb</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>
    <span class="p">]</span>
<span class="p">})</span>


<span class="c1"># Tampilkan Hasil</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== Hasil Evaluasi Tiga Model ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">evaluation_results</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>=== Hasil Evaluasi Tiga Model ===
           Model   Akurasi  Precision    Recall  F1 Score
0  Decision Tree  0.961039   0.974138  0.931818  0.950140
1            KNN  0.896104   0.911373  0.831818  0.860254
2    Naive Bayes  0.909091   0.920588  0.854545  0.879822
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Binning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Teknik Binning</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kekambuhan-kanker-tiroid-terdiferensiasi">Kekambuhan Kanker Tiroid Terdiferensiasi</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-understanding">Data Understanding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sumber-data-set">Sumber Data Set</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#penjelasan-fitur-dan-variabel">Penjelasan Fitur dan Variabel</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#integrasi-data">Integrasi Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualisasi-data">Visualisasi Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#missing-value">Missing Value</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocessing-data">Preprocessing Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transformasi-data">Transformasi Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalisasi-data">Normalisasi Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#split-data">Split Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hasil-data-setelah-di-preprocessing">Hasil Data setelah di Preprocessing</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modeling">Modeling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pemodelan-prediksi-kekambuhan-kanker-tiroid-menggunakan-decision-tree">Pemodelan Prediksi Kekambuhan Kanker Tiroid Menggunakan Decision Tree</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluasi-model">4. Evaluasi Model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pemodelan-kekambuhan-kanker-tiroid-dengan-k-nearest-neighbors-knn">Pemodelan Kekambuhan Kanker Tiroid dengan K-Nearest Neighbors (KNN)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pemodelan-prediksi-kekambuhan-kanker-tiroid-dengan-gaussian-naive-bayes">Pemodelan Prediksi Kekambuhan Kanker Tiroid dengan Gaussian Naive Bayes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kesimpulan">Kesimpulan</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluasi">Evaluasi</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Muhammad Umar Faruq - 008
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
       Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>